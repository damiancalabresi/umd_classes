# ##############################################################################
# Client-Server Architecture
# ##############################################################################

This slide explains the client-server model, which is a way to organize
distributed applications. In this model, tasks are divided between clients and
servers. Clients are the ones that request services, like using a dashboard or a
graphical user interface. Servers, on the other hand, provide the resources or
services, such as accessing a database. The architecture of a database system is
also discussed, where the back-end (server) handles tasks like query evaluation
and recovery, while the front-end (clients) uses tools like forms and GUIs. The
connection between the front-end and back-end is made through SQL or APIs.

Let's move on to understand the differences between parallel and distributed
computing.

# ##############################################################################
# Parallel vs Distributed Computing
# ##############################################################################

This slide highlights the differences between parallel and distributed
computing. Parallel computing involves one computer with multiple CPUs or a
cluster of computers working on a single task. These systems are usually
homogenous and located close to each other. In contrast, distributed computing
consists of separate systems that are autonomous and geographically distant.
These systems are heterogeneous and perform different tasks. The key takeaway is
that parallel computing focuses on a single task with close systems, while
distributed computing involves separate tasks with distant systems.

Next, we will delve deeper into the specifics of parallel systems.

# ##############################################################################
# Parallel Systems
# ##############################################################################

This slide provides details about parallel systems, which include multiple
processors, memories, and disks connected through a fast network. There are two
types of parallel machines: coarse-grain and fine-grain. Coarse-grain machines
have a small number of powerful processors, like a laptop with multiple CPUs.
Fine-grain machines, also known as massively parallel machines, have thousands
of smaller processors, offering a higher degree of parallelism. These can have
shared or separate memory and include examples like GPUs and The Connection
Machine from MIT in the 1980s. The main point is understanding the scale and
structure of different parallel systems.

# ##############################################################################
# Parallel Databases: Introduction
# ##############################################################################

Parallel databases were the go-to solution before the advent of MapReduce. With
the drop in prices of microprocessors, memory, and disks, parallel machines have
become more accessible and common. This trend is expected to continue as
technology advances. Databases are growing larger due to the accumulation of
transaction data and the storage of multimedia objects. Large-scale parallel
databases are increasingly used to store vast amounts of data, handle
time-consuming queries, and provide high throughput for transaction processing.
This makes them essential for managing the growing data needs of modern
applications.

Transition: Let's delve deeper into how parallel databases meet the demands of
big data.

# ##############################################################################
# Parallel Databases
# ##############################################################################

The rise of the internet and big data has created a demand for large and fast
databases capable of storing petabytes of data and processing thousands of
transactions per second, such as those required by e-commerce websites.
Databases can be parallelized because their set-oriented queries are well-suited
for parallel processing. Some operations, like certain joins, are naturally
parallelizable and can be executed as MapReduce tasks. Parallel databases aim to
increase transactions per second or reduce query time, balancing throughput and
response time. However, perfect speedup is elusive due to start-up costs, task
interference, and data skew.

Transition: Now, let's explore how we can measure the performance of parallel
systems.

# ##############################################################################
# How to Measure Parallel Performance
# ##############################################################################

Parallel performance can be measured using throughput and latency. Throughput
refers to the number of tasks completed in a given time, which can be increased
by processing tasks in parallel. Latency is the time taken to complete a single
task from submission, which can be reduced by performing subtasks in parallel.
While throughput and latency are related, they are not the same. Throughput can
be increased by reducing latency or by pipelining, which involves overlapping
task execution. An example of pipelining is in microprocessor instructions,
where tasks are completed in a staggered manner to improve efficiency.

# ##############################################################################
# Speed-Up and Scale-Up: Intuition
# ##############################################################################

This slide explains how to handle workloads by adjusting computing power. When
you have a task to complete, you can change the workload size, like the number
of database transactions or the amount of data to query. To execute this
workload, you can either improve the CPU (scale up) or add more CPUs (scale
out). Efficiency is measured in two ways: speed-up, where you keep the workload
size constant and increase computing power, and scale-up, where both the
workload size and computing power are increased.

# ##############################################################################
# Speed-Up vs Scale-Up
# ##############################################################################

This slide compares speed-up and scale-up. Speed-up involves solving a
fixed-sized problem on a larger system, and it's measured by comparing the time
taken on a small system versus a larger one. If the speed-up is linear, the
result equals the increase in system size. Scale-up, on the other hand, involves
increasing both the problem size and system size. It's measured by comparing the
time taken for a small system-problem versus a big system-problem. A linear
scale-up means the result equals 1.

Let's now look at what limits these processes.

# ##############################################################################
# Factors Limiting Speed-up and Scale-up
# ##############################################################################

This slide discusses why speed-up and scale-up are often less than ideal. Not
all computations can be parallelized; some must be done sequentially. Amdahl's
Law is introduced to explain this limitation. It shows that the speed-up is
limited by the fraction of the task that can be parallelized and the number of
nodes used. For example, if 90% of a task is parallelizable, the maximum
speed-up is 10 times. If only 50% is parallelizable, the maximum speed-up is
just 2 times, even with infinite nodes.

# ##############################################################################
# Factors Limiting Speed-up and Scale-up
# ##############################################################################

Description of the slide

- Startup costs can slow down processes because they take time to get going. For
  example, databases need to set up threads when they start.
- Interference happens when processes fight over shared resources like system
  buses or disks, causing delays. Developers working on the same code can also
  create conflicts.
- Synchronization costs go up when tasks are broken into smaller pieces, making
  coordination harder. This is similar to managing many new hires in a company.
- Skew refers to the challenge of dividing tasks evenly, as uneven splits can
  lead to delays. The slowest task often sets the pace for completion.

Let's move on to how parallel systems are organized.

# ##############################################################################
# Topology of Parallel Systems
# ##############################################################################

Description of the slide

- There are different ways to set up computation and storage in parallel
  systems, involving memory, processors, and disks.
- Topologies include shared memory, shared disk, shared nothing, and
  hierarchical setups.
- Each topology has its own set of challenges, such as keeping cache data
  consistent, managing data communication, ensuring fault tolerance, and
  avoiding resource congestion.
- The images illustrate these different topologies and their characteristics.

Now, let's compare these topologies to understand their differences better.

# ##############################################################################
# Topology of Parallel Systems: Comparison
# ##############################################################################

Description of the slide

- The images provide a visual comparison of different parallel system
  topologies.
- Each topology has unique advantages and disadvantages, which are depicted in
  the images.
- The comparison helps in understanding which topology might be best suited for
  specific tasks or environments.
- Factors like data communication, fault tolerance, and resource management play
  a significant role in choosing the right topology.

This concludes our discussion on parallel systems and their topologies.

# ##############################################################################
# Distributed Databases
# ##############################################################################

Distributed databases are systems where the database is stored across multiple
nodes located in different geographical locations. These nodes communicate
through high-speed networks or the internet. The primary reasons for using
distributed databases include the need for redundancy, disaster recovery, and
maintaining high availability even during failures like natural disasters or
cyber-attacks. However, distributed databases are not typically used for
performance enhancement; parallel databases are better suited for that purpose.
Wide-area networks used in distributed databases have lower bandwidth and higher
latency compared to local-area networks, which can lead to network partition
issues. Additionally, distributed databases do not share memory or disks,
leading to communication delays, and the nodes can vary in size and function.

Transition: Now, let's delve into the consistency issues that arise in
distributed database systems.

# ##############################################################################
# Consistency Issues in Distributed DB Systems
# ##############################################################################

While parallel and distributed databases are efficient for query processing,
updating them requires consistency enforcement. One major issue is atomicity,
where a transaction must be completed entirely or not at all across multiple
nodes. The two-phase commit (2PC) is a centralized approach to handle this,
where a coordinator node decides whether to commit or abort a transaction based
on the readiness of each node. If a node fails, it can recover using write-ahead
logs. Distributed consensus methods like Paxos and Blockchain are also used.
Concurrency issues arise when multiple processes read and write simultaneously,
requiring locks and deadlock management. Autonomy issues occur when different
units or departments want to maintain control over their systems, affecting
system administration and updates.

Transition: Let's explore how parallel databases are designed and how they
handle data processing.

# ##############################################################################
# Parallel Databases
# ##############################################################################

Parallel databases are designed to enhance performance by distributing data
across multiple disks for parallel input/output operations. Relational
operations like sorting, joining, and aggregation can be executed in parallel,
with each processor working independently on its data partition. Queries are
expressed in SQL, which simplifies parallelization. Different queries can run
concurrently, with concurrency control managing conflicts. This natural
parallelism in databases allows for efficient data processing. I/O parallelism
reduces retrieval time by partitioning data across multiple disks. Techniques
like round-robin, hash, and range partitioning are used to distribute data, each
with its advantages and challenges, especially concerning range queries and data
clustering.

Transition: Next, we will discuss the various partitioning techniques and their
effectiveness in data access.

# ##############################################################################
# Comparison of Partitioning Techniques
# ##############################################################################

Partitioning techniques are evaluated based on their support for data access
types like scanning entire relations, locating tuples associatively, and
handling range queries. Round-robin partitioning is ideal for sequential scans
as it balances retrieval work across disks but struggles with range queries due
to scattered data. Hash partitioning is effective for sequential access and
point queries, as it evenly distributes tuples across disks, allowing efficient
local indexing. However, it lacks clustering, making range queries challenging.
Range partitioning offers data clustering, facilitating sequential access and
point queries on partitioning attributes. It can handle range queries
efficiently if the result tuples are from a few blocks, but execution skew can
occur if many blocks are fetched from limited disks.

Transition: Let's examine how relations are partitioned across disks and how
skew is managed.

# ##############################################################################
# Partitioning a Relation across Disks
# ##############################################################################

When partitioning a relation across disks, if the relation is small enough to
fit into a single disk block, it should be assigned to one disk. Larger
relations are better partitioned across all available disks. If a relation
consists of m disk blocks and there are n disks, it should be allocated to
min(m, n) disks. Skew in partitioning can occur, where some disks have more
tuples than others. Attribute-value skew happens when many tuples share the same
partitioning attribute value, leading to uneven distribution. Partition skew can
occur with range-partitioning if the partition vector is poorly chosen.
Hash-partitioning is less prone to skew if a good hash function is used.

Transition: Now, let's explore how skew is handled in range-partitioning using
balanced partitioning vectors and histograms.

# ##############################################################################
# Handling Skew in Range-Partitioning
# ##############################################################################

To handle skew in range-partitioning, a balanced partitioning vector is created
by sorting the relation on the partitioning attribute and scanning it in sorted
order. After reading every 1/nth of the relation, the partitioning attribute
value of the next tuple is added to the partition vector. This method assumes
the partitioning attribute forms a key of the relation. However, duplicates can
lead to imbalances. An alternative technique involves using histograms, which
assume uniform distribution within each range. Histograms can be constructed by
scanning the relation or sampling blocks containing tuples. This approach helps
create a balanced partitioning vector, reducing skew.

Transition: Let's discuss how virtual processor partitioning can elegantly
handle skew in range partitioning.

# ##############################################################################
# Handling Skew Using Virtual Processor Partitioning
# ##############################################################################

Virtual processor partitioning is an effective method to manage skew in range
partitioning. It involves creating a large number of partitions, significantly
more than the number of processors, and assigning virtual processors to these
partitions. This can be done in a round-robin fashion or based on the estimated
cost of processing each virtual partition. The idea is that any skew in normal
partitions is spread across multiple virtual partitions, distributing the
workload evenly across processors. This approach ensures that skewed virtual
partitions are spread across several processors, balancing the work and
preventing any single processor from being overloaded.

Transition: Now, let's explore interquery parallelism and how it increases
transaction throughput.

# ##############################################################################
# Interquery Parallelism
# ##############################################################################

Interquery parallelism allows multiple queries or transactions to execute
simultaneously, increasing transaction throughput. This approach is primarily
used to scale up transaction processing systems, enabling them to handle more
transactions per second. It is the easiest form of parallelism to support,
especially in shared-memory parallel databases, as even sequential database
systems support concurrent processing. However, implementing interquery
parallelism on shared-disk or shared-nothing architectures is more complex. It
requires coordination of locking and logging through message passing between
processors. Additionally, cache coherency must be maintained to ensure that
reads and writes access the latest data version.

Transition: Let's delve into cache coherency protocols and their role in shared
disk systems.

# ##############################################################################
# Cache Coherency Protocol
# ##############################################################################

Cache coherency protocols ensure that data in a local buffer reflects the latest
version in shared disk systems. Before reading or writing to a page, it must be
locked in shared or exclusive mode. Once locked, the page is read from the disk.
If modified, the page must be written back to the disk before unlocking. More
complex protocols exist to minimize disk reads and writes. In shared-nothing
systems, each database page is assigned a home processor, and requests to fetch
or write the page are sent to this processor. These protocols are crucial for
maintaining data consistency across processors.

Transition: Let's explore intraquery parallelism and its importance in speeding
up long-running queries.

# ##############################################################################
# Intraquery Parallelism
# ##############################################################################

Intraquery parallelism involves executing a single query in parallel across
multiple processors or disks, significantly speeding up long-running queries.
There are two complementary forms: intraoperation parallelism, which
parallelizes each individual operation in the query, and interoperation
parallelism, which executes different operations in a query expression
concurrently. Intraoperation parallelism scales better with increasing
parallelism, as the number of tuples processed by each operation is typically
more than the number of operations in a query. This approach is crucial for
efficiently processing large datasets and complex queries, leveraging the full
potential of parallel processing.

Transition: Let's discuss parallel processing of relational operations and the
assumptions involved.

# ##############################################################################
# Parallel Processing of Relational Operations
# ##############################################################################

Parallel processing of relational operations assumes read-only queries and a
shared-nothing architecture with n processors and disks. If a processor has
multiple disks, they simulate a single disk. Shared-nothing architectures can be
efficiently simulated on shared-memory and shared-disk systems, allowing
algorithms for shared-nothing systems to run on these architectures. However,
some optimizations may be possible. Parallel processing involves executing
operations like sorting, joining, and aggregation in parallel, leveraging
multiple processors and disks to enhance performance and efficiency. This
approach is essential for handling large datasets and complex queries in a
timely manner.

Transition: Let's explore parallel sorting techniques and their implementation.

# ##############################################################################
# Parallel Sort
# ##############################################################################

Parallel sorting involves distributing the sorting task across multiple
processors to enhance efficiency. The range-partitioning sort selects processors
to perform sorting and creates a range-partition vector on the sorting
attributes. The relation is redistributed using range partitioning, with tuples
sent to the appropriate processor. Each processor sorts its partition locally,
executing the same operation in parallel with others, achieving data
parallelism. The final merge operation is straightforward, as range-partitioning
ensures sorted key values across processors. Parallel external sort-merge
assumes the relation is already partitioned among disks, with each processor
sorting its data locally and merging sorted runs to produce the final output.

Transition: Let's delve into parallel join operations and their significance.

# ##############################################################################
# Parallel Join
# ##############################################################################

Parallel join operations involve testing pairs of tuples to see if they satisfy
the join condition and adding them to the join output. Parallel join algorithms
distribute the pairs to be tested across several processors, with each processor
computing part of the join locally. In the final step, results from each
processor are collected to produce the final result. Partitioned join is used
for equi-joins and natural joins, where input relations are partitioned across
processors, and the join is computed locally. Fragment-and-replicate join is
used when partitioning is not possible, distributing the workload across
processors to achieve parallelization.

Transition: Let's explore partitioned join techniques and their implementation.

# ##############################################################################
# Partitioned Join
# ##############################################################################

Partitioned join techniques involve partitioning input relations across
processors to compute joins locally. For equi-joins and natural joins, relations
are partitioned on their join attributes using range or hash partitioning.
Partitions are sent to processors, where the join is computed locally using
standard join methods. This approach efficiently distributes the workload across
processors, enhancing performance and scalability. Partitioned join is
particularly effective for handling large datasets and complex join operations,
leveraging the full potential of parallel processing to achieve timely and
accurate results.

Transition: Let's discuss fragment-and-replicate join techniques and their
application.

# ##############################################################################
# Fragment-and-Replicate Join
# ##############################################################################

Fragment-and-replicate join techniques are used when partitioning is not
feasible for certain join conditions, such as non-equijoin conditions. In
asymmetric fragment-and-replicate, one relation is partitioned, and the other is
replicated across processors. Each processor computes the join locally using any
join technique. The general case reduces relation sizes at each processor by
partitioning both relations and replicating them across processors. This
approach works with any join condition, as every tuple in one relation can be
tested with every tuple in the other. While it usually has a higher cost than
partitioning, it can be preferable when one relation is small and the other is
large.

Transition: Let's explore partitioned parallel hash-join techniques and their
implementation.

# ##############################################################################
# Partitioned Parallel Hash-Join
# ##############################################################################

Partitioned parallel hash-join techniques involve parallelizing the hash join
process. The smaller relation is chosen as the build relation, and a hash
function maps its tuples to processors. Each processor receives tuples,
partitions them using another hash function, and computes the hash-join locally.
The larger relation is then redistributed across processors using the same hash
function. Each processor executes the build and probe phases of the hash-join
algorithm on local partitions to produce a partition of the final result.
Hash-join optimizations, like the hybrid hash-join algorithm, can be applied to
cache incoming tuples in memory, reducing I/O costs.

Transition: Let's discuss parallel nested-loop join techniques and their
application.

# ##############################################################################
# Parallel Nested-Loop Join
# ##############################################################################

Parallel nested-loop join techniques are used when one relation is much smaller
than the other and stored by partitioning. An index on the join attribute of the
larger relation is used at each partition. Asymmetric fragment-and-replicate is
employed, with the smaller relation replicated across processors. Each processor
performs an indexed nested-loop join of the smaller relation with its partition
of the larger relation. This approach efficiently distributes the workload
across processors, leveraging parallel processing to handle large datasets and
complex join operations, achieving timely and accurate results.

Transition: Let's explore other relational operations and their parallel
processing.

# ##############################################################################
# Other Relational Operations
# ##############################################################################

Selection operations can be parallelized based on the selection condition. If
the condition is an equality on a partitioned attribute, the selection is
performed at a single processor. For range selections on range-partitioned
attributes, selection occurs at processors with overlapping partitions. In other
cases, selection is parallelized across all processors. Duplicate elimination
can be performed using parallel sort techniques, eliminating duplicates during
sorting. Projection without duplicate elimination is done as tuples are read
from disk in parallel. If duplicate elimination is needed, parallel sort
techniques are used. Grouping and aggregation involve partitioning the relation
on grouping attributes and computing aggregates locally, reducing tuple transfer
costs.

Transition: Let's discuss the cost of parallel evaluation of operations and
factors affecting it.

# ##############################################################################
# Cost of Parallel Evaluation of Operations
# ##############################################################################

The cost of parallel evaluation of operations depends on partitioning skew and
overheads. Without skew and overheads, expected speed-up is 1/n. Considering
skew and overheads, the time taken by a parallel operation is estimated as the
sum of partitioning time, assembly time, and the maximum time taken by any
processor. Partitioning time involves distributing relations, assembly time
involves combining results, and processor time accounts for skew and contention.
Efficient parallel processing requires minimizing skew and overheads to achieve
optimal performance and scalability, leveraging the full potential of parallel
processing to handle large datasets and complex operations.

Transition: Let's explore interoperator parallelism and its significance in
database processing.

# ##############################################################################
# Interoperator Parallelism
# ##############################################################################

Interoperator parallelism involves executing multiple operations in a query
expression concurrently. Pipelined parallelism sets up a pipeline for
operations, allowing them to execute in parallel and send results to the next
operation. This approach avoids writing intermediate results to disk, enhancing
efficiency. However, it doesn't scale well with more processors due to limited
pipeline chain length. Independent parallelism assigns operations to processors
independently, allowing them to work in parallel. While it doesn't provide high
parallelism, it's useful with a lower degree of parallelism. Combining
independent and pipelined parallelism can enhance performance, leveraging
parallel processing to handle complex queries efficiently.
