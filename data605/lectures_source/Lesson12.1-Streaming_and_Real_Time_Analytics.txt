// Dir is https://drive.google.com/drive/folders/1u8ZUAkLc8yZBwGgXvfBcAY_oSCyzT_pp
//
//

::: columns
:::: {.column width=15%}
![](data605/lectures_source/images/UMD_Logo.png)
::::
:::: {.column width=75%}

\vspace{0.4cm}
\begingroup \large
UMD DATA605 - Big Data Systems
\endgroup
::::
:::

\vspace{1cm}

\begingroup \Large
**$$\text{\blue{12.1: Streaming and Real-time Analytics}}$$**
\endgroup

::: columns
:::: {.column width=75%}
- **Instructor**: Dr. GP Saggese, [gsaggese@umd.edu](gsaggese@umd.edu)

::::
:::: {.column width=20%}
::::
:::

* Motivation
- Big Data is generated as a **continuous, unbounded stream**

- Applications generate data at **high velocity**
  - Financial transactions and market feeds
  - Sensor instrumentation, RFID, IoT telemetry
  - Network and system monitoring
  - Continuous media (video, audio)

- A **data stream** is a time-ordered sequence of events
  - Stream processing treats streams as first-class computational objects

- **Requirements**
  - Ingest and handle high-throughput event streams
  - Low-latency, near-real-time operations (e.g., time-series analytics)
  - Efficient dissemination of relevant subsets to consumers
  - Distributed processing to scale beyond a single machine

* Examples of Data Stream Tasks
- Continuous queries
  - Any SQL query can be continuous
  - E.g., _"compute moving average over last hour every 10 mins"_

- Anomaly detection, pattern recognition
  - E.g., _"alert me when A occurs and then B within 10 mins"_
  - Correlate events from different streams

- Statistical tasks
  - E.g., de-noising measured readings
  - Build an online machine learning model

- Process multimedia data
  - E.g., online object detection, activity detection

* Why Not Using the Usual RDBM?
- **Example**
  - "_Report moving average over last hour every 10 minutes_"

- **Solution**
  - Insert arriving items into a relational table
  - Re-run query repeatedly

:::columns
::::{.column width=60%}
- **Problems**
  - Re-executes full query instead of leveraging incremental updates
  - Many streaming computations are recursive by nature
  - Complex computations may not be easily expressed incrementally
  - Real systems may need to run thousands of such continuous queries
::::
::::{.column width=40%}
\centering \tiny
![](data605/lectures_source/images/lecture_12_1/lec_12_1_slide_4_image_3.png)

![](data605/lectures_source/images/lecture_12_1/lec_12_1_slide_4_image_1.png){width=60%}

Computation

![](data605/lectures_source/images/lecture_12_1/lec_12_1_slide_4_image_2.png){width=80%}
::::
:::

* Pub-Sub Systems
- Modern distributed systems use small, independent components
  - E.g., serverless architectures, microservices (e.g., Uber)
  - Easier evolution, isolation, scalability

:::columns
::::{.column width=65%}
- **Publish-subscribe (pub-sub) systems**
  - Aka "message queues", "message brokers"
  - Connect producers and consumers for event distribution
  - Topics cluster related messages
  - Typically provide lightweight dissemination rather than complex queries
  - Examples: AWS SQS, Kinesis, Kafka, RabbitMQ, Redis Streams, Celery, JBoss
::::
::::{.column width=35%}
![](data605/lectures_source/images/lecture_12_1/lec_12_1_slide_5_image_1.png)
::::
:::

* Pub-Sub Systems
:::columns
::::{.column width=45%}
- **Publishers**
  - Send messages or events

- **Subscribers**
  - Consume messages

- **Message broker**
  - Message broker routes event flow between publishers and subscribers, based on
    topics and subscriptions

- **Design parameters**
  - Event distribution model (topics, filters)
  - Push vs pull consumption
  - Subscriber interest patterns
  - Delivery guarantees
    - At-most-once
    - At-least-once
    - Exactly-once
::::
::::{.column width=50%}
![](data605/lectures_source/images/lecture_12_1/lec_12_1_slide_6_image_1.png)
::::
:::

* Delivery Semantics
:::columns
::::{.column width=60%}
- **At-most once**
  - Message may be lost, not redelivered
  - Pros
    - High-performance
    - Small implementation overhead
    - Easy to implement: "fire-and-forget"
  - E.g., monitoring metrics allow small data loss

- **At-least once**
  - Deliver message more than once, no message lost
  - Handle transport message loss
    - Keep state at sender
    - Acknowledge state at receiver
  - Works if
    - Data duplication is acceptable
    - Deduplication (e.g., storing key-value)
    - Idempotency

- **Exactly once**
  - Every message sent once
  - Friendly for downstream consumers
  - Difficult to implement
    - Two Generals' Problem
  - E.g., mission-critical systems
  - E.g., financial use cases (e.g., payment, trading, accounting)
::::
::::{.column width=40%}
![](data605/lectures_source/images/lecture_12_1/lec_12_1_slide_7_image_1.png)

\vspace{1cm}

![](data605/lectures_source/images/lecture_12_1/lec_12_1_slide_7_image_2.png)

\vspace{1.5cm}

![](data605/lectures_source/images/lecture_12_1/lec_12_1_slide_7_image_3.png)
::::
:::

* Event vs Processing Time
:::columns
::::{.column width=60%}
- In both streaming and pub-sub architectures

- **Event time**
  - Time when each record is generated

- **Processing time**
  - Time when each record is received
  - Ingestion vs processing time: when events are received vs processed

- **Problems with events**
  - Out of order
  - Tardiness
  - How long to wait for late data?
    - In an asynchronous system, never sure all data has arrived
    - Use bounds on delay
    - Assume data arrives every minute
  - Recompute once late data arrives? If not, drop late data?
::::
::::{.column width=50%}
![](data605/lectures_source/images/lecture_12_1/lec_12_1_slide_8_image_1.png)
::::
:::

* Apache Streaming Zoo
- Many different streaming frameworks
  - Apache Apex, Apache Beam, Apache Flink, Apache Kafka, Apache Spark, Apache
    Storm, Apache NiFi

- Use cases
  - Real-time analytics
  - Online machine learning
  - Continuous computation
  - ETL processes
  - Data pipeline processing
  - Messaging
  - Log aggregation

- Different solutions to the same problem
  - Batch vs streaming
  - Delivery semantic type
  - Computing vs messaging/pub-sub
  - Throughput vs fault-tolerance
  - Supported languages

- Built simultaneously at different companies, then open-sourced

* Apache Storm
- Open-source distributed real-time computation system
  - Acquired and open-sourced by Twitter

- Horizontal scalability: add machines to handle increasing data

- Fault tolerance: **at-least-once** processing, automatic task restarts,
  workload redistribution

- Directed acyclic graph (DAG) with:
  - spouts (data sources)
  - bolts (processing units) as vertices
  - data streams as edges

- Suitable for complex data processing workflows with multiple stages and
  parallelism

![](data605/lectures_source/images/lecture_12_1/lec_12_1_slide_10_image_1.png)

* Apache Kafka
:::columns
::::{.column width=75%}
- Open-source distributed streaming platform
  - Developed at LinkedIn, open-sourced in 2011

- Producers, brokers, consumers, topics, partitions

- Persistent storage, data replication across brokers

- High-throughput, low-latency messaging

- Fault tolerance: broker replication, automatic recovery

- Message delivery semantics:
  - **At-least-once, at-most-once, exactly-once**

- Kafka Connect: Integrate with data sources and sinks

- Kafka Streams: Stream processing library on Kafka
::::
::::{.column width=25%}
![](data605/lectures_source/images/lecture_12_1/lec_12_1_slide_11_image_1.png){width=30%}

![](data605/lectures_source/images/lecture_12_1/lec_12_1_slide_11_image_2.png)
::::
:::

* Apache Flink
- Open-source, distributed data processing framework

- Focus on stateful computations over data streams

- Scalability: Horizontal scaling across large clusters

- Fault tolerance:
  - **Exactly-once** processing semantics, checkpointing, state management

- Batch processing support: **unified API for stream and batch processing**

- Flexible windowing: time-based, count-based, session windows

- Deployment options: standalone, YARN, Mesos, Kubernetes, cloud environments

![](data605/lectures_source/images/lecture_12_1/lec_12_1_slide_12_image_1.png)

* Record-at-a-time Processing
:::columns
::::{.column width=60%}
- Implemented in Apache Kafka

- Goal: handle endless data stream

- **Multiple-node distributed processing engine**
  - Map computation on DAG of nodes
  - Each node continuously
    - Receives one record
    - Processes it
    - Forwards to next node

- **Pros**
  - Very low latencies
    - E.g., less than msecs

- **Cons**
  - Inefficient node failure recovery
    - E.g., need failover resources/redundancy
  - Straggler nodes (slower than others)
::::
::::{.column width=35%}
![](data605/lectures_source/images/lecture_12_1/lec_12_1_slide_13_image_1.png)
::::
:::

* Micro-Batch Stream Processing
:::columns
::::{.column width=60%}
- Aka DStreams

- Implemented in Spark Streaming

- **Computation as a continuous series of batch jobs on small chunks of stream
  data**
  - E.g., 1 second
  - Process each batch in the Spark cluster in a distributed manner

- **Pros**
  - Recover from failures and stragglers using task scheduling
    - E.g., schedule same task multiple times
  - Deterministic tasks
    - Exactly-once processing guarantees
    - Consistent API: same functional semantics as RDDs
    - Fault-tolerance

- **Cons**
  - Higher latency
    - E.g., seconds
::::
::::{.column width=35%}
![](data605/lectures_source/images/lecture_12_1/lec_12_1_slide_14_image_1.png)
::::
:::

* Spark Micro-Batch Processing: Cons
- Line between real-time and batch processing blurred
  - Application computing data hourly: stream or batch?

- No single API for batch and stream processing
  - Same abstractions (RDD) and operations
  - Rewrite code with different classes

- No support for event-time windows
  - Operations defined by processing time
  - No support for tardy data

- Spark replaced DStreams with Structured Streaming in v3
  - Supports micro-batch and continuous streaming
  - Closer batch vs streaming API

* Spark Structured Streaming
:::columns
::::{.column width=60%}
- New approach used by Spark

- Goal: write stream processing as easy as writing batch pipelines
  - Single unified programming model
  - Use SQL or DataFrames on stream

- Handle automatically
  - Fault tolerance
  - Optimizations
  - Incremental computation
  - Tardy data

- **Data abstraction**
  - Batch applications: table (DataFrame) is the abstraction
  - Structured Streaming: table is an unbounded, continuously appended table
    - New record becomes a new row appended
    - At time $T$, it's like a static dataframe with data until $T$
::::
::::{.column width=40%}
![](data605/lectures_source/images/lecture_12_1/lec_12_1_slide_16_image_1.png)
::::
:::

* Incrementalization
:::columns
::::{.column width=50%}
- Automatically detect state to maintain
  - Build DAG of computation
  - Express output of graph at time T in terms of graph at time T-1
  - Cache results

- Developers specify triggers to update results

- Incrementally update result with each record arrival
::::
::::{.column width=45%}
![](data605/lectures_source/images/lecture_12_1/lec_12_1_slide_17_image_1.png)
::::
:::

* Triggering Modes
- Indicate when to process newly available streaming data
  - **Default**
    - Process micro-batch after previous completes
  - **Trigger interval**
    - Specify fixed interval for each micro-batch
    - E.g., "every 10 minutes"
  - **Once**
    - Wait for external trigger
    - E.g., "at end of day"
  - **Continuous (experimental)**
    - Process data continuously
    - Not all operations available
    - Lower latency

* Saving Data
- Each time result table updates, write to external file system (e.g., HDFS, AWS
  S3) or DB (e.g., MySQL, Cassandra)
  - **Append mode**
    - Append new rows since last trigger
    - Use when existing rows don't change
  - **Update mode**
    - Write updated rows since last trigger
    - Update in place
  - **Complete mode**
    - Write entire updated result table
    - General but expensive

* Spark Streaming "Hello world"
:::columns
::::{.column width=50%}
- `lines` looks like an RDD but it's a `DataStreamReader`
  - Unbounded DataFrame
  - Set up reading but doesn't start reading

- `words` split data in words

- `counts` is a streaming DataFrame
  - Running word count

- Stateless transformations don't require maintaining state
  - E.g., `select()`, `filter()`

- Stateful transformations
  - E.g., `count()`

- Define how to write processed output
  - Where to write (e.g., `console`)
  - How to write (e.g., `complete` for updated word counts)

- When to trigger computation
  - E.g., every 1 second

- Where to save metadata for:
  - Exactly-once guarantees
  - Failure recovery

- `start()` processing (non-blocking)

- `awaitTermination()` blocks until data is available
::::
::::{.column width=50%}
![](data605/lectures_source/images/lecture_12_1/lec_12_1_slide_20_image_1.png)
::::
:::
