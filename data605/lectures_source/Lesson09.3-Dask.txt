// Dir is https://drive.google.com/drive/folders/1u8ZUAkLc8yZBwGgXvfBcAY_oSCyzT_pp
// 
// 

::: columns
:::: {.column width=15%}
![](data605/lectures_source/images/UMD_Logo.png)
::::
:::: {.column width=75%}

\vspace{0.4cm}
\begingroup \large
UMD DATA605 - Big Data Systems
\endgroup
::::
:::

\vspace{1cm}

\begingroup \Large
**$$\text{\blue{9.3: Python Dask}}$$**
\endgroup

::: columns
:::: {.column width=75%}
- **Instructor**: Dr. GP Saggese, [gsaggese@umd.edu](gsaggese@umd.edu)

- **Resources**
  - Web resources:
    - [Dask project](https://docs.dask.org/en/stable/)
    - [Dask examples](https://examples.dask.org/)
  - Tutorial
    - [Dask\_tutorial](https://github.com/gpsaggese-org/umd_classes/blob/master/data605/tutorials/tutorial_dask/Dask_tutorial.ipynb)
    - [Dask\_advanced\_tutorial](https://github.com/gpsaggese-org/umd_classes/blob/master/data605/tutorials/tutorial_dask/Dask_advanced_tutorial.ipynb)
  - Class project
  - Mastery
    - Data science with Python and Dask, 2019
::::
:::: {.column width=20%}
![](data605/lectures_source/images/lecture_9_2/lec_9_2_slide_2_image_1.png)
::::
:::

* Dataset Size Issues
::: columns
:::: {.column width=75%}
- **Small datasets (< 1 GB)**
  - Fits into RAM
  - No disk paging needed

- **Medium dataset (< 1TB)**
  - Doesn't fit into RAM
  - Fits into local disk
    - Performance penalty with local disk
  - Need multiple CPU cores
    - Difficult to leverage parallelism with Python/Pandas

- **Large dataset (> 1TB)**
  - Doesn't fit into RAM
  - Doesn't fit into local disk
  - Need multiple servers
    - Python/Pandas not built for distributed datasets
    - Use frameworks for massive datasets
    - E.g., Hadoop, Spark, Dask, Ray
::::
:::: {.column width=20%}
![](data605/lectures_source/images/lecture_9_2/lec_9_2_slide_3_image_3.png)

\vspace{1cm}

![](data605/lectures_source/images/lecture_9_2/lec_9_2_slide_3_image_4.png)

\vspace{1cm}

![](data605/lectures_source/images/lecture_9_2/lec_9_2_slide_3_image_1.png)

![](data605/lectures_source/images/lecture_9_2/lec_9_2_slide_3_image_2.png)
::::
:::

* Dataset Size Issues

| **Category**    | **Size**    |
|-----------------|-------------|
| Small datasets  | < 1 GB      |
| Medium datasets | < 1 TB      |
| Large datasets  | > 1 TB      |

- **The thresholds are fuzzy and changing over time**
  - Scale computer 10x to get 10x bigger datasets

- **Problem with scaling datasets**
  - Long run times
  - Rewriting code for different dataset sizes
  - Plan what and how to do efficiently
  - Cumbersome framework (Pandas easy, Hadoop difficult)

* Dask
::: columns
:::: {.column width=60%}
- **Dask is written in Python**
  - Scales Numpy, Pandas, sklearn
  - Dask objects wrap library objects (e.g., Pandas DataFrame, numpy array)
  - Parallel parts are "chunks" or "partitions"
    - Queued for work
    - Shipped between machines
    - Worked locally
::::
:::: {.column width=35%}
![](data605/lectures_source/images/lecture_9_2/lec_9_2_slide_5_image_1.png)
::::
:::

- **Pros**
  - Use familiar interfaces
  - Write code optimized for parallelism
    - Dask handles heavy lifting

- **Scaling Dask is easy**
  - Prototype on local machine, use cluster when needed
  - No code refactoring needed
  - No cluster-specific issues
    - E.g., resource management, data recovery, data movement
  - Runs on multi-core
  - Uses cluster managers
    - E.g., Yarn, Mesos, Kubernetes, AWS ECS

* Dask Layers

![](data605/lectures_source/images/lecture_9_2/lec_9_2_slide_6_image_1.png)

* Scaling Up vs Scaling Out
::: columns
:::: {.column width=80%}
- **Scaling up**
  - Replace equipment with larger, faster options
    - E.g., buy a larger pot, replace knife with food processor
  - **Pros**
    - Better hardware, no code changes needed
  - **Cons**
    - Exceed current machine capacity eventually
    - Cost: more powerful machines are expensive
::::
:::: {.column width=15%}
![](data605/lectures_source/images/lecture_9_2/lec_9_2_slide_7_image_1.png)
::::
:::

\vspace{1cm}

::: columns
:::: {.column width=80%}
- **Scaling out**
  - Divide work between many workers in parallel
    - E.g., buy more pots and hire more cooks
  - **Pros**
    - Task scheduler organizes computation, assigns workers to tasks
    - Cost-effective, no specialized hardware needed
  - **Cons**
    - Write code to expose parallelism
    - Maintain cluster costs
::::
:::: {.column width=15%}
![](data605/lectures_source/images/lecture_9_2/lec_9_2_slide_7_image_2.png)
::::
:::

* Dask: Computation
- **Lazy computations**
  - Define transformations on data
  - Define next computation without waiting
  - Operate in chunks to avoid loading entire data in memory
  - E.g.,
    - Split 2GB file into 32 64MB chunks
    - Operate on 8 chunks per server
    - Max memory use: 512MB = (8 x 64MB)
  - Track object dimensions and data types
    - No code execution

- `compute()`
  - Run computation (materialize)
    ```python
    missing_count_pct = missing_count.compute()
    ```

- `persist()`
  - Discard intermediate work to minimize memory
  - Re-run graph for additional computation on intermediate nodes
  - `persist()` keeps intermediate result in memory
  - Speeds up large, complex DAGs for reuse


* Dask: Data Structures
::: columns
:::: {.column width=75%}
- **Dask DataFrame**
  - Implements Pandas DataFrame
  - Tabular/relational data
::::
:::: {.column width=20%}
![](data605/lectures_source/images/lecture_9_2/lec_9_2_slide_9_image_2.png)
::::
:::

\vspace{1cm}

::: columns
:::: {.column width=65%}
- **Dask Array**
  - Implements numpy ndarray
  - Multidimensional array
::::
:::: {.column width=30%}
![](data605/lectures_source/images/lecture_9_2/lec_9_2_slide_9_image_1.png)
::::
:::

\vspace{1cm}

::: columns
:::: {.column width=65%}
- **Dask Bag**
  - Coordinates Python lists of objects
  - Parallelize computations on unstructured/semi-structured data
::::
:::: {.column width=30%}
![](data605/lectures_source/images/lecture_9_2/lec_9_2_slide_9_image_3.png)
::::
:::

* Dask Reading Data
::: columns
:::: {.column width=40%}
- dask.dataframe.read_csv()
  - Doesn't load the data in memory with 
  - Tries to infer the types of the columns
    - By randomly sampling some data
    - Best to set the data types
    - Even better is to use Parquet since it stores data and types together
- Partitions = chunks of data that can be worked independently
  - E.g., 33 partitions
  - Graph is composed of 99 tasks
  - Each partition reads data, splits data, initializes df object

```python
  import dask.dataframe as dd

  df = dd.read_csv('nyc-parking-tickets-2017.csv')

  missing_values = df.isnull().sum()

  missing_values 
```
::::
:::: {.column width=40%}
![](data605/lectures_source/images/lecture_9_2/lec_9_2_slide_10_image_1.png)
::::
:::: {.column width=20%}
![](data605/lectures_source/images/lecture_9_2/lec_9_2_slide_10_image_2.png)
::::
:::

* Low Level APIs: Delayed
::: columns
:::: {.column width=40%}
- Handle computations that don't fit in native Dask data structures (e.g., Dask DataFrame)
- In the example below there is parallelism that can be exploited
![](data605/lectures_source/images/lecture_9_2/lec_9_2_slide_11_image_1.png)
::::
:::: {.column width=40%}

![](data605/lectures_source/images/lecture_9_2/lec_9_2_slide_11_image_2.png) 
::::
:::

* Low Level APIs: Futures
::: columns
:::: {.column width=40%}
- In parallel programming, a "future" encapsulates the asynchronous execution of a callable, representing the eventual result of the operation
- Futures is the most general way of specifying concurrency in Dask
  - Everything can be expressed in terms of futures
  - User can specify what's blocking and what's not blocking
- Python** concurrent.futures**
  - High-level interface for asynchronously executing callables
  - Thread pool or Process pool (same interface **Executor**)
- Dask extends **concurrent.futures**
  - Dask client can be used anywhere **concurrent.futures** can be used
::::
:::: {.column width=40%}
![](data605/lectures_source/images/lecture_9_2/lec_9_2_slide_12_image_1.png)

![](data605/lectures_source/images/lecture_9_2/lec_9_2_slide_12_image_2.png)

![](data605/lectures_source/images/lecture_9_2/lec_9_2_slide_12_image_3.png)

![](data605/lectures_source/images/lecture_9_2/lec_9_2_slide_12_image_4.png)
::::
:::

* Different Types of Parallel Workload
::: columns
:::: {.column width=50%}
- Break program in medium-size tasks of computation
  - E.g., a function call
![](data605/lectures_source/images/lecture_9_2/lec_9_2_slide_13_image_2.png)
::::
:::: {.column width=50%}
![](data605/lectures_source/images/lecture_9_2/lec_9_2_slide_13_image_1.png)
![](data605/lectures_source/images/lecture_9_2/lec_9_2_slide_13_image_3.png)
::::
:::

* Encoding Task Graph

- Dask encodes tasks in terms of Python dicts and functions
::: columns
:::: {.column width=33%}
![](data605/lectures_source/images/lecture_9_2/lec_9_2_slide_14_image_1.png)
![](data605/lectures_source/images/lecture_9_2/lec_9_2_slide_14_image_4.png)
::::
:::: {.column width=33%}
![](data605/lectures_source/images/lecture_9_2/lec_9_2_slide_14_image_2.png)
![](data605/lectures_source/images/lecture_9_2/lec_9_2_slide_14_image_5.png)
::::
:::: {.column width=33%}
![](data605/lectures_source/images/lecture_9_2/lec_9_2_slide_14_image_3.png)
::::
:::

* Task Scheduling
- Data collections (Bags, Arrays, DataFrame) and their operations create task graphs
  - Nodes in the task graph are Python functions
  - Edges are dependencies (e.g., output from one task used as input in another task)
- Task graphs are scheduled for execution
- Single-machine scheduler
  - Use local process or thread pool
  - Simple but it can only run on a single machine
- Distributed scheduler
  - It can run locally or distributed across a cluster

![](data605/lectures_source/images/lecture_9_2/lec_9_2_slide_15_image_1.png) \footnotesize


* Task Scheduling
::: columns
:::: {.column width=40%}
- **Dask task scheduler orchestrates the work dynamically**
  - Not a static scheduling of operations like a relational DB
  - When the computation takes place, Dask dynamically assesses:
    - What tasks has been completed
    - What tasks is left to do
    - What resources (CPUs) are free
    - Where the data is located
- **This dynamic approach handles a variety issues:**
  - Worker failure
    - Just re-run
  - Workers completing work at different speeds because of:
    - Different computation
    - Different hardware
    - Different workloads on the servers
    - Slower access to the data
  - Network unreliability
    - Just re-run or remove the isolated nodes
::::
:::: {.column width=40%}
![](data605/lectures_source/images/lecture_9_2/lec_9_2_slide_16_image_1.png)
::::
:::
* Dask vs Spark
- Spark has
- **Pros**
  - Popular framework for analyzing large datasets
  - In-memory alternative to MapReduce / Hadoop
- **Cons**
  - Spark is a Java library, supporting Python through PySpark API
    - Python code is executed on JVM through `py4j`
    - Difficult to debug since execution occurs outside Python
  - Different DataFrame API than Pandas
    - Learn how to do things "the Spark way"
    - You might need to implement things twice to go from exploratory analysis to large experiments / production
  - Optimized for MapReduce operations over a collection
  - Difficult to set-up and configure

* Tutorial
Tutorial
- From the official documentation
https://docs.dask.org/en/stable/10-minutes-to-dask.html
