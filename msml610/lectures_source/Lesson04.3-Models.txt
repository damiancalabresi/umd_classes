
::: columns
:::: {.column width=15%}
![](msml610/lectures_source/figures/UMD_Logo.png)
::::
:::: {.column width=75%}

\vspace{0.4cm}
\begingroup \large
MSML610: Advanced Machine Learning
\endgroup
::::
:::

\vspace{1cm}

\begingroup \Large
**$$\text{\blue{Lesson 04.3: Machine Learning Models}}$$**
\endgroup

::: columns
:::: {.column width=75%}
\vspace{1cm}

**Instructor**: Dr. GP Saggese, [gsaggese@umd.edu](gsaggese@umd.edu)

**References**:

- Burkov: _"Machine Learning Engineering"_ (2020)

- Hastie et al.: _"The Elements of Statistical Learning"_ (2nd ed, 2009)

::::
:::: {.column width=20%}

![](msml610/lectures_source/figures/book_covers/Book_cover_Hundred_page_ML_book.jpg){ height=20% }

![](msml610/lectures_source/figures/book_covers/Book_cover_The_Elements_of_Statistical_Learning.jpg){ height=20% }

::::
:::



## #############################################################################
## Similarity-Based Models
## #############################################################################

* Similarity-Based Models: Intuition
- **Idea**: the model evaluated in one point $h(\vx)$ is affected by:
  - Other data points in the training set $(\vx_n, y_n) \in \calD$
  - The effect is based on the distance $d(\vx, \vx_n) = \|\vx - \vx_n\|$

- The model is a **superposition of effects**
  - Sum of the effect of each point in the training set, scaled down by the
    distance
    $$
    h(\vx)
    = \sum_i \text{ effect of } h(\vx_i) \text{ scaled by } d(\vx, \vx_i)
    $$

- This approach allows to define complex decision boundaries

* Similarity-Based Models: Gaussian Kernels
- Consider a Gaussian kernel with a "landmark" point $\vx_i$ and a similarity
  distance defined as:
  $$
  K(\vx, \vx_i) = \exp(- \frac{\| \vx - \vx_i \| ^ 2}{2 \sigma^2})
  $$

- E.g., the hypothesis model has the form:
  $$
  h(\vx)
  = \sum_{i=1}^3 y_i K(\vx, \vx_i)
  = y_1 K(\vx, \vx_1) + y_2 K(\vx, \vx_2) + y_3 K(\vx, \vx_3)
  $$
  - The response is weighting the responses $y_i = \{0, 1\}$ through the
    similarity of $\vx$ from the landmark points
  - This can be seen by plotting $h(\vx)$ on a plane

// TODO: Add plot

* Radial Basis Function Model for Regression
- Aka RBF

- The model form for **regression** is:
  $$
  h(\vx) = \sum_{i=1}^{N} w_i \exp(-\gamma \|\vx - \vx_i\| ^2)
  $$
  where:
  - If $\gamma$ is small $\implies$ the exponential falls off slowly, and
    multiple training points affect a point between them
  - If $\gamma$ is large $\implies$ there are spikes centered in the training
    points and nothing outside

// TODO: Add plot

* Radial Basis Function Model for Classification
- For **classification** use a similar approach to "linear regression for
  classification"
  - Fit a regression model:
    $$
    s(\vx) = \sum_{i=1}^{N} w_i \exp(-\gamma \|\vx - \vx_i\| ^2)
    $$
  - Take the sign to make predictions:
    $$
    h(\vx) = \sign(s(\vx))
    $$

* RBF: Block Diagram
::: columns
:::: {.column width=50%}

- The params(fixed by learning) are one for each training point
  - The weights depending on the distance of the input to the examples
  - The weighted params are summed together

::::
:::: {.column width=50%}

![](msml610/lectures_source/figures/Lesson04_RBF_Model.png)

::::
:::

// TODO: Create a better diagram

* RBF: Number of Parameters
- Some variants for RBF:
  - Add bias term
  - Use different $\gamma_i$ for each point
  - Increase degrees of freedom

- RBF has lots of parameters
  - Parameters $\vw$ equal data points $N$ (e.g., $N = 10^{9}$)
  - One parameter $w_i$ per training point (e.g., $N = 10^6$)
  - **Cons**: Negative impact on generalization error

* RBF: Reducing Model VC Dimension
- **To reduce number of parameters**
  - Pick $K \ll N$ centers $\vmu_1, ... , \vmu_K$ instead of
    $\vx_1, ... , \vx_N$
    - Use $k$-means clustering to find centers
    - Unsupervised learning; doesn't use labels
  - Same as RBF model using distances from cluster centers:
    $$
    h(\vx) = \sum_{i=1}^{K} w_i \exp (-\gamma \|\vx - \vmu_i\|^2)
    $$

- **Still many parameters** because:
  - $K$ (scalar) weights $w_k$
  - $K$ reference points $\vmu_k$ ($d$-dimensional vectors)

* RBF: Learning Models (1/2)
- **Learn** $w_i, \gamma$, with fixed centers $\vmu_i$, for an RBF model:
  $$
  h(\vx) = \sum_{i=1}^{K} w_i \exp (-\gamma \|\vx - \vmu_i\|^2)
  $$

- **Minimize:**
  $$
  E_{in} = \sum_i (h_{\vw, \gamma}(\vx_i) - y_i)^2 = f(\vw, \gamma)
  $$
  - Use an iterative approach (e.g., EM, coordinate descent)

* RBF: Learning Models (2/2)
- Use iterative approach (similar to EM algorithm):
  - Fix $\gamma$, solve for $\vw$ (one-step learning)
  - Fix $\vw$, solve for $\gamma$ (gradient descent)

- **Step 1**
  - Assume $\gamma$ is known and fixed
  - Learn $\vw$

- Impose perfect interpolation:
  $$E_{in} = \frac{1}{n} \sum (h(\vx_i) - y_i)^2 = 0$$
- **Problem**:
  $$
  h(\vx_j)
  = \sum_i w_i \exp(-\gamma \|\vx_i - \vx_j\| ^2)
  = \sum_i w_i \phi_{i,j}
  = \vphi_j^T \vw = y_i
  $$
  - $N$ equations (one per point) and $N$ unknowns $\vw$
  - $\mat{\Phi}$ is known, function of data set and $\gamma$

* Learning RBF Models

- The problem in matrix form is:
  $$\mat{\Phi} \cdot \vw = \vy$$
  - If $\mat{\Phi}$ is invertible, then $\vw = \mat{\Phi}^{-1} \vy$
    - Desired values on training points
    - Exponential interpolates other points
  - If $\mat{\Phi}$ is not invertible, optimize in least square sense:
    $$\argmin_{\vw} E_{in} = \sum_i (h(\vx_j) - y_i)^2$$
    - Compute pseudo-inverse (assuming $\mat{\Phi}^T \mat{\Phi}$ is invertible)
    - Assign weights:
      $$\vw = (\mat{\Phi}^T \mat{\Phi})^{-1} \mat{\Phi}^T \vy$$

- **Step 2**
  - Assume $\vw$ is known and fixed
  - Learn $\gamma$

* RBF Network vs Neural Networks
- The regression model for Neural Networks and RBF model is **similar**:
  - **RBF**:
    $$
    h(\vx)
    = \sum_i w_i e^{-\gamma \|\vx - \vx_i\|^2}
    = \vw^T \vv{\vphi}
    $$
  - **Neural networks**:
    $$
    h(\vx)
    = \Theta(\vw^{(L)T} \vx^{(L)})
    = \Theta(\vw^{(L)T} \vv{\Theta}(\mW^{(L-1)} ...))
    $$

- **Difference**:
  - RBF has a single layer
  - Neural networks have multiple layers

- **Similarities**:
  - Combine features with weights using dot product
  - Extract features from inputs
    - RBF features: $e^{-\gamma \|\vx - \vx_i\|^2}$, always $> 0$
    - NN hidden layers: features can be $> 0$ or $< 0$

* RBF Network vs SVM
- The **model form** is the same:
  - RBF:
    $$h(\vx) = \sign(\sum_i w_i e^{-\gamma \|\vx - \vx_i\|^2}$$
  - SVM:
    $$h(\vx) = \sign(\vw^T \vx + b)$$ 

- The interpretation is completely different (interpolation vs large margin)
  - RBF: all vectors (or centers of few clusters) contribute to the model
  - SVM: only support vectors contribute to the model

* K-Nearest Neighbor (KNN) Model
- The **model form** is like:
  $$
  h_{\vw}(\vx) = \frac{1}{n} \sum_{\vx_i \text{closest to } \vx} w_i
  $$

::: columns
:::: {.column width=60%}

- **Idea**:
  - Closeness implies a distance (e.g., euclidean metric) or similarity (e.g., a
    kernel)
  - Consider the $k$ closest points to the evaluation point $\vx$
  - Take an average of their response

::::
:::: {.column width=40%}

![](msml610/lectures_source/figures/Lesson04_KNN_Model.png)

::::
:::

* KNN: Intuition of Number Degrees of Freedom
::: columns
:::: {.column width=60%}

- Nearest neighbor model ($k = 1$)
  - Use response of closest point to $\vx$
  - Similar to Voronoi tessellations: each point has a region where it is the
    closest and assigns its output to that region

- $k$ is **the only parameter** for KNN
  - For $k = 1$ $\implies$ $N$ neighborhoods, one around each training point
  - For $k = N$ $\implies$ single neighborhood
  - Effective number of parameters: $\frac{N}{k}$, imagining $N / k$
    non-overlapping neighborhoods

::::
:::: {.column width=40%}

![](msml610/lectures_source/figures/Lesson04_Voronoi.png)

::::
:::

* KNN: Assumptions on the Data
- KNN makes **no assumption on data**
  - Opposite of linear model with strong data assumption

- KNN assumes **locality in parameter space**
  - Model is constant in example's neighborhood
  - E.g., $k = 1$: Voronoi tessellation (low-bias/high-variance)
  - E.g., $k = N$: Average value (high-bias/low-variance)

* KNN: Training and Test Error
- For $k=1$
  - No error on training set (low bias / high variance) assuming non-noisy target
  - $E_{out}$ larger than $E_{in}$

- Increasing $k$
  - Training error $E_{in}$ increases
  - Test error $E_{out}$ decreases, then increases
  - Typical model complexity behavior in bias-variance diagrams

* KNN vs RBF Models
- **Similarities**
  - K-Nearest Neighbor is a _discrete_ version of the RBF model

- **Differences:**
  - Consider only the $k$ _closest examples_ to the point $\vx$ (not all
    examples in the training set)
  - Use a _constant kernel_ (responses are not weighted by distance)

## #############################################################################
## Clustering
## #############################################################################

* K-Means Clustering: Problem Formulation
- $N$ _unlabeled_ points $\{\vx_1, \vx_2, ..., \vx_N\}$

- Partition points into $K$ clusters $S_1, ..., S_K$
  - Each cluster defined by center $\vmu_k$
  - Each point $\vx_i$ assigned to cluster $c(\vx_i)$
  - Unknowns are $c(\vx_1), ..., c(\vx_N), \vmu_1, ..., \vmu_K$

- Minimize distance between each $\vx_i$ and assigned center $\vmu_k$ where
  $k = c(\vx_i)$
  \vspace{-0.5cm}
  \begin{alignat*}{3}
  J(c_1, ..., c_N, \mu_1, ..., \mu_K) 
  &= \sum_{k=1}^K \sum_{\vx_n \in S_k} \|\vx_n - \vmu_k\| ^ 2
  & \text{(scanning the clusters)}
  \\
  &= \sum_{i=1}^N \| \vx_i - \vmu_{c(\vx_i)} \| ^ 2
  & \text{(scanning the points)} \\
  \end{alignat*}
  \vspace{-1cm}

- **K-means clustering is NP-hard** (combinatorial) and thus intractable
  - In fact there are $K^N$ possible assignments

* K-Means Clustering: Lloyd's Algorithm
- **Start with a random assignment** of $N$ points to $K$ clusters
  - Better than picking random centroids

- **Each iteration** has 2 steps

  - **Step 1**: Move centroid
    - Move each cluster's centroid to the mean point
    - Iterate over $K$ clusters
    - $\vmu_k \leftarrow \frac{1}{|S_k|}\sum_{\vx_n \in S_k}\vx_n$

  - **Step 2**: Cluster assignment
    - Assign each $\vx_n$ to the closest cluster center
    - Iterate over $N$ points
    - $S_k \leftarrow
      \{\vx_n: \| \vx_n - \vmu_k \| \le \|\vx_n - \vmu_l\| \;
      \forall l \ne k \}$

* K-Means Clustering: Convergence
- **K-means algorithm converges** since:
  - Finite number of possible partitions (and values of objective functions)
  - Objective function $J(\cdot)$ always decreases

- **Objective function always decreases**
  - Cost function $J(\vmu_1, ..., \vmu_K, c_1, ..., c_N)$ depends on:
    - Centroids $c_1, .., c_N$
    - Point assignments $\vmu_1, ..., \vmu_K$
  - K-means minimizes $J$ by:
    - Adjusting centroids (fixed assignments)
    - Adjusting assignments (fixed centroids)
  - Similar to coordinate descent

- Generally converges to a **local minimum**
  - Run K-means multiple times with different random initializations
  - Choose best result

* K-Means Clustering: Non-Separable Clusters

::: columns
:::: {.column width=60%}

- For simplicity, you tend to imagine a clear separation between clusters
  - Clusters, especially in high dimensions, are not obviously separable

- Using K-means on data not obviously separated
  - E.g., market segmentation
  - E.g., t-shirt sizing
    - Collect height and width of customers
    - Run K-means
    - Find optimal way to split population into 3 sizes (S, M, L)

::::
:::: {.column width=40%}

![](msml610/lectures_source/figures/Lesson04_Clustering.png)

::::
:::

* Choosing the Number of Clusters
- Often unclear how many clusters $K$ exist
  - Visual analysis can be inconclusive in 2D or 3D
  - More difficult in high dimensions

::: columns
:::: {.column width=60%}

- **Elbow Method**
  - Vary clusters $K$
  - Compute optimal cost function $J(\cdot)$
  - Choose $K$ at "elbow" point if visible
  - Elbow absent if curve resembles hyperbole $\approx 1/K$

::::
:::: {.column width=40%}

![](msml610/lectures_source/figures/Lesson04_Elbow_method.png)

::::
:::

- **End-to-end approach**
  - Choose $K$ to optimize later stages
  - E.g., more t-shirt sizes (more clusters) $\implies$
    - Satisfy customers
    - Complicates manufacturing
    - Increases inventory management

* Interpretation of Clusters
- Cluster meaning **often interpreted manually** (difficult to automate)
  - Examine cluster centroids
    - Centroid values show "typical" point in each cluster
    - High, low, or zero feature values highlight key characteristics
  - Analyze distribution of features per cluster
    - Plot histograms or boxplots for each feature
    - Identify features that vary sharply across clusters
  - Visualize clusters in 2D or 3D
    - E.g., PCA, t-SNE, UMAP
    - Understand separation and internal structure
  - Compare clusters to external labels if available
    - See if clusters align with known real-world groups
  - Train a classifier like decision tree
    - Important features for predicting cluster reveal their meaning

- **Example: Customer Segmentation**

\begingroup \scriptsize
| Cluster    | Age       | Annual Income | Spending Score | Label                              |
|------------|-----------|----------------|----------------|-------------------------------------|
| Cluster 1  | 25 yrs    | 30K            | 90             | Young Big Spenders                  |
| Cluster 2  | 50 yrs    | 80K            | 40             | Comfortable Mid-Lifers              |
| Cluster 3  | 35 yrs    | 120K           | 20             | High Income, Low Spending Customers |
\endgroup

## #############################################################################
## Anomaly Detection
## #############################################################################

* Anomaly Detection: Problem Formulation
- **Problem**:
  - $\{\vx_1, ..., \vx_N\}$ examples with features $\vx \in \bbR^P$ for good
    instances
  - Detect bad/anomalous instances

- **Algorithm**:
  - Unknown characteristics of _"bad instances"_
  - Learn common traits of _"good instances"_ using unsupervised learning
    - Find distribution for $\vx_i$: $\Pr(\vx \text{ is good})$
  - Pick features
    - Find "sensitive" features, e.g., ratio between CPU load and network
      traffic
  - Estimate distribution $\Pr(\vx \text{ is good})$
  - Choose threshold $\varepsilon$
  - For new instance $\vx_{new}$, if
    $\Pr(\vx_{new} \text{ is good}) \le \varepsilon$ flag as anomaly

* Anomaly Detection: Example of Aircraft Engines
- **Problem**
  - Test aircraft engines to identify anomalies in a new engine

- **Solution**:
  - Features $\vx_i$ can be:
    - Heat generated
    - Vibration intensity
    - ...
  - Collect data for all engines
  - Model $\Pr(\vx \text{ is good})$
  - Decide if a new engine is acceptable
    $\Pr(\vx_{new} \text{ is good}) \le \varepsilon$
    or needs more testing

* Anomaly Detection: Example of Hacked Account
- **Problem**
  - Find if an account for a given user $i$ was hacked

- **Solution**:
  - Model features that represent "user $i$ activity" 
  - Features $\vx_i$ can be:
    - How many times s/he logs a day
    - How many times s/he fails to enter the password
    - How fast s/he types
    - How many pages s/he visits
    - How many times s/he posts comments
    - ...
  - Model $\Pr(\vx \text{ is good})$
  - Identify unusual users by checking $\Pr(\vx_{new} \text{ is good}) \le \varepsilon$

* Anomaly Detection: Example of Data Center
- **Problem**
  - Monitor servers in a data center to find malfunctioning or hanged servers

- **Solution**:
  - Features $\vx_i$ can be:
    - Memory in use
    - CPU load
    - Network traffic
    - Number of reads/writes per sec
    - CPU load / network activity
    - ...
  - Model $\Pr(\vx \text{ is good})$
  - Identify unusual systems by checking $\Pr(\vx_{new} \text{ is good}) \le \varepsilon$

* Using a Gaussian Model for Anomaly Detection
- Aka "density estimation"
- Given $N$ examples $\vx_1, ..., \vx_N \in \bbR^p$
- Ensure that the **features have a Gaussian distribution**
  - If not, apply some transformations, e.g., $\log(x_i + k)$
- **Estimate the parameters** of the Gaussian model $f_X(\vx)$

- Given a new example $\vx_{new}$ **compute**
  $\Pr(\vx_{new} \text{ is good}) \le \varepsilon$ to flag an anomaly

* Estimate Univariate Gaussian Model
::: columns
:::: {.column width=60%}
- You have $N$ (scalar) examples $\vx_1, ..., \vx_N \in \bbR$ representing _"good
  instances"_

- Assume the data is generated by a Gaussian distribution
  $$X \sim \calN(\mu, \sigma)$$
  which has a PDF:
  $$
  f_X(x; \mu, \sigma) = \frac{1}{\sqrt{2 \pi} \sigma} \exp(- \frac{(x - \mu)^2}{2 \sigma^2})
  $$

- Estimate mean and sigma with maximum likelihood:
  \begin{alignat*}{2}
  &\mu = \frac{1}{N}\sum_i x_i \\
  &\sigma^2 = \frac{1}{N-1}\sum_i (x_i - \mu)^2 \\
  \end{alignat*}
::::
:::: column

![](msml610/lectures_source/figures/Lesson04_1D_Gaussian.png)
::::
:::

* Estimate Multivariate Independent Gaussian Model
- You have $N$ examples $\vx_1, ..., \vx_N \in \bbR^p$ for _"good instances"_

- Assume independence of the features, the PDF of a multi-variate Gaussian $X$
  is:
  $$
  f_X(\vx; \vmu, \vsigma)
  = \prod_{i=1}^p f_{X_i}(x_i; \mu_i, \sigma_i)
  $$

- Infer the parameters $\mu_i$ and $\sigma_i$ using discrete formulas to get the
  complete model

* Estimate a Multi-Variate Gaussian Model
::: columns
:::: {.column width=60%}
- **Problem**:
  - Often features vary together (e.g., network use and CPU load), causing
    mis-classifications with independent assumptions
  - Components of $\vx_{new}$ are within range but nonsensical together
    - E.g., low network use with high CPU load

- **Solution 1**:
  - Engineer features to highlight anomalies
  - Address variable correlation not modeled in independent Gaussian models

- **Solution 2**:
  - Estimate the entire multivariate model instead of assuming independence
::::
:::: column

![](msml610/lectures_source/figures/Lesson04_2D_Gaussian.png)
::::
:::

* Estimate a Multi-Variate Gaussian Model
- The PDF of a multi-variate Gaussian is:
  $$
  f_X(\vx; \vmu, \mSigma) =
  \frac{1}{(2 \pi)^\frac{n}{2} |\mSigma|^\frac{1}{2}}
  \exp(-\frac{1}{2} (\vx - \vmu)^T \mSigma^{-1} (\vx - \vmu))
  $$

- Estimate:
  $$
  \vmu \in \bbR^d = \frac{1}{N}\sum_{k=1}^N \vx_k
  $$
  $$
  \mSigma \in \bbR^{d \times d}
  = \{s_{ij}\}
  = \frac{1}{N - 1} \sum_{k=1}^N (\vx_k - \vmu) (\vx_k - \vmu)^T
  $$

- Model requires more examples to train due to more parameters

- Independence between variables decomposes multivariate Gaussian into product
  of Gaussian distributions

* Evaluate Anomaly Detection Systems
- To evaluate models one needs to:
  - Compare different models
  - Tune hyperparameters (e.g., $\varepsilon$) of models
  - Estimate out-of-sample error

- As always we should use a single real number for comparison
  - Use any classification metric, e.g.,
    - True/false positive/negative rate
    - Precision or recall
  - F-score

- _Labeled_ data is still needed to rate models
  \begin{alignat*}{2}
  y = 0 & \text{ good} \\
  y = 1 & \text{ anomalous} \\
  \end{alignat*}

* Evaluate Anomaly Detection Systems
- Often, anomalous examples $y = 1$ are much fewer than good examples $y = 0$
  - E.g., 10,000 good vs 20 bad examples
  - Address class imbalance for accurate model performance

- **Algorithm**:
  - Pick 60% of $y = 0$ data to train (only on good examples)
  - Split remaining $y = 0$ and $y = 1$ into validation and test sets
    - Ensure both sets represent the overall dataset
    - Train, validation, and test sets should not overlap but have the same
      characteristics
    - Helps evaluate model performance accurately
  - Use validation set to compare models, estimate hyperparameters
    - E.g., $\varepsilon$ is the threshold for anomaly detection
  - Use test set to evaluate final model
    - Train on normal data, test on both normal and anomalous data

- **Aircraft engine example**

\begingroup \scriptsize
| **Dataset**    | $y = 0$ (Good Engines) | $y = 1$ (Bad Engines) |
|----------------|------------------------|------------------------|
| Total Example  | 10,000                 | 20                     |
| Train Set      | 6,000                  | 0                      |
| Validation Set | 2,000                  | 10                     |
| Test Set       | 2,000                  | 10                     |
\endgroup

* Anomaly Detection vs Supervised Learning
- Even in unsupervised learning, you need labeled data for model evaluation

- Difference with supervised learning:
  - Anomaly detection/unsupervised learning: Train only on good examples
  - Supervised learning: Train on both good and bad examples

- Use:
  - Anomaly detection/unsupervised learning:
    - Learn from good examples due to few anomalous examples
    - Strong prior on the model
    - Future anomalous examples unknown
  - Supervised learning: Less skewed classes in training set
  - Not a clear-cut decision
