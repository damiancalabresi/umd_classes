::: columns
:::: {.column width=15%}
![](msml610/lectures_source/figures/UMD_Logo.png)
::::
:::: {.column width=75%}

\vspace{0.4cm}
\begingroup \large
MSML610: Advanced Machine Learning
\endgroup
::::
:::

\vspace{1cm}

\begingroup \Large
**$$\text{\blue{Lesson 09.1: Reasoning Over Time}}$$**
\endgroup
\vspace{1cm}

**Instructor**: Dr. GP Saggese, [gsaggese@umd.edu](gsaggese@umd.edu)

**References**:
- AIMA 14: Probabilistic reasoning over time
- https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python

// ./notes/math.Kalman_filter.txt
// ./notes/math.Kalman_and_Bayesian_filters_in_Python.Labbe.2018.txt
// ./notebooks/quantopian/quantopian_kalman_filter.ipynb
// ./notebooks/Kalman_and_Bayesian_filters_in_python
// - AIMA 14.4 (p. 479)

# ##############################################################################
# Reasoning Over Time
# ##############################################################################

## Definitions

* Static vs Dynamic Probabilistic Reasoning
- **Static probabilistic reasoning**
  - Random variables have _fixed_ values
  - E.g., repairing a car:
    - Broken parts remain broken during diagnosis
    - Observed evidence stays fixed

- **Dynamic probabilistic reasoning**
  - Random variables _change over time_
    - E.g., tracking a plane's location, economic activity
  - E.g., treating a diabetic patient
    - Assess patient state, decide insulin dose
    - Evidence: insulin doses, food intake, blood sugar (change over time)
    - Time dependency (e.g., metabolic activity, time of day)

* Agents in Partially Observable Environments

```graphviz
digraph BeliefStateTracking {
    rankdir=LR;
    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.7]

    Percepts [label="Percepts", shape="ellipse", fillcolor="#F4A6A6"];
    SensorModel [label="Sensor Model", fillcolor="#FFD1A6"];
    BeliefUpdate [label="Update Belief", fillcolor="#B2E2B2"];
    BeliefState [label="Belief State", shape="ellipse", fillcolor="#A0D6D1"];
    TransitionModel [label="Transition Model", fillcolor="#A6E7F4"];
    Prediction [label="Predict Next State", fillcolor="#A6C8F4"];

    Percepts -> SensorModel;
    SensorModel -> BeliefUpdate;
    BeliefState -> Prediction;
    TransitionModel -> Prediction;
    Prediction -> BeliefUpdate;
    BeliefUpdate -> BeliefState;
}
```

- Agents in _partially observable environments_ track current state using:
  1. **Belief state**
     - Store possible world states
     - Use probability theory to quantify belief
     - Posterior distribution of current state given all evidence
  2. **Belief state + Transition model**
     - Predict world evolution in next step
  3. **Sensor model + Percepts**
     - Update belief state

- _Idea_: handle time by making each quantity a function of time
    - $\vX_{a:b}$ represents variables in $[a, b]$

* Agent: Model Components

1. **State of the world**: $\vX_t$
   - Not directly observable
2. **Prior probability of the state** at time 0: $\vX_0$
3. **Evidence variables**: $\vE_t$
   - Observable
4. **Transition model**: $\Pr(\vX_t | \vX_{0:t-1})$
   - Models world evolution
   - Specifies probability distribution of state $\vX_t$ given previous values
5. **Sensor model**: $\Pr(\vE_t | \vX_{0:t}, \vE_{0:t-1})$
   - Models generation of evidence variables $\vE_t$

// TODO(gp): Add diagram with vars and a single slice

// TODO(gp): Create slide splitting things over time

// TODO: Improve graph

```graphviz
digraph POMDP_Model {
  rankdir=LR;
  labelloc="t";
  compound=true;
  splines=true;
  fontname="Helvetica";

  node [fontname="Helvetica"];
  edge [fontname="Helvetica"];

  // ---- Prior ----
  Prior [shape=box, style="rounded,filled", fillcolor="#E6F2FF", label="Prior  P(X₀)"];

  // ---- Time 0 ----
  subgraph cluster_t0 {
    label="time 0";
    color="#CCCCCC"; style="rounded";
    X0 [shape=ellipse, label=<X<sub>0</sub>>];
    E0 [shape=ellipse, label=<E<sub>0</sub>>];
  }

  // ---- Time 1 ----
  subgraph cluster_t1 {
    label="time 1";
    color="#CCCCCC"; style="rounded";
    X1 [shape=ellipse, label=<X<sub>1</sub>>];
    E1 [shape=ellipse, label=<E<sub>1</sub>>];
    T1 [shape=box, style="filled", fillcolor="#F7F7F7", label="Transition model T₁\nPr(X₁ | X₀:…)" ];
    S1 [shape=box, style="filled", fillcolor="#F7F7F7", label="Sensor model S₁\nPr(E₁ | X₀:1, E₀:0)"];
  }

  // ---- Time 2 ----
  subgraph cluster_t2 {
    label="time 2";
    color="#CCCCCC"; style="rounded";
    X2 [shape=ellipse, label=<X<sub>2</sub>>];
    E2 [shape=ellipse, label=<E<sub>2</sub>>];
    T2 [shape=box, style="filled", fillcolor="#F7F7F7", label="Transition model T₂\nPr(X₂ | X₀:1, …)" ];
    S2 [shape=box, style="filled", fillcolor="#F7F7F7", label="Sensor model S₂\nPr(E₂ | X₀:2, E₀:1)"];
  }

  // ---- Edges: Prior to initial state ----
  Prior -> X0;

  // ---- Edges: Transition dynamics (core) ----
  X0 -> T1 -> X1;
  X1 -> T2 -> X2;

  // (Optional) show possible higher-order dependence with dashed edges
  X0 -> T2 [style=dashed, color="#888888"];

  // ---- Edges: Sensor/Observation generation ----
  // Evidence at t depends on current state X_t and previous evidence (to reflect Pr(E_t | X_{0:t}, E_{0:t-1}))
  X1 -> S1 -> E1;
  E0 -> S1;

  X2 -> S2 -> E2;
  E1 -> S2;

  // ---- Light coupling between slices for readability ----
  E0 -> E1 [style=invis]; // keeps ranks tidy
  E1 -> E2 [style=invis];

}
```

* Discrete vs Continuous Time Models
- **Discrete time models**
  - View world as time slices ("snapshots")
    - Equal time intervals, equispaced samples
    - Label times $t = 0, 1, 2, ...$
  - Each slice contains random variables:
    - Hidden RVs (e.g., $\vX_t$)
    - Observable RVs (e.g., $\vE_t$)

- **Continuous time models**
  - Model uncertainty over continuous time with stochastic differential
    equations (SDEs)
  - Discrete time models approximate SDEs

* Markov Property
- **In general**, current state $\vX_t$ depends on a growing number of past
  states:
  $$
  \Pr(\vX_t | history)
  \defeq \Pr(\vX_t | \vX_{0:t-1})
  = \Pr(\vX_t | \vX_0, \vX_1, ..., \vX_{t-1})
  $$
  - Of course, there can't be dependency from the future $\vX_{t + k}$ with
    $k > 0$

- **Markov property**: current state depends (conditionally) only on a finite
  fixed number of $k$ previous states:
  $$
  \begin{aligned}
  \Pr(\vX_t | \vX_{0:t-1})
  & = \Pr(\vX_t | \red{\vX_0, \vX_1, ..., \vX_{t-k-1}},
    \purple{\vX_{t-k}, ..., \vX_{t-1}}) \\
  & = \Pr(\vX_t | \purple{\vX_{t-k:t-1}})
  \end{aligned}
  $$

* Markov Process
- **Markov processes** (aka Markov chains) have the Markov property
  $$
  \Pr(\vX_t | history) = \Pr(\vX_t | \purple{\vX_{t-k:t-1}})
  \; \forall k, t
  $$

- **First-order Markov process**: current state $\vX_t$ depends only on previous
  state $\vX_{t-1}$:
  $$
  \Pr(\vX_t | history) = \Pr(\vX_t | \purple{\vX_{t-1}})
  \; \forall k, t
  $$
  - Next state depends only on previous state, not full history
  - _Intuition_: system "forgets" everything except immediate last state
  - Bayesian network for a first-order Markov process:
  ```graphviz[width=70%]
  digraph TemporalModel {
      rankdir=LR
      node [shape=ellipse style=filled fillcolor=lightblue fontname="Helvetica", penwidth=1.7]
      X_t_2 [label="X_{t-2}"]
      X_t_1 [label="X_{t-1}"]
      X_t   [label="X_t" style=bold]
      X_t1  [label="X_{t+1}"]
      X_t2  [label="X_{t+2}"]
      X_t_2 -> X_t_1 -> X_t -> X_t1 -> X_t2
  }
  ```
  - E.g., probability of rain today depends only on yesterday,
    $\Pr(R_t | R_{t-1}) \; \forall t$

- **Second-order Markov process**: current state $\vX_t$ depends only on
  $\vX_{t-1}$ and $\vX_{t-2}$

* Time-Homogeneous Process
- Even with the Markov assumption, there is an infinite number of probability
  distributions, one for each $t$
  $$
  \Pr(\vX_t | \vX_{t-1:t-k})
  $$

- **Time-homogeneous** (aka stationarity): probability remains constant by
  translation over $t$
  $$
  \Pr(\purple{\vX_t} | \vX_{0:t-1})
  = \Pr(\purple{\vX_{t-k}} | \vX_{0:t-k-1}) \; \forall k, t
  $$
  - _Intuition_: even if process evolves, governing laws remain unchanged
  - E.g., in the real-world, most physical laws are constant

* First-Order Time-Homogeneous Process
- **First-order Markov property**
  $$
  \Pr(\vX_t | history) = \Pr(\vX_t | \purple{\vX_{t-1}})
  $$

- **Time-homogeneous**
  $$
  \Pr(\purple{\vX_t} | \vX_{0:t-1}) = \Pr(\purple{\vX_{t-k}} | \vX_{0:t-k-1}) \;
  \forall k, t
  $$

- **First-order time-homogeneous**: putting both properties together, one
  conditional probability table suffices:
  $$
  \Pr(\vX_t | \vX_{t-1}) = \Pr(\vX_{t-k} | \vX_{t-k-1}) \; \forall k, t
  $$
  - E.g., rain probability for today depends only on yesterday and is constant:
    $\Pr(R_t | R_{t-1}) = f(R_{t-1}) \; \forall t$

* Sensor Model
- **In general**, evidence variables $\vE_t$ depend on:
  - Previous state of the world $\vX_{0:t}$
  - Previous sensor values $\vE_{0:t-1}$
  $$
  \Pr(\vE_t | \vX_{0:t}, \vE_{0:t-1})
  $$

- **Sensor Markov property**
  - Assume sensor value $\vE_t$ depends only on current state $\vX_t$, not on
    previous sensor values
    $$
    \Pr(\vE_t | \vX_{0:t}, \purple{\vE_{0:t-1}})
    = \Pr(\vE_t | \vX_t)
    $$
  - In a Bayesian network, even if $\vX_t$ and $\vE_t$ are contemporaneous, the
    arrow goes from $\vX_t \to \vE_t$ since the world causes the sensor to take on
    particular values

* Sensor Model: Rain Example
- Consider the Bayesian network for the **rain model**

- The world causes the sensor to take specific values $\vX_t \to \vE_t$ 
  - E.g., $Rain_t \to Umbrella_t$, since rain "causes" the umbrella to appear
  - Inference goes the other direction: _"see the umbrella, guess if it's
    raining"_

::: columns
:::: {.column width=50%}

- The **\green{transition model}** is $\Pr(Rain_t | Rain_{t-1})$
  - $\Pr(R_t | R_{t-1}=T) = 0.7$
  - $\Pr(R_t | R_{t-1}=F) = 0.4$
  - The sum doesn't have to be 1 since it's a conditional probability

- The **\blue{sensor model}** is $\Pr(Umbrella_t | Rain_t)$
  - $\Pr(U_t | R_t=T) = 0.9$ (people forget the umbrella)
  - $\Pr(U_t | R_t=F) = 0.2$ (people are paranoid)

::::
:::: {.column width=45%}
  \vspace{1cm}
  ```graphviz
  digraph UmbrellaHMM {
    rankdir=LR
    node [shape=ellipse, style=filled, fillcolor=lightblue, fontname="Helvetica", penwidth=1.7]

    // Hidden states (Rain)
    Rain_t_1 [label="Rain_{t-1}"]
    Rain_t   [label="Rain_t"]
    Rain_t1  [label="Rain_{t+1}"]

    // Observations (Umbrella)
    U_t_1 [label="Umbrella_{t-1}"]
    U_t   [label="Umbrella_t"]
    U_t1  [label="Umbrella_{t+1}"]

    // Time alignment
    { rank = same; Rain_t_1; Rain_t; Rain_t1 }
    { rank = same; U_t_1; U_t; U_t1 }

    // Transitions
    Rain_t_1 -> Rain_t
    Rain_t -> Rain_t1

    // Emissions
    Rain_t_1 -> U_t_1
    Rain_t -> U_t
    Rain_t1 -> U_t1
  }
  ```
::::
:::


* Prior Probability
- Complete system specification needs **prior probability of state variables** at
  initial time, $\Pr(\vX_0)$
  - Represents initial belief about system state before observations
  - Crucial for initializing state estimation process

- E.g.,
  - $\vX_0$ represents position and velocity of a moving object
  - $\Pr(\vX_0)$ could be a Gaussian distribution centered around an initial
    guess of object's position and velocity with uncertainty

* First-Order Markov Process: Joint Distribution
- **Goal**: model a sequence of states $\vX_0, \vX_1, ..., \vX_t$ and
  observations $\vE_1, ..., \vE_t$ over time, i.e., $\Pr(\vX_{0:t}, \vE_{1:t})$

- **Express the joint distribution** of $n$ random variables using the chain
  rule:
  $$
  \Pr(\vX_1, ..., \vX_n) = \prod_{i=1}^n \Pr(\vX_i | \vX_{0:i-1})
  $$

- Like for Bayesian networks **factorize joint distribution** according to graph
  dependencies:
  $$
  \Pr(\vX_1, ..., \vX_n) = \prod_{i=1}^n \Pr(\vX_i | \text{parents}(\vX_i))
  $$

::: columns
:::: {.column width=55%}
- **First-order Markov assumption**:
  $$\Pr(\vX_i | \vX_{0:i-1}) = \Pr(\vX_i | \vX_{i-1})$$

- **First-order Markov sensor model**:
  $$\Pr(\vE_i | \vX_{0:i}, \vE_{1:i-1}) = \Pr(\vE_i | \vX_i)$$
::::
:::: {.column width=40%}

```graphviz
digraph HMM {
    rankdir=TB;
    node [shape=circle, fontname="Helvetica", fontsize=12, penwidth=1.7]

    // Hidden states
    X0 -> X1 -> X2 -> dotsX -> Xt;

    // Observations
    X0 -> E1;
    X1 -> E2;
    X2 -> E3;
    dotsX -> dotE;
    Xt -> Et;

    // Style observations differently
    E1 [shape=box, style=filled, fillcolor=lightblue];
    E2 [shape=box, style=filled, fillcolor=lightblue];
    E3 [shape=box, style=filled, fillcolor=lightblue];
    dotE [label="...", shape=plaintext];
    Et [shape=box, style=filled, fillcolor=lightblue];

    // Labels for states
    X0 [label="X0"];
    X1 [label="X1"];
    X2 [label="X2"];
    dotsX [label="... ", shape=plaintext];
    Xt [label="Xt"];

    // Labels for observations
    E1 [label="E1"];
    E2 [label="E2"];
    E3 [label="E3"];
    Et [label="Et"];

    // Alignment
    { rank = same; X0; X1; X2; dotsX; Xt }
    { rank = same; E1; E2; E3; dotE }
}
```
::::
:::

* First-Order Markov Process: Joint Distribution
- Putting everything together, the joint distribution probability for a
  time-homogeneous first-order Markov process:
  \begin{align*}
  \Pr(\vX_{0:t}, \vE_{1:t})
  & = \red{\Pr(\vX_0)} 
    \prod_{i=1}^t \green{\Pr(\vX_i | \vX_{i-1})} \blue{\Pr(\vE_i | \vX_i)} \\
  & = \red{prior} \times \prod_{i} \green{transition model} \times \blue{sensor model} \\
  \end{align*}

\vspace{-0.5cm}

- **Remarks**:
  - The state evolves probabilistically from the previous state (transition
    model)
  - This structure reduces complexity and enables tractable inference
  - A Bayesian network can represent a temporal model by modeling time with
    indices $t$, i.e., "unrolling the model"

- **Problem**: infinite $t$, even assuming the Markov property

* Improving Approximation of Real-World Systems
- Is first-order Markov process a **reasonable approximation of reality**?
  - Particle following random walk is well represented by Markov process
  - In umbrella example, rain depends only on previous day

- **How to improve the approximation**
  1. _Increase order of Markov process model_
     - Model "rarely rains more than two days in a row" with second-order Markov
       model $\Pr(Rain_t | Rain_{t-1}, Rain_{t-2})$
  2. _Increase number of state variables_
     - Add $Season_t$ to incorporate historical records
     - Transition model becomes more complicated
  3. _Increase number of sensor variables_
     - Add $Location_t, Temperature_t, Humidity_t, Pressure_t$
     - Simplifies modeling of state

## Defining Temporal Inference Tasks

// ## 14.2, Inference in temporal models (p. 478)

* Inference Tasks in Temporal Models

- There are **several tasks** in temporal inference

\begingroup \scriptsize
| **Task**                | **Description**                                             | **Estimate** |
| ----------------------- | ------------------------------------------------------------| ------------------------------------------ |
| Filtering               | Estimate _current_ state given past / current obs           | $\Pr(\vX_t | \vE_{1:t})$ |
| Prediction              | Estimate _future_ state given past / current obs            | $\Pr(\vX_{t+k} | \vE_{1:t})$ for $k > 0$ |
| Smoothing               | Estimate _past_ state given past, current, and _future_ obs | $\Pr(\vX_k | \vE_{1:T})$ for $T < k$ |
| Most likely explanation | Find most probable sequence of states given the evidence    | $\argmax_{\vx_{1:T}} \Pr(\vX_{1:t} | \vE_{1:t})$ |
| Learning                | Learn model parameters or structure from data               | $\theta$ of a model |
\endgroup

* Task 1: Filtering
- **Filtering** (aka "state estimation") computes the posterior distribution of
  the _current state_ given _all evidence to date_:
  $$
  \Pr(\vX_t | \vE_{1:t} = \ve_{1:t})
  $$
  - Estimate probability of rain today, given all umbrella observations so far
    $\Pr(Rain_t | Umbrella_{1:t})$

- **Application**
  - Filtering needed by a rational agent to _track current state_
  - Agent believes current state $\Pr(\vX_{t-1})$ at time $t - 1$
  - New evidence $\ve_t$ arrives for time $t$
  - Agent updates belief about current state $\Pr(\vX_t)$ at time $t$

- _"Filtering"_ refers to filtering out noise in a signal by estimating system
  parameters

* Task 2: Prediction
- **Prediction** involves predicting the posterior distribution over a _future
  state_, given _all evidence to date_:
  $$
  \Pr(\vX_{t+k} | \vE_{1:t} = \ve_{1:t}) \text{ with } k > 0
  $$
  - E.g., compute the probability of rain three days from now:
    $$
    \Pr(Rain_{t+3} | Umbrella_{1:t})
    $$

- **Application**
  - Prediction helps rational agents evaluate actions based on expected outcomes

* Task 3: Smoothing
- **Smoothing** compute posterior distribution over a _past state_ given _all
  past, present, and future evidence_:
  $$
  \Pr(\vX_k | \vE_{1:t} = \ve_{1:t}) \text{ with } 0 \le k < t
  $$
  - **Important**: you have information about the "future" of the evidence, but
    not the state

- **Application**
  - Smoothing provides a better state estimate by incorporating more future
    evidence
  - E.g., compute the probability it rained last Wednesday, given all
    observations up to today

- The term _"smoothing"_ refers to the state estimate being smoother than
  filtering

* Task 4: Most-Likely Explanation
- **Most-likely explanation** finds the sequence of states $\vX_{1:t}$ most
  likely to have generated observations $\vE_{1:t}$:
  $$
  \argmax_{\vX_{1:t}} \Pr(\vX_{1:t} | \vE_{1:t})
  $$
  - E.g.,
    - Umbrella appeared on 3 days, not on the fourth
    - Most likely explanation: rained for 3 days, then stopped

- **Applications**
  - Speech recognition: most likely sequence of words given sounds
  - Digital processing: reconstruct bit strings over a noisy channel

* Task 5: Learning
- **Learning** involves estimating the transition model $\Pr(\vX_t | \vX_{0:t-1})$
  and the sensor model $\Pr(\vE_i | \vX_i)$ from observations

- Learning benefits from smoothing rather than filtering for better state
  estimates
    - Smoothing uses all data to estimate states, leading to more accurate models
    - E.g., in weather prediction, smoothing uses past, present, and future data
      to better estimate current weather state

## Solving Temporal Inference Tasks

// ### 14.2.1, Filtering and prediction (p. 479)

* Solving Task 1: Filtering
- **Filtering** computes the posterior distribution of the _current state_ given
  _all evidence to date_, i.e., $\Pr(\vX_t | \vE_{1:t} = \ve_{1:t})$

- A practical filtering algorithm updates the current state estimate $\vX_{t+1}$
  using the previous state $\vX_t$ and the new evidence $\ve_{t+1}$
  - Instead of recomputing each state by going over the entire history of the
    percepts
  - Aka "recursive state estimation"
    \begin{align*}
    & \Pr(\vX_{t+1} | \ve_{1:t+1}) = f(\Pr(\vX_t | \ve_{1:t}), \ve_{t+1}) \\
    & NextState = f(PreviousState, \ve_{t+1}) \\
    \end{align*}

- **Why?**
  - Time and space requirements for updating must be constant for a (finite)
    agent to keep track of current state indefinitively

- **Is it possible?**
  - What is the formula $f(...)$?

* Recursive Filtering: Update Formula
- Compute the state at time $t+1$ with all the evidence up to that time

- Assume that state and evidence are scalar and not vector
  \begingroup \footnotesize
  \begin{alignat*}{2}
  & \Pr(X_{t+1} | e_{1:t+1})
    & & \\
//
  &= \Pr(X_{t+1} | \red{e_{1:t}, e_{t+1}})
    & \quad & \red{\text{Divide up the evidence}} \\
//
  &= \alpha \Pr(e_{t+1}|X_{t+1},\green{e_{1:t}}) \Pr(X_{t+1}|\green{e_{1:t}})
    & \quad & \green{\text{Bayes rule given }} \\
//
  &= \alpha \Pr(e_{t+1}|X_{t+1}) \Pr(X_{t+1}|e_{1:t})
    & \quad & \text{\cyan{Markov sensor assumption}} \\
//
  &= \alpha \Pr(e_{t+1}|X_{t+1}) \sum_{x_t} \Pr(X_{t+1}|\blue{x_t},\violet{e_{1:t}}) \Pr(\blue{x_t} | e_{1:t})
    & \quad & \text{\blue{Condition on current state}} \\
//
  &= \alpha \Pr(e_{t+1}|X_{t+1}) \sum_{x_t} \Pr(X_{t+1}|x_t) \Pr(x_t | e_{1:t})
    & \quad & \text{\violet{Markov assumption}} \\
  \end{alignat*}
  \endgroup

- It has the expected form:
  $$
  \Pr(X_{t+1} | e_{1:t+1}) = f(\Pr(X_t | e_{1:t}), e_{t+1})
  $$

* Recursive Filtering: Update Formula
- The update formula for the state is:
  $$
  \Pr(X_{t+1} | e_{1:t+1})
  = \alpha \Pr(e_{t+1}|X_{t+1}) \sum_{x_t} \Pr(X_{t+1}|x_t) \Pr(x_t | e_{1:t})
  $$
- The next state is "Sensor model x Transition model x Recursive state"
  - Sensor model: $\Pr(e_{t+1}|X_{t+1})$
  - Transition model: $\Pr(X_{t+1}|x_t)$
  - Recursive term: $\Pr(x_t|e_{1:t})$

// TODO: Add colors

* Recursive Filtering: Intuition
- Recursive state estimation updates the state belief as new evidence arrives
  $$
  \red{\Pr(X_{t+1} | e_{1:t+1})}
  = \green{\alpha \Pr(e_{t+1} | X_{t+1})}
  \blue{\sum_{x_t} \Pr(X_{t+1} | x_t) \purple{\Pr(x_t | e_{1:t})}}
  $$
  in **\black{two steps}**

  1. **\black{Prediction step}**: Use the transition model to predict the next
     state based on the current belief
     $$
     \blue{\Pr(X_{t+1} | e_{1:t})
       = \sum_{x_t} \Pr(X_{t+1} | x_t) \purple{\Pr(x_t | e_{1:t})}}
     $$
     - Intuition: Project the current belief forward using the model of system
       evolution
  2. **\black{Update step}**: Incorporate the new observation to refine the
     prediction
     $$
     \red{\Pr(X_{t+1} | e_{1:t+1})} =
     \green{\alpha \Pr(e_{t+1} | X_{t+1})} \blue{\Pr(X_{t+1} | e_{1:t})}
     $$
     - Intuition: Correct the prediction using the likelihood of the new
       evidence

- Maintain \purple{$\Pr(X_t | e_{1:t})$}, the probability of the current state
  given all past evidence
  - E.g., in a weather model, if it was likely to rain today and rain usually
    continues, the prediction leans toward rain tomorrow
  - Seeing an umbrella supports this and updates the belief accordingly

* Forward update
- We achieved:
  \begin{align*}
  \Pr(\vX_{t+1} | \ve_{1:t+1})
  &= \alpha \Pr(\ve_{t+1}|\vX_{t+1}) \sum_{x_t} \Pr(\vX_{t+1}|\vx_t) \Pr(\vx_t | \ve_{1:t}) \\
  &= f(\Pr(\vX_t | \ve_{1:t}), \ve_{t+1}) \\
  \end{align*}

- The filtered estimate $\vf_{1:t} = \Pr(\vX_t | \ve_{1:t})$ is propagated
  forward and updated by each transition and new observation
  $$
  \vf_{1:t+1} = Forward(\vf_{1:t}, \ve_{t+1})
  $$
  starting with the initial condition $\vf_{1:0} = \Pr(\vX_0)$
  - This is called "forward update"

- This process allows efficient online inference without storing the full
  history
  - Time and space requirements for updating is constant
  - A (finite) agent can keep track of current state indefinitively

* Solving Task 2: Prediction
- Prediction is equivalent to filtering without updating the state with new
  evidence, since there is no evidence
  - Only the transition model is needed, not the sensor model

- The rule predicting state $\vX_{t+k+1}$ given state $\vX_{t+k}$ and evidence
  $\vE_{1:t}$ is:

  $$
  \Pr(\vX_{t+k+1} | \ve_{1:t}) =
  \sum_{\vx_{t+k}} \Pr(\vX_{t+k+1} | \vx_{t+k}) \Pr(\vx_{t+k} | \ve_{1:t})
  $$

- This equation can be used recursively to advance over time
  - Predicting even a few steps ahead generally incurs large uncertainty

// ### Smoothing (p. 594)

* Solving Task 3: Smoothing

- You want to calculate the probability distribution over the hidden state at
  time $k$, given all evidence up to time $t$ (in the future!)
  $$
  \Pr(X_k | e_{1:t}) \text{  where } 0 \le k < t
  $$
  - Filtering gives $\Pr(X_k | e_{1:k})$ using past and present evidence
  - Smoothing refines the estimate of past states using later evidence

- **Example**
  - You're tracking whether it was raining yesterday
  - You had some evidence up to yesterday (e.g., a cloudy sky)
  - Today you see puddles on the ground
  - That new observation supports the idea that yesterday was raining

* Task 3: Smoothing: Update Formula
- Using the same math as for filtering and the two key assumptions of Markov
  process and Markov sensor

- **Forward Pass (aka filtering):**
  - Move forward through time, using the filtering algorithm to compute:
    $$
    f_{1:k} = \Pr(X_k | e_{1:k})
    $$
  - This gives you a "best guess" of the state at time $k$, based only on
    evidence up to $k$

- **Backward Pass (aka smoothing):**
  - Move backward through time from time $t$, computing:
    $$
    b_{k+1:t} = \Pr(e_{k+1:t} | X_k)
    $$
  - This captures how likely the future evidence is, given a particular value of
    $X_k$

- **Combine them:**
  - Multiply forward and backward messages to get:
    $$
    \Pr(X_k | e_{1:t}) \propto f_{1:k} \times b_{k+1:t}
    $$

* Task 4: Most Likely Explanation: Intuition 1/2
- You are tracking the weather (sunny or rainy) based on whether someone carries
  an umbrella
  - You can't see $Weather$ directly (hidden state), but you observe umbrellas
    (which is a noisy observation)
  - You have 5 observations $Umbrella = [T, T, F, T, T]$

- **Question**: what is the most likely sequence of $Weather$ states that
  explains the $Umbrella$ observations?
  - You know something about:
    - the transition model (i.e., "it tends to rain several days in a row")
    - the sensor model (i.e., "people often forget the umbrella")

- Mathematically:
  $$
  \argmax_{x_{1:t}} \Pr(x_{1:t} | e_{1:t}) =
  \argmax_{Weather_{1:t}} \Pr(Weather_{1:t} | Umbrella_{1:t})
  $$

* Task 4: Most Likely Explanation: Intuition 2/2

- **Naive approach**: Use smoothing to choose the most likely state at each time
  step
  - Cons
    - Might lead to an implausible overall path
    - Suboptimal since the question addresses joint probability and we are not
      using all the information (only one step at the time!)

- **Viterbi algorithm**:
  - Constructs a path through a state-time graph with states as nodes and
    transitions as edges
  - Finds the most likely entire path through the hidden states

- **Key difference:**
  - In speech recognition, find the most likely word sequence behind a noisy
    audio signal
    - Smoothing: Best guess per time step (may miss non-English words or
      suboptimal sequence)
    - Viterbi: Best overall path (maximizes joint probability of the entire
      sequence)

* Viterbi Algorithm: Intuition

- **Goal**: Find the most likely sequence of hidden states given observations

1. Initialization
   - At $t = 1$, estimate probability of starting in each state using initial
     state distribution and observation likelihood

2. Recursion via dynamic programming
   - For each $t > 1$, for each state $x_t$:
   - Compute maximum probability path to $x_t$ from any previous state
   - Use:
     - $\Pr(x_t | x_{t-1})$: transition model
     - $\Pr(e_t | x_t)$: sensor model
     - Best path probability to $x_{t-1}$ from prior step
   - Store probability and corresponding back-pointer to $x_{t-1}$

3. Termination and backtrace
   - At final time $t = T$, identify state with highest final probability
   - Trace back through stored pointers to reconstruct optimal path

* Viterbi Algorithm: Example 1/2

- You observe a friend carrying an umbrella over 3 days
  - $Umbrella = [Yes, Yes, No]$

- You want to infer the most likely sequence of hidden $Weather$ states
  - States: $S = \{\text{Sunny}, \text{Rainy}\}$ (weather)
  - Observations: $O = \{\text{Yes}, \text{No}\}$ (umbrella)

  - Initial Probabilities:

    $$
    \Pr(\text{Sunny}) = 0.6, \quad \Pr(\text{Rainy}) = 0.4
    $$

  - Transition Probabilities:

    $$
    \begin{aligned}
    \Pr(\text{Sunny} \to \text{Sunny}) &= 0.7, \quad \Pr(\text{Sunny} \to \text{Rainy}) = 0.3 \\
    \Pr(\text{Rainy} \to \text{Sunny}) &= 0.4, \quad \Pr(\text{Rainy} \to \text{Rainy}) = 0.6
    \end{aligned}
    $$

  - Observation (Emission) Probabilities:

    $$
    \begin{aligned}
    \Pr(\text{Yes} | \text{Sunny}) &= 0.1, \quad \Pr(\text{No} | \text{Sunny}) = 0.9 \\
    \Pr(\text{Yes} | \text{Rainy}) &= 0.8, \quad \Pr(\text{No} | \text{Rainy}) = 0.2
    \end{aligned}
    $$

* Viterbi Algorithm: Example 2/2

- Viterbi table

\begingroup \scriptsize
| Day | State  | Probability                             | Backpointer |
|-----|--------|-----------------------------------------|-------------|
| 1   | Sunny  | $0.6 \times 0.1 = \mathbf{0.06}$     | —           |
|     | Rainy  | $0.4 \times 0.8 = \mathbf{0.32}$     | —           |
| 2   | Sunny  | $\max(0.06 \times 0.7,\ 0.32 \times 0.4) \times 0.1 = \mathbf{0.0128}$ | Rainy |
|     | Rainy  | $\max(0.06 \times 0.3,\ 0.32 \times 0.6) \times 0.8 = \mathbf{0.1536}$ | Rainy |
| 3   | Sunny  | $\max(0.0128 \times 0.7,\ 0.1536 \times 0.4) \times 0.9 = \mathbf{0.0553}$ | Rainy |
|     | Rainy  | $\max(0.0128 \times 0.3,\ 0.1536 \times 0.6) \times 0.2 = \mathbf{0.0184}$ | Rainy |
\endgroup

::: columns
:::: {.column width=50%}

- Final most probable state
  - Sunny (Day 3)
- Find the most likely sequence
  - $\text{Rainy} \rightarrow \text{Rainy} \rightarrow \text{Sunny}$

::::
:::: {.column width=45%}

```graphviz
digraph Viterbi {
    rankdir=LR;
    node [shape=ellipse, style=filled, fillcolor=lightgrey];

    // Nodes for each day and state
    S1 [label="Sunny\n0.06"];
    R1 [label="Rainy\n0.32"];
    S2 [label="Sunny\n0.0128"];
    R2 [label="Rainy\n0.1536"];
    S3 [label="Sunny\n0.0553"];
    R3 [label="Rainy\n0.0184"];

    // Transitions Day 1 → Day 2
    S1 -> S2 [label="×0.7"];
    S1 -> R2 [label="×0.3"];
    R1 -> S2 [label="×0.4"];
    R1 -> R2 [label="×0.6"];

    // Transitions Day 2 → Day 3
    S2 -> S3 [label="×0.7"];
    S2 -> R3 [label="×0.3"];
    R2 -> S3 [label="×0.4"];
    R2 -> R3 [label="×0.6"];

    // Highlight most likely path
    edge [color=blue, penwidth=2];
    R1 -> R2;
    R2 -> S3;
}
```
::::
:::
