{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Lesson 05.1: Machine Learning Theories\n",
    "\n",
    "**Course**: MSML610: Advanced Machine Learning\n",
    "\n",
    "**Instructor**: Dr. GP Saggese\n",
    "\n",
    "**References**:\n",
    "- Abu-Mostafa et al.: _\"Learning From Data\"_ (2012)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db21821",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db4c198",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact, FloatSlider, IntSlider, fixed\n",
    "\n",
    "# Set plotting style.\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5c9fee",
   "metadata": {},
   "source": [
    "## 1. Is Machine Learning Even Possible?\n",
    "\n",
    "In this section, we explore the fundamental question: can we learn anything from a limited training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fd26a1",
   "metadata": {},
   "source": [
    "### 1.1 A Simple Visual ML Experiment\n",
    "\n",
    "We begin with a simple supervised classification problem using 9-bit vectors represented as 3x3 arrays.\n",
    "\n",
    "**Key Questions**:\n",
    "- Given a training set with examples labeled as $f(\\mathbf{x}) = -1$ or $f(\\mathbf{x}) = +1$, can we predict the label for a new test pattern?\n",
    "- Multiple models can fit the same training data but give different predictions on test data\n",
    "- Which model is correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97eb3eb",
   "metadata": {},
   "source": [
    "### 1.2 Possible vs Probable\n",
    "\n",
    "**The Challenge**:\n",
    "- A function can assume **any value outside the training data**\n",
    "- Example: summer temperature data tells us nothing guaranteed about winter temperatures\n",
    "\n",
    "**Key Distinction**:\n",
    "- **Possible**: Without additional knowledge, the unknown function could behave in any way outside the known data (linear, quadratic, sine wave, etc.)\n",
    "- **Probable**: With domain knowledge or historical patterns, we can make reasonable predictions about unknown points\n",
    "\n",
    "Machine learning relies on moving from \"possible\" to \"probable\" statements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dc1150",
   "metadata": {},
   "source": [
    "### 1.3 Supervised Learning: Bin Analogy (Part 1)\n",
    "\n",
    "**The Setup**:\n",
    "- Consider a bin with red and green marbles\n",
    "- We want to estimate $\\mu = \\Pr(\\text{pick a red marble})$ where $\\mu$ is unknown\n",
    "- We pick $N$ marbles independently with replacement\n",
    "- The fraction of red marbles in our sample is $\\nu$\n",
    "\n",
    "**Question**: Does the sample frequency $\\nu$ tell us anything about the true frequency $\\mu$?\n",
    "\n",
    "**Answer**:\n",
    "- **\"No\"** (strictly): We don't know anything certain about the marbles we didn't pick. The sample could be mostly green while the bin is mostly red. This is _possible_.\n",
    "- **\"Yes\"** (practically): Under certain conditions, the sample frequency is likely close to the real frequency. This is _probable_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b9b949",
   "metadata": {},
   "source": [
    "### 1.4 Hoeffding Inequality\n",
    "\n",
    "The Hoeffding inequality formalizes the intuition that sample statistics are probably close to population statistics.\n",
    "\n",
    "**Statement**:\n",
    "For a Bernoulli random variable $X$ with probability of success $\\mu$, if we estimate the mean using $N$ samples with $\\nu = \\frac{1}{N} \\sum_i X_i$, then:\n",
    "\n",
    "$$\n",
    "\\Pr(|\\nu - \\mu| > \\varepsilon) \\le \\frac{2}{e^{2 \\varepsilon^2 N}}\n",
    "$$\n",
    "\n",
    "**Key Properties**:\n",
    "- Valid for all $N$ and $\\varepsilon$ (not asymptotic)\n",
    "- Requires random sampling in the same way for both $\\nu$ and $\\mu$\n",
    "- Exponentially small probability that $\\nu$ deviates from $\\mu$ by more than $\\varepsilon$ as $N$ increases\n",
    "- Does not depend on $\\mu$\n",
    "- Trade-off: smaller $\\varepsilon$ requires larger $N$ for the same probability bound\n",
    "\n",
    "This is a **Probably Approximately Correct (PAC)** statement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957401d0",
   "metadata": {},
   "source": [
    "### Interactive Visualization: Hoeffding Inequality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ec5deef-a69a-4875-8ed3-5daec266e86e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f17d62c2835e412baa3025d23a68bc2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.6, description='mu (true prop)', max=0.9, min=0.1, step=0.05), IntSl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function utils_Lesson05_Learning_Theory.plot_hoeffding_interactive(mu: float = 0.6, N: int = 100, epsilon: float = 0.1, n_trials: int = 1000, figsize: Optional[Tuple[int, int]] = None) -> None>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interactive visualization showing how sample statistics track population statistics.\n",
    "import utils_Lesson05_Learning_Theory as utils\n",
    "\n",
    "interact(\n",
    "    utils.plot_hoeffding_interactive,\n",
    "    mu=FloatSlider(\n",
    "        min=0.1, max=0.9, step=0.05, value=0.6, description=\"mu (true prop)\"\n",
    "    ),\n",
    "    N=IntSlider(\n",
    "        min=10, max=1000, step=10, value=100, description=\"N (sample size)\"\n",
    "    ),\n",
    "    epsilon=FloatSlider(\n",
    "        min=0.01,\n",
    "        max=0.3,\n",
    "        step=0.01,\n",
    "        value=0.1,\n",
    "        description=\"epsilon (tolerance)\",\n",
    "    ),\n",
    "    n_trials=IntSlider(\n",
    "        min=100, max=10000, step=100, value=1000, description=\"n_trials\"\n",
    "    ),\n",
    "    figsize=fixed((20, 5)),\n",
    ")\n",
    "# Adjust the sliders to see how:\n",
    "# - Larger N makes the bound tighter (exponential improvement)\n",
    "# - Smaller epsilon requires larger N for same confidence\n",
    "# - Empirical violation rate is typically much less than the bound\n",
    "# - The bound works regardless of the true value of mu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb750bd-447b-4388-af58-9d97930ab08b",
   "metadata": {},
   "source": [
    "# Hoeffding Inequality: Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dfaa30",
   "metadata": {},
   "source": [
    "### Cell 1: Empirical vs Expected Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f4ca5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b770fc939acd4e98b39e0ecfb3542581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.6, description='mu (true prob)', max=0.9, min=0.1, step=0.05), IntSl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function utils_Lesson05_Learning_Theory.plot_hoeffding_study_empirical_vs_expected(mu: float = 0.6, N: int = 100, n_trials: int = 1000, figsize: Optional[Tuple[int, int]] = None) -> None>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Bernoulli binomial with probability mu, sample N times, and compare empirical vs expected distribution.\n",
    "interact(\n",
    "    utils.plot_hoeffding_study_empirical_vs_expected,\n",
    "    mu=FloatSlider(\n",
    "        min=0.1, max=0.9, step=0.05, value=0.6, description=\"mu (true prob)\"\n",
    "    ),\n",
    "    N=IntSlider(\n",
    "        min=10, max=500, step=10, value=100, description=\"N (sample size)\"\n",
    "    ),\n",
    "    n_trials=IntSlider(\n",
    "        min=100, max=5000, step=100, value=1000, description=\"n_trials\"\n",
    "    ),\n",
    "    figsize=fixed((20, 5)),\n",
    ")\n",
    "# The empirical distribution of nu converges to the expected normal distribution as n_trials increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482aab74",
   "metadata": {},
   "source": [
    "### Cell 2: Distribution of mu - nu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d5a651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and visualize the distribution of the difference mu - nu.\n",
    "interact(\n",
    "    utils.plot_hoeffding_study_difference_distribution,\n",
    "    mu=FloatSlider(\n",
    "        min=0.1, max=0.9, step=0.05, value=0.6, description=\"mu (true prob)\"\n",
    "    ),\n",
    "    N=IntSlider(\n",
    "        min=10, max=500, step=10, value=100, description=\"N (sample size)\"\n",
    "    ),\n",
    "    n_trials=IntSlider(\n",
    "        min=100, max=5000, step=100, value=1000, description=\"n_trials\"\n",
    "    ),\n",
    "    figsize=fixed((20, 5)),\n",
    ")\n",
    "# The distribution of mu - nu is centered at zero and its spread decreases with larger N."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbde44b",
   "metadata": {},
   "source": [
    "### 1.5 Supervised Learning: Bin Analogy (Part 2)\n",
    "\n",
    "**Connecting to Machine Learning**:\n",
    "\n",
    "| Bin Analogy | Machine Learning |\n",
    "|-------------|------------------|\n",
    "| Each marble is a point $\\mathbf{x} \\in \\mathcal{X}$ | Point in input space |\n",
    "| Red marble = correct prediction | $h(\\mathbf{x}) = f(\\mathbf{x})$ |\n",
    "| Green marble = incorrect prediction | $h(\\mathbf{x}) \\neq f(\\mathbf{x})$ |\n",
    "| Sample frequency $\\nu$ | In-sample error $E_{in}(h)$ |\n",
    "| Population frequency $\\mu$ | Out-of-sample error $E_{out}(h)$ |\n",
    "\n",
    "**Result**: Hoeffding inequality bounds the generalization error:\n",
    "\n",
    "$$\n",
    "\\Pr(|E_{in} - E_{out}| > \\varepsilon) \\le c\n",
    "$$\n",
    "\n",
    "**Conclusion**: Generalization to unknown points is possible. **Machine learning is possible!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5c4bf4",
   "metadata": {},
   "source": [
    "### 1.6 Validation vs Learning\n",
    "\n",
    "**Validation Setup**:\n",
    "- Given a **fixed** hypothesis $h$\n",
    "- Hoeffding tells us that $E_{in}(h)$ is probably close to $E_{out}(h)$\n",
    "- This validates that our model generalizes\n",
    "\n",
    "**Learning Setup**:\n",
    "- Choose the **best** hypothesis from $M$ hypotheses: $h \\in \\mathcal{H} = \\{h_1, \\ldots, h_M\\}$\n",
    "- Need a bound that works for the chosen hypothesis, regardless of which one we pick\n",
    "- Using the union bound:\n",
    "\n",
    "\\begin{align*}\n",
    "\\Pr(|E_{in}(g) - E_{out}(g)| > \\varepsilon) &\\le \\Pr\\left(\\bigcup_{i=1}^M |E_{in}(h_i) - E_{out}(h_i)| > \\varepsilon\\right) \\\\\n",
    "&\\le \\sum_{i=1}^M \\Pr(|E_{in}(h_i) - E_{out}(h_i)| > \\varepsilon) \\\\\n",
    "&\\le 2M \\exp(-2\\varepsilon^2 N)\n",
    "\\end{align*}\n",
    "\n",
    "**Problem**: The bound is weak because $M$ can be very large (or infinite)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baebb009",
   "metadata": {},
   "source": [
    "### 1.7 Validation vs Learning: Coin Analogy\n",
    "\n",
    "**Validation (Single Coin)**:\n",
    "- Have one coin, want to determine if it's fair\n",
    "- Assume $\\mu = 0.5$ (unbiased)\n",
    "- Toss 10 times\n",
    "- Probability of getting 10 heads (appears biased with $\\nu = 1.0$):\n",
    "  $$\\Pr(\\nu = 1.0) = 1/2^{10} \\approx 0.1\\%$$\n",
    "- **Conclusion**: Very unlikely that out-of-sample behavior differs significantly from in-sample\n",
    "\n",
    "**Learning (Many Coins)**:\n",
    "- Have 1000 fair coins, need to choose one\n",
    "- Probability that at least one appears totally biased (10 heads in 10 tosses):\n",
    "  $$\\Pr(\\text{at least one } \\nu = 1.0) = 1 - (1 - 1/2^{10})^{1000} \\approx 63\\%$$\n",
    "- **Conclusion**: More than 50% chance that we find a coin that looks biased!\n",
    "\n",
    "This illustrates why the learning bound is weaker than the validation bound."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29a3de2",
   "metadata": {},
   "source": [
    "### 1.8 Why the Union Bound Is Weak\n",
    "\n",
    "The union bound:\n",
    "$$\\Pr(|E_{in} - E_{out}| > \\varepsilon) \\le 2M \\exp(-2\\varepsilon^2 N)$$\n",
    "\n",
    "is **artificially too loose** because:\n",
    "\n",
    "- The union bound assumes \"bad events\" $\\mathcal{B}_i$ (where hypothesis $h_i$ doesn't generalize) are disjoint\n",
    "- **In reality**, bad events are extremely overlapping because similar hypotheses fail in similar ways\n",
    "- Similar hypotheses (e.g., two perceptrons with similar weights) make similar mistakes on similar data points\n",
    "- The union bound counts overlapping events multiple times, leading to a conservative estimate\n",
    "\n",
    "This motivates the need for a tighter bound based on the **effective number of hypotheses**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485c358d",
   "metadata": {},
   "source": [
    "### 1.9 Training vs Testing: College Course Analogy\n",
    "\n",
    "Machine learning phases parallel studying for a college course:\n",
    "\n",
    "| ML Phase | College Course Equivalent |\n",
    "|----------|---------------------------|\n",
    "| **Learning Phase** (Training Set) | Studying the course material |\n",
    "| **Validation Phase** (Validation Set) | Practice problems with solutions - helps identify weaknesses |\n",
    "| **Testing Phase** (Test Set) | Final exam - different from practice, gauges true learning |\n",
    "| **Out-of-Sample Phase** (Production) | Using knowledge on the job after graduation |\n",
    "\n",
    "**Key Insights**:\n",
    "- The goal isn't to do well on the exam (test set), but to actually learn (generalize)\n",
    "- Giving out exam problems in advance wouldn't gauge learning effectively (data snooping)\n",
    "- What ultimately matters is real-world performance (out-of-sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd8423d",
   "metadata": {},
   "source": [
    "## 2. Growth Function\n",
    "\n",
    "To get a tighter bound than the union bound, we need to count the **effective number** of hypotheses rather than the total number $M$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7867d676",
   "metadata": {},
   "source": [
    "### 2.1 Dichotomy: Definition\n",
    "\n",
    "**Setup**:\n",
    "- Classify $N$ fixed points $\\mathbf{x}_1, \\ldots, \\mathbf{x}_N$ using hypothesis set $\\mathcal{H}$\n",
    "- Consider an assignment $D$ of points to classes: $\\mathbf{d}_1, \\ldots, \\mathbf{d}_N$\n",
    "\n",
    "**Definition**: $D$ is a **dichotomy** for $\\mathcal{H}$ if and only if there exists $h \\in \\mathcal{H}$ that achieves the classification $D$.\n",
    "\n",
    "**Example: 4 points in a plane with 2D perceptrons**:\n",
    "- Different positions of the separating hyperplane create different dichotomies\n",
    "- There are at most $2^N$ possible dichotomies\n",
    "- Certain classifications are impossible (e.g., XOR pattern for linearly separable data)\n",
    "- For 4 points: perceptrons can achieve 14 out of 16 possible dichotomies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136fca80",
   "metadata": {},
   "source": [
    "### 2.2 Dichotomies vs Hypotheses\n",
    "\n",
    "**Hypothesis**: Classifies every point in $\\mathcal{X}$: $\\mathcal{X} \\rightarrow \\{-1, +1\\}$\n",
    "\n",
    "**Dichotomy**: Classifies only a fixed set of points: $\\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_N\\} \\rightarrow \\{-1, +1\\}$\n",
    "- Dichotomies are \"mini-hypotheses\" (hypotheses restricted to given points)\n",
    "\n",
    "**Key Differences**:\n",
    "- Number of hypotheses: Usually infinite ($|\\mathcal{H}| = \\infty$)\n",
    "- Number of dichotomies: Always finite ($|\\mathcal{H}(\\mathbf{x}_1, \\ldots, \\mathbf{x}_N)| \\le 2^N$)\n",
    "\n",
    "**What Determines a Dichotomy**:\n",
    "- Number of points $N$\n",
    "- Hypothesis set $\\mathcal{H}$ (possible models)\n",
    "- Where points are placed\n",
    "- How points are assigned to classes\n",
    "\n",
    "**From the training set perspective**:\n",
    "- What matters are dichotomies, not hypotheses\n",
    "- Many (infinite) hypotheses can correspond to the same dichotomy\n",
    "- The \"complexity\" of $\\mathcal{H}$ relates to the number of achievable dichotomies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea336cc0",
   "metadata": {},
   "source": [
    "### 2.3 Growth Function: Definition\n",
    "\n",
    "The **growth function** counts the maximum number of dichotomies on $N$ points:\n",
    "\n",
    "$$\n",
    "m_{\\mathcal{H}}(N) = \\max_{\\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\in \\mathcal{X}} |\\mathcal{H}(\\mathbf{x}_1, \\ldots, \\mathbf{x}_N)|\n",
    "$$\n",
    "\n",
    "**Why use the maximum?**\n",
    "- Dichotomies depend on point distribution and assignment\n",
    "- Growth function considers the \"most favorable\" placement for the hypothesis set\n",
    "- Provides a worst-case bound on the effective number of hypotheses\n",
    "\n",
    "**Computing $m_{\\mathcal{H}}(N)$ by brute force**:\n",
    "1. Consider all possible placements of $N$ points\n",
    "2. Consider all possible class assignments for these points\n",
    "3. For each hypothesis $h \\in \\mathcal{H}$, compute the resulting dichotomy\n",
    "4. Count the number of unique dichotomies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f3f3dc",
   "metadata": {},
   "source": [
    "### 2.4 Growth Function: Properties\n",
    "\n",
    "**Growth function increases with $N$**:\n",
    "- $m_{\\mathcal{H}}(N)$ increases (not always monotonically) with $N$\n",
    "- Can ignore additional points to get same classification, so $m_{\\mathcal{H}}(N) \\ge m_{\\mathcal{H}}(N-1)$\n",
    "\n",
    "**Growth function increases with complexity**:\n",
    "- More complex $\\mathcal{H}$ (more flexible models) → larger $m_{\\mathcal{H}}(N)$\n",
    "- Higher dimensional input space → larger $m_{\\mathcal{H}}(N)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b865f796",
   "metadata": {},
   "source": [
    "### 2.5 Growth Function: Examples\n",
    "\n",
    "**1. Perceptron on a Plane**:\n",
    "- $m_{\\mathcal{H}}(3) = 8 = 2^3$ (can shatter 3 points)\n",
    "- $m_{\\mathcal{H}}(4) = 14 < 2^4$ (cannot achieve XOR patterns)\n",
    "\n",
    "**2. Positive Rays** on $\\mathbb{R}$: $h(x) = \\text{sign}(x - a)$\n",
    "- $m_{\\mathcal{H}}(N) = N + 1$\n",
    "- The threshold $a$ can be placed in $N+1$ intervals created by $N$ points\n",
    "\n",
    "**3. Positive Intervals** on $\\mathbb{R}$: $h(x) = 1$ if $x \\in [a,b]$, else $-1$\n",
    "- $m_{\\mathcal{H}}(N) = \\binom{N+1}{2} + 1 \\sim N^2$\n",
    "- Choose 2 endpoints from $N+1$ intervals, plus the \"all negative\" case\n",
    "\n",
    "**4. Convex Sets on a Plane**:\n",
    "- $m_{\\mathcal{H}}(N) = 2^N$\n",
    "- Place points on a circle; any subset can be enclosed by a convex polygon\n",
    "- Can shatter any number of points (infinite VC dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c278fb",
   "metadata": {},
   "source": [
    "### 2.6 Break Point of a Hypothesis Set\n",
    "\n",
    "**Shattering**: A hypothesis set $\\mathcal{H}$ **shatters $N$ points** if and only if $m_{\\mathcal{H}}(N) = 2^N$\n",
    "- There exists some arrangement of $N$ points where all $2^N$ classifications are achievable\n",
    "- Doesn't mean all arrangements of $N$ points can be shattered\n",
    "\n",
    "**Break Point**: $k$ is a **break point** for $\\mathcal{H}$ if and only if $m_{\\mathcal{H}}(k) < 2^k$\n",
    "- No data set of size $k$ can be shattered by $\\mathcal{H}$\n",
    "\n",
    "**Examples**:\n",
    "- **2D Perceptron**: break point is 4 (cannot shatter 4 points)\n",
    "- **Positive rays**: break point is 2 (cannot shatter 2 points)\n",
    "- **Positive intervals**: break point is 3 (cannot shatter 3 points)\n",
    "- **Convex sets**: no break point (can shatter any number of points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550d474f",
   "metadata": {},
   "source": [
    "### 2.7 Break Point and Learning\n",
    "\n",
    "**Key Result**: If there exists a break point for $\\mathcal{H}$, then:\n",
    "\n",
    "1. **Growth function is polynomial**: $m_{\\mathcal{H}}(N)$ is polynomial in $N$\n",
    "\n",
    "2. **Vapnik-Chervonenkis (VC) Inequality**: Instead of Hoeffding's bound\n",
    "   $$\\Pr(|E_{in}(g) - E_{out}(g)| > \\varepsilon) \\le 2M e^{-2\\varepsilon^2 N}$$\n",
    "\n",
    "   we get:\n",
    "   $$\\Pr(\\text{bad generalization}) \\le 4 m_{\\mathcal{H}}(2N) e^{-\\frac{1}{8}\\varepsilon^2 N}$$\n",
    "\n",
    "3. **Generalization**: Since $m_{\\mathcal{H}}(N)$ is polynomial, it's dominated by the negative exponential for large enough $N$\n",
    "\n",
    "**Conclusion**: A hypothesis set can be characterized by the **existence and value of a break point**. With a break point, machine learning works!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fae73f",
   "metadata": {},
   "source": [
    "## 3. The VC Dimension\n",
    "\n",
    "The VC (Vapnik-Chervonenkis) dimension provides a single number that characterizes the complexity of a hypothesis set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643cc5dd",
   "metadata": {},
   "source": [
    "### 3.1 VC Dimension: Definition\n",
    "\n",
    "The **VC dimension** of a hypothesis set $\\mathcal{H}$, denoted $d_{VC}(\\mathcal{H})$, is the **largest value of $N$** for which $m_{\\mathcal{H}}(N) = 2^N$.\n",
    "\n",
    "- I.e., the VC dimension is the maximum number of points $\\mathcal{H}$ can shatter\n",
    "\n",
    "**Properties**: If $d_{VC}(\\mathcal{H}) = d$, then:\n",
    "\n",
    "1. **Existence**: There exists some arrangement of $d$ points that can be shattered by $\\mathcal{H}$\n",
    "   - Not all sets of $d$ points can be shattered\n",
    "   - Random placement of $d$ points may not be shatterable\n",
    "\n",
    "2. **No larger shattering**: Cannot shatter $d+1$ points in any arrangement\n",
    "\n",
    "3. **Smaller sets**: $\\mathcal{H}$ can shatter $N$ points for any $N \\le d_{VC}$\n",
    "\n",
    "4. **Break point**: The smallest break point is $d_{VC} + 1$\n",
    "\n",
    "5. **Growth function bound**: $m_{\\mathcal{H}}(N) \\le \\sum_{i=0}^{d_{VC}} \\binom{N}{i}$\n",
    "\n",
    "6. **Polynomial order**: $d_{VC}$ is the order of the polynomial bounding $m_{\\mathcal{H}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c59f40",
   "metadata": {},
   "source": [
    "### 3.2 VC Dimension: Interpretation\n",
    "\n",
    "**VC dimension measures complexity** in terms of **effective parameters**.\n",
    "\n",
    "**Key Insights**:\n",
    "\n",
    "1. **Often equals number of parameters**:\n",
    "   - A perceptron in $d$-dimensional space has $d_{VC} = d + 1$\n",
    "   - This equals the number of parameters (weights)!\n",
    "\n",
    "2. **Black box measure**:\n",
    "   - Estimates effective parameters by counting shatterable points\n",
    "   - Doesn't require inspecting the model's internals\n",
    "\n",
    "3. **Not all parameters are effective**:\n",
    "   - Combining $N$ 1D perceptrons gives $2N$ parameters\n",
    "   - But effective degrees of freedom remain 2\n",
    "   - Some parameters may be redundant or constrained\n",
    "\n",
    "4. **Implications for training**:\n",
    "   - More complex $\\mathcal{H}$ (higher $d_{VC}$) → more parameters\n",
    "   - More parameters → requires more training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d24513",
   "metadata": {},
   "source": [
    "### 3.3 VC Generalization Bounds\n",
    "\n",
    "**Question**: How many data points $N$ are needed to ensure $\\Pr(|E_{in} - E_{out}| > \\varepsilon) \\le \\delta$?\n",
    "\n",
    "**VC Inequality**:\n",
    "$$\\Pr(\\text{bad generalization}) \\le 4 m_{\\mathcal{H}}(2N) e^{-\\frac{1}{8}\\varepsilon^2 N}$$\n",
    "\n",
    "**Behavior**: The bound behaves like $N^d e^{-N}$:\n",
    "- For small $N$: polynomial term $N^d$ dominates (bound is loose)\n",
    "- For large $N$: exponential term $e^{-N}$ dominates (bound approaches 0)\n",
    "- Larger $d$ (more complex models) requires larger $N$ to reach the useful region\n",
    "\n",
    "**Rule of Thumb**:\n",
    "$$N \\ge 10 \\cdot d_{VC}$$\n",
    "for reasonable generalization guarantees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ba8c20",
   "metadata": {},
   "source": [
    "### 3.4 Using the VC Bound\n",
    "\n",
    "The VC inequality can be rearranged to answer different questions:\n",
    "\n",
    "**Given $\\varepsilon$ and $\\delta$, find required $N$**:\n",
    "- \"To get 1% error with 95% confidence, how many examples do I need?\"\n",
    "\n",
    "**Given $N$ and $\\delta$, find achievable $\\varepsilon$**:\n",
    "- \"With 1000 examples, what error can I achieve with 95% confidence?\"\n",
    "\n",
    "**Generalization bound**: Setting $\\delta = 4 m_{\\mathcal{H}}(2N) e^{-\\frac{1}{8}\\varepsilon^2 N}$ and solving for $\\varepsilon$:\n",
    "\n",
    "$$\\Omega(N, \\mathcal{H}, \\delta) = \\sqrt{\\frac{8}{N} \\ln \\frac{4 m_{\\mathcal{H}}(2N)}{\\delta}}$$\n",
    "\n",
    "Then with probability $\\ge 1 - \\delta$:\n",
    "$$E_{out} \\le E_{in} + \\Omega(N, \\mathcal{H}, \\delta)$$\n",
    "\n",
    "**Interpretation**: Out-of-sample error is bounded by in-sample error plus a complexity penalty that decreases with more data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36eb561",
   "metadata": {},
   "source": [
    "### 3.5 How to Void the VC Analysis Guarantee\n",
    "\n",
    "**Scenario**: Data is genuinely non-linear (e.g., circles in center, crosses in corners)\n",
    "\n",
    "**Approach**: Transform to higher-dimensional space $\\mathcal{Z}$ where data becomes linearly separable:\n",
    "$$\\Phi: \\mathbf{x} = (x_0, \\ldots, x_d) \\rightarrow \\mathbf{z} = (z_0, \\ldots, z_{\\tilde{d}})$$\n",
    "\n",
    "**The Trap**: Progressively refining the transformation:\n",
    "- Start with: $\\mathbf{z} = (1, x_1, x_2, x_1 x_2, x_1^2, x_2^2)$\n",
    "- \"Simplify\" to: $\\mathbf{z} = (1, x_1^2, x_2^2)$\n",
    "- \"Simplify\" to: $\\mathbf{z} = (1, x_1^2 + x_2^2)$\n",
    "- \"Optimize\" to: $\\mathbf{z} = (x_1^2 + x_2^2 - 0.6)$\n",
    "\n",
    "**What went wrong?**\n",
    "- Each \"simplification\" was based on examining the data\n",
    "- Setting coefficients to zero or choosing specific transformations based on data is **data snooping**\n",
    "- The effective $d_{VC}$ is that of the **initial hypothesis set** before simplification, not the final model\n",
    "\n",
    "**Key Principle**: VC analysis is a warranty, **forfeited if data is examined before model selection**.\n",
    "- Once you peek at the data to guide model selection, you've effectively searched through a much larger hypothesis space\n",
    "- The complexity penalty should reflect the full search space, not just the final model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1019a93b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Is machine learning possible?**\n",
    "- Yes! Through the lens of \"probable\" rather than \"possible\"\n",
    "- Hoeffding inequality shows in-sample performance probably tracks out-of-sample performance\n",
    "\n",
    "**Growth function and dichotomies**:\n",
    "- Effective number of hypotheses (dichotomies) is much smaller than total number\n",
    "- Growth function $m_{\\mathcal{H}}(N)$ counts maximum achievable dichotomies\n",
    "- Break point indicates when $m_{\\mathcal{H}}(N)$ becomes polynomial\n",
    "\n",
    "**VC dimension**:\n",
    "- Single number characterizing hypothesis set complexity\n",
    "- Maximum number of points the hypothesis set can shatter\n",
    "- Often relates to number of parameters\n",
    "- Determines sample complexity: need $N \\ge 10 \\cdot d_{VC}$ examples\n",
    "\n",
    "**VC inequality**:\n",
    "- Provides generalization bounds: $E_{out} \\le E_{in} + \\Omega(N, \\mathcal{H}, \\delta)$\n",
    "- Complexity penalty $\\Omega$ decreases with more data\n",
    "- Warranty is void if you peek at data during model selection (data snooping)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
