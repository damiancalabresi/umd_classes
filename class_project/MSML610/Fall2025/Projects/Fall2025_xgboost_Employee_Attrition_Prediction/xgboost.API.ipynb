{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "020eea42",
   "metadata": {},
   "source": [
    "# XGBoost API Demo\n",
    "\n",
    "This notebook gives a **minimal, self-contained demonstration** of:\n",
    "\n",
    "- The **native XGBoost Python API** (fit / predict / predict_proba / feature importances).\n",
    "- The **wrapper layer** used in the Employee Attrition project:\n",
    "  - A scikit-learn `Pipeline` that combines preprocessing and XGBoost.\n",
    "  - Accessing the trained model and feature names from inside the pipeline.\n",
    "  - Simple threshold tuning on top of `predict_proba`.\n",
    "\n",
    "The goal is to mirror the way XGBoost is used in the main project notebook, but on a small sample dataset so the structure of the API is clear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "513af192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports for this API demo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757c6e0e",
   "metadata": {},
   "source": [
    "## 1. Load a sample binary classification dataset\n",
    "\n",
    "For this API demo we use scikit-learn's built-in **breast cancer** dataset.  \n",
    "To mimic the employee attrition setting (mixed numeric + categorical features),  \n",
    "we create a simple **categorical feature** by binning one of the numeric columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc402eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>radius_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  radius_group  \n",
       "0          0.4601                  0.11890        medium  \n",
       "1          0.2750                  0.08902         large  \n",
       "2          0.3613                  0.08758         large  \n",
       "3          0.6638                  0.17300         small  \n",
       "4          0.2364                  0.07678         large  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset as a pandas DataFrame\n",
    "data = load_breast_cancer(as_frame=True)\n",
    "X_raw = data.frame.drop(columns=[\"target\"])\n",
    "y = data.target  # binary labels (0/1)\n",
    "\n",
    "# Create a simple categorical feature by binning 'mean radius'\n",
    "X = X_raw.copy()\n",
    "X[\"radius_group\"] = pd.cut(\n",
    "    X[\"mean radius\"],\n",
    "    bins=[X[\"mean radius\"].min() - 1, 12, 18, X[\"mean radius\"].max() + 1],\n",
    "    labels=[\"small\", \"medium\", \"large\"]\n",
    ")\n",
    "\n",
    "# Train/test split (stratified to preserve class balance)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557ecf20",
   "metadata": {},
   "source": [
    "## 2. Native XGBoost API (no wrapper layer)\n",
    "\n",
    "In this section we use **XGBClassifier directly** on numeric features only (no wrapper, no pipeline).  \n",
    "The steps are:\n",
    "\n",
    "1. Drop the synthetic `radius_group` feature so we only keep numeric columns.  \n",
    "2. Configure an `XGBClassifier` with key hyperparameters.  \n",
    "3. Call `fit()` to train the model on the training set.  \n",
    "4. Use `predict()` for hard labels and `predict_proba()` for probabilities.  \n",
    "5. Evaluate performance with Accuracy, F1-score, ROC-AUC, and a classification report.\n",
    "\n",
    "This cell demonstrates the **core XGBoost workflow** used in the main project in its simplest form: clean numeric data in → train model → get p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16c72b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Native XGBoost – numeric only\n",
      "Accuracy : 0.947\n",
      "F1-score : 0.959\n",
      "ROC-AUC  : 0.994\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.93        42\n",
      "           1       0.95      0.97      0.96        72\n",
      "\n",
      "    accuracy                           0.95       114\n",
      "   macro avg       0.95      0.94      0.94       114\n",
      "weighted avg       0.95      0.95      0.95       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For the native XGBoost demo, drop the synthetic categorical feature\n",
    "X_train_num = X_train.drop(columns=[\"radius_group\"])\n",
    "X_test_num = X_test.drop(columns=[\"radius_group\"])\n",
    "\n",
    "# Configure a native XGBoost classifier\n",
    "xgb_native = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_native.fit(X_train_num, y_train)\n",
    "\n",
    "# Predictions and predicted probabilities\n",
    "y_pred = xgb_native.predict(X_test_num)\n",
    "y_proba = xgb_native.predict_proba(X_test_num)[:, 1]\n",
    "\n",
    "print(\"Native XGBoost – numeric only\")\n",
    "print(f\"Accuracy : {accuracy_score(y_test, y_pred):.3f}\")\n",
    "print(f\"F1-score : {f1_score(y_test, y_pred):.3f}\")\n",
    "print(f\"ROC-AUC  : {roc_auc_score(y_test, y_proba):.3f}\")\n",
    "print()\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ebaf8d",
   "metadata": {},
   "source": [
    "Here we inspect which numeric features matter most to the **native XGBoost model**.  \n",
    "We take `xgb_native.feature_importances_`, wrap it in a `pandas.Series` with the column names as the index, sort the values in descending order, and display the **top 10 most important features**.  \n",
    "This helps us see which variables XGBoost relies on most when predicting the binary labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f650fe10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "worst perimeter         0.225325\n",
       "worst radius            0.186433\n",
       "mean concave points     0.115287\n",
       "worst concave points    0.084676\n",
       "worst area              0.055128\n",
       "worst compactness       0.033726\n",
       "worst concavity         0.026196\n",
       "texture error           0.025838\n",
       "concavity error         0.023321\n",
       "mean concavity          0.023273\n",
       "dtype: float32"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top feature importances from the native XGBoost model\n",
    "native_importances = pd.Series(\n",
    "    xgb_native.feature_importances_,\n",
    "    index=X_train_num.columns\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "native_importances.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1061b6",
   "metadata": {},
   "source": [
    "## 3. Wrapper layer with scikit-learn Pipeline\n",
    "\n",
    "In many tabular ML problems, we don’t want to call XGBoost directly on raw data.  \n",
    "Instead, we wrap data preprocessing and the model into a single **scikit-learn `Pipeline`**:\n",
    "\n",
    "- We first **identify categorical and numeric columns**.\n",
    "- A `ColumnTransformer` applies **one-hot encoding** to categoricals and **standard scaling** to numerics.\n",
    "- An `XGBClassifier` is then trained on this transformed feature space.\n",
    "- The `Pipeline` object exposes a **single interface** with `fit`, `predict`, and `predict_proba`, and it automatically applies the same preprocessing to any new data.\n",
    "\n",
    "This pattern makes the code cleaner, reduces the risk of data leakage, and allows us to treat “preprocessing + XGBoost” as one reusable model component.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f2f3338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost + preprocessing pipeline\n",
      "Accuracy : 0.956\n",
      "F1-score : 0.966\n",
      "ROC-AUC  : 0.995\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94        42\n",
      "           1       0.96      0.97      0.97        72\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.96      0.95      0.95       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical and numeric columns\n",
    "categorical_cols = [\"radius_group\"]\n",
    "numeric_cols = [c for c in X.columns if c not in categorical_cols]\n",
    "\n",
    "# Preprocessing: one-hot encode categoricals, standardize numerics\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n",
    "        (\"num\", StandardScaler(), numeric_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# XGBoost classifier (same configuration as before)\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Combined preprocessing + model pipeline\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"clf\", xgb_model),\n",
    "])\n",
    "\n",
    "# Train the full pipeline on raw X (numeric + categorical)\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred_pipe = xgb_pipeline.predict(X_test)\n",
    "y_proba_pipe = xgb_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"XGBoost + preprocessing pipeline\")\n",
    "print(f\"Accuracy : {accuracy_score(y_test, y_pred_pipe):.3f}\")\n",
    "print(f\"F1-score : {f1_score(y_test, y_pred_pipe):.3f}\")\n",
    "print(f\"ROC-AUC  : {roc_auc_score(y_test, y_proba_pipe):.3f}\")\n",
    "print()\n",
    "print(classification_report(y_test, y_pred_pipe))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f5a489",
   "metadata": {},
   "source": [
    "## 4. Accessing the trained XGBoost model and feature importances\n",
    "\n",
    "The pipeline wrapper keeps preprocessing and the classifier together,  \n",
    "but we can still **reach inside** the pipeline to:\n",
    "\n",
    "- Get the trained XGBoost model.  \n",
    "- Reconstruct the transformed **feature names** (including one-hot encoded columns).  \n",
    "- View feature importances in a human-readable way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f015a092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "worst perimeter         0.266257\n",
       "worst radius            0.139660\n",
       "mean concave points     0.120133\n",
       "worst concave points    0.076687\n",
       "worst compactness       0.055995\n",
       "worst area              0.043156\n",
       "concavity error         0.039118\n",
       "worst concavity         0.028683\n",
       "mean area               0.025770\n",
       "mean texture            0.018867\n",
       "dtype: float32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the fitted preprocessor and XGBoost model from the pipeline\n",
    "fitted_preprocessor = xgb_pipeline.named_steps[\"preprocess\"]\n",
    "fitted_xgb = xgb_pipeline.named_steps[\"clf\"]\n",
    "\n",
    "# Get feature names created by the one-hot encoder\n",
    "ohe = fitted_preprocessor.named_transformers_[\"cat\"]\n",
    "encoded_cat_names = list(ohe.get_feature_names_out(categorical_cols))\n",
    "\n",
    "# Final list of all features seen by XGBoost\n",
    "all_feature_names = encoded_cat_names + numeric_cols\n",
    "\n",
    "# Feature importances from the pipeline's XGBoost model\n",
    "pipeline_importances = pd.Series(\n",
    "    fitted_xgb.feature_importances_,\n",
    "    index=all_feature_names\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "pipeline_importances.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5099715c",
   "metadata": {},
   "source": [
    "## 5. Example: threshold tuning on top of `predict_proba`\n",
    "\n",
    " we can also go **one step beyond** the default 0.5 threshold:  \n",
    "\n",
    "- We keep the trained pipeline as-is.  \n",
    "- We vary the decision threshold on the predicted probabilities.  \n",
    "- For each threshold we compute accuracy and F1-score, and choose a value that fits our goal \n",
    "  (e.g., higher recall or higher F1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edff0a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold tuning for the XGBoost pipeline:\n",
      "Threshold 0.1 -> Accuracy=0.956, F1=0.966\n",
      "Threshold 0.2 -> Accuracy=0.965, F1=0.973\n",
      "Threshold 0.3 -> Accuracy=0.956, F1=0.966\n",
      "Threshold 0.4 -> Accuracy=0.956, F1=0.966\n",
      "Threshold 0.5 -> Accuracy=0.956, F1=0.966\n",
      "Threshold 0.6 -> Accuracy=0.947, F1=0.958\n",
      "Threshold 0.7 -> Accuracy=0.939, F1=0.951\n",
      "Threshold 0.8 -> Accuracy=0.956, F1=0.965\n",
      "Threshold 0.9 -> Accuracy=0.947, F1=0.957\n"
     ]
    }
   ],
   "source": [
    "thresholds = np.linspace(0.1, 0.9, 9)\n",
    "\n",
    "print(\"Threshold tuning for the XGBoost pipeline:\")\n",
    "for t in thresholds:\n",
    "    y_pred_t = (y_proba_pipe >= t).astype(int)\n",
    "    acc = accuracy_score(y_test, y_pred_t)\n",
    "    f1 = f1_score(y_test, y_pred_t)\n",
    "    print(f\"Threshold {t:.1f} -> Accuracy={acc:.3f}, F1={f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29245bdb",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "This API demo notebook showed:\n",
    "\n",
    "- How the **native XGBoost classifier** is configured and used (`fit`, `predict`, `predict_proba`, `feature_importances_`).  \n",
    "- How a **scikit-learn Pipeline** wraps preprocessing + XGBoost into a single object that works directly on DataFrames.  \n",
    "- How to access the **inner model and feature names** from the pipeline.  \n",
    "- How to perform simple **threshold tuning** using `predict_proba`.\n",
    "\n",
    "These are the same patterns used in the main Employee Attrition project notebook, just demonstrated on a small, self-contained dataset.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
