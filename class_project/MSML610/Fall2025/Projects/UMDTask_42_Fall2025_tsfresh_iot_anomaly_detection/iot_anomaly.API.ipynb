{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# IoT Anomaly Detection - API Demonstration\n\nThis notebook demonstrates the programming interface (API) for the IoT Anomaly Detection system.\n\n## Overview\n\nThe API is built around utility functions in `iot_anomaly_utils.py` that provide:\n- Data loading and validation\n- Feature engineering functions\n- Model training and evaluation\n- Visualization helpers\n- Model persistence"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from iot_anomaly_utils import (\n",
    "    load_iot_data,\n",
    "    get_feature_columns,\n",
    "    compute_basic_features,\n",
    "    compute_rolling_features,\n",
    "    train_anomaly_detector,\n",
    "    evaluate_model,\n",
    "    save_model,\n",
    "    load_model,\n",
    "    plot_confusion_matrix,\n",
    "    plot_feature_importance,\n",
    "    create_forward_looking_labels,\n",
    "    validate_data_quality\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading API\n",
    "\n",
    "### load_iot_data()\n",
    "Load IoT sensor data from CSV with automatic timestamp conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = load_iot_data('data/raw/smart_manufacturing_data.csv')\n",
    "\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validate_data_quality()\n",
    "Get data quality statistics and validation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate data quality\n",
    "validation_results = validate_data_quality(df)\n",
    "\n",
    "print(f\"Total rows: {validation_results['total_rows']}\")\n",
    "print(f\"Number of machines: {validation_results['num_machines']}\")\n",
    "print(f\"Date range: {validation_results['date_range']['min']} to {validation_results['date_range']['max']}\")\n",
    "print(f\"Duplicate rows: {validation_results['duplicate_rows']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering API\n",
    "\n",
    "### compute_basic_features()\n",
    "Generate basic transformations: squared, sqrt, log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sensor columns\n",
    "sensors = ['temperature', 'vibration', 'humidity', 'pressure', 'energy_consumption']\n",
    "\n",
    "# Sample data for demonstration\n",
    "df_sample = df.sample(n=1000, random_state=42).copy()\n",
    "df_sample = df_sample.sort_values(['machine_id', 'timestamp']).reset_index(drop=True)\n",
    "\n",
    "# Compute basic features\n",
    "df_features = compute_basic_features(df_sample, sensors)\n",
    "\n",
    "print(f\"Original columns: {len(df_sample.columns)}\")\n",
    "print(f\"After basic features: {len(df_features.columns)}\")\n",
    "print(f\"New features added: {len(df_features.columns) - len(df_sample.columns)}\")\n",
    "\n",
    "# Show some new features\n",
    "new_cols = [c for c in df_features.columns if c not in df_sample.columns]\n",
    "print(f\"\\nExample new features: {new_cols[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute_rolling_features()\n",
    "Generate rolling window statistics per machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute rolling features\n",
    "df_rolling = compute_rolling_features(df_features, sensors, windows=[6, 12, 24])\n",
    "\n",
    "print(f\"After rolling features: {len(df_rolling.columns)}\")\n",
    "print(f\"Total features engineered: {len(df_rolling.columns) - len(df_sample.columns)}\")\n",
    "\n",
    "# Show rolling feature examples\n",
    "rolling_cols = [c for c in df_rolling.columns if 'rolling' in c]\n",
    "print(f\"\\nExample rolling features: {rolling_cols[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_feature_columns()\n",
    "Extract feature column names excluding metadata and target columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature columns\n",
    "feature_cols = get_feature_columns(df_rolling)\n",
    "\n",
    "print(f\"Number of feature columns: {len(feature_cols)}\")\n",
    "print(f\"\\nFeature columns (first 20):\")\n",
    "print(feature_cols[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training API\n",
    "\n",
    "### train_anomaly_detector()\n",
    "Train Random Forest anomaly detector with SMOTE balancing and StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_rolling[feature_cols].values\n",
    "y = df_rolling['anomaly_flag'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nClass distribution in training:\")\n",
    "print(f\"Normal: {(y_train == 0).sum()} ({(y_train == 0).sum() / len(y_train) * 100:.1f}%)\")\n",
    "print(f\"Anomaly: {(y_train == 1).sum()} ({(y_train == 1).sum() / len(y_train) * 100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model, scaler = train_anomaly_detector(X_train, y_train)\n",
    "\n",
    "print(\"Model trained successfully!\")\n",
    "print(f\"Model type: {type(model).__name__}\")\n",
    "print(f\"Number of estimators: {model.n_estimators}\")\n",
    "print(f\"Scaler type: {type(scaler).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation API\n",
    "\n",
    "### evaluate_model()\n",
    "Compute accuracy, precision, recall, F1 score, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "metrics = evaluate_model(y_test, y_pred)\n",
    "\n",
    "print(\"Model Performance:\")\n",
    "print(f\"Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"Recall:    {metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score:  {metrics['f1_score']:.4f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(metrics['confusion_matrix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization API\n",
    "\n",
    "### plot_confusion_matrix()\n",
    "Visualize confusion matrix as heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(\n",
    "    metrics['confusion_matrix'],\n",
    "    labels=['Normal', 'Anomaly'],\n",
    "    title='Anomaly Detection - Confusion Matrix'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot_feature_importance()\n",
    "Display top N most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(model, feature_cols, top_n=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Persistence API\n",
    "\n",
    "### save_model() / load_model()\n",
    "Save and load trained models using joblib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "save_model(model, 'models/demo_model.pkl')\n",
    "save_model(scaler, 'models/demo_scaler.pkl')\n",
    "\n",
    "print(\"Models saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "loaded_model = load_model('models/demo_model.pkl')\n",
    "loaded_scaler = load_model('models/demo_scaler.pkl')\n",
    "\n",
    "# Test loaded model\n",
    "y_pred_loaded = loaded_model.predict(loaded_scaler.transform(X_test))\n",
    "metrics_loaded = evaluate_model(y_test, y_pred_loaded)\n",
    "\n",
    "print(\"Loaded model performance:\")\n",
    "print(f\"Accuracy: {metrics_loaded['accuracy']:.4f}\")\n",
    "print(f\"F1 Score: {metrics_loaded['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Predictive API\n",
    "\n",
    "### create_forward_looking_labels()\n",
    "Generate labels that indicate if an anomaly will occur in the next N hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 24-hour forward-looking labels\n",
    "df_sample_sorted = df_sample.sort_values(['machine_id', 'timestamp']).reset_index(drop=True)\n",
    "labels_24h = create_forward_looking_labels(df_sample_sorted, horizon_hours=24)\n",
    "\n",
    "print(f\"Original anomaly rate: {df_sample_sorted['anomaly_flag'].mean():.2%}\")\n",
    "print(f\"24h forward-looking rate: {labels_24h.mean():.2%}\")\n",
    "print(f\"\\nThis shows {labels_24h.mean() / df_sample_sorted['anomaly_flag'].mean():.1f}x increase in early warnings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Complete API Workflow Example\n",
    "\n",
    "This section demonstrates a complete end-to-end workflow using the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load data\n",
    "data = load_iot_data('data/raw/smart_manufacturing_data.csv')\n",
    "data_sample = data.sample(n=2000, random_state=42)\n",
    "data_sample = data_sample.sort_values(['machine_id', 'timestamp']).reset_index(drop=True)\n",
    "\n",
    "# Step 2: Engineer features\n",
    "sensors = ['temperature', 'vibration', 'humidity', 'pressure', 'energy_consumption']\n",
    "data_engineered = compute_basic_features(data_sample, sensors)\n",
    "data_engineered = compute_rolling_features(data_engineered, sensors, windows=[6, 12])\n",
    "\n",
    "# Step 3: Prepare train/test split\n",
    "features = get_feature_columns(data_engineered)\n",
    "X = data_engineered[features].values\n",
    "y = data_engineered['anomaly_flag'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 4: Train model\n",
    "model, scaler = train_anomaly_detector(X_train, y_train)\n",
    "\n",
    "# Step 5: Evaluate\n",
    "y_pred = model.predict(scaler.transform(X_test))\n",
    "results = evaluate_model(y_test, y_pred)\n",
    "\n",
    "print(\"Complete Workflow Results:\")\n",
    "print(f\"Features engineered: {len(features)}\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  Accuracy: {results['accuracy']:.4f}\")\n",
    "print(f\"  F1 Score: {results['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete API for IoT Anomaly Detection:\n",
    "\n",
    "1. **Data Loading**: `load_iot_data()`, `validate_data_quality()`\n",
    "2. **Feature Engineering**: `compute_basic_features()`, `compute_rolling_features()`, `get_feature_columns()`\n",
    "3. **Model Training**: `train_anomaly_detector()`\n",
    "4. **Evaluation**: `evaluate_model()`\n",
    "5. **Visualization**: `plot_confusion_matrix()`, `plot_feature_importance()`\n",
    "6. **Persistence**: `save_model()`, `load_model()`\n",
    "7. **Predictive**: `create_forward_looking_labels()`\n",
    "\n",
    "For a complete application example, see [iot_anomaly.example.ipynb](iot_anomaly.example.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}