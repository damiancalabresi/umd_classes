{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c686b9df",
   "metadata": {},
   "source": [
    "# ONNX API Tutorial\n",
    "\n",
    "## Introduction\n",
    "\n",
    "ONNX (Open Neural Network Exchange) is an open format for representing machine learning models. It enables interoperability between different ML frameworks, allowing you to train a model in one framework (TensorFlow, PyTorch, scikit-learn) and deploy it in another for optimized inference.\n",
    "\n",
    "ONNX provides essential tools for production ML deployment:\n",
    "\n",
    "- **Model Conversion**: Convert models from TensorFlow, PyTorch, and other frameworks to ONNX format\n",
    "- **Model Verification**: Validate model structure and ensure correctness\n",
    "- **Optimized Inference**: Run models with ONNX Runtime for 2-5x faster inference\n",
    "- **Hardware Acceleration**: Support for CPU, GPU, and specialized accelerators\n",
    "- **Cross-Platform Deployment**: Deploy on cloud, edge devices, mobile, and browsers\n",
    "\n",
    "This document provides a concise overview of ONNX's core APIs and usage patterns, along with wrapper utilities for time series forecasting.\n",
    "\n",
    "---\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b86ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 08:09:25.519903: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765458565.535856  130713 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765458565.540607  130713 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-12-11 08:09:25.557689: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX version: 1.17.0\n",
      "ONNX Runtime version: 1.23.2\n",
      "Available execution providers: ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx import checker, helper, numpy_helper\n",
    "import onnxruntime as ort\n",
    "import tf2onnx\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(\"ONNX version:\", onnx.__version__)\n",
    "print(\"ONNX Runtime version:\", ort.__version__)\n",
    "print(\"Available execution providers:\", ort.get_available_providers())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dfe0c8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Native ONNX API\n",
    "\n",
    "### 1.1 Model Loading and Inspection\n",
    "\n",
    "The `onnx` package provides the core API for loading and inspecting ONNX models.\n",
    "\n",
    "`onnx.load()` reads an ONNX model from disk and returns a `ModelProto` object that contains:\n",
    "- Graph structure (nodes, edges)\n",
    "- Input/output specifications\n",
    "- Operator definitions\n",
    "- Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff026cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1765458569.123925  130713 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3582 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "I0000 00:00:1765458570.574366  130713 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765458570.574799  130713 single_machine.cc:361] Starting new session\n",
      "I0000 00:00:1765458570.575547  130713 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3582 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "I0000 00:00:1765458570.708100  130713 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3582 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "I0000 00:00:1765458570.750852  130713 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "I0000 00:00:1765458570.751082  130713 single_machine.cc:361] Starting new session\n",
      "I0000 00:00:1765458570.751870  130713 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3582 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'models/demo_model.onnx'.\n",
      "Model converted to ONNX successfully\n",
      "Model loaded successfully\n",
      "IR Version: 8\n",
      "Producer: tf2onnx\n",
      "Opset Version: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1765458570.858362  130713 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3582 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "I0000 00:00:1765458570.859840  130713 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\n"
     ]
    }
   ],
   "source": [
    "# Create a simple model for demonstration\n",
    "# (In practice, you'd convert an existing trained model)\n",
    "\n",
    "# First, let's create and save a simple Keras model\n",
    "# Note: use_cudnn=False is required for ONNX compatibility\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(10, 5)),\n",
    "    keras.layers.LSTM(32, use_cudnn=False),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Save it\n",
    "os.makedirs('models', exist_ok=True)\n",
    "model.save('models/demo_model.keras')\n",
    "\n",
    "# Note: Keras 3 requires the model to be called at least once before export\n",
    "dummy_input = np.random.randn(1, 10, 5).astype(np.float32)\n",
    "_ = model(dummy_input)\n",
    "model.export('models/demo_model.onnx', format='onnx')\n",
    "print(\"Model converted to ONNX successfully\")\n",
    "\n",
    "onnx_model = onnx.load('models/demo_model.onnx')\n",
    "\n",
    "print(\"Model loaded successfully\")\n",
    "print(f\"IR Version: {onnx_model.ir_version}\")\n",
    "print(f\"Producer: {onnx_model.producer_name}\")\n",
    "print(f\"Opset Version: {onnx_model.opset_import[0].version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9babb56f",
   "metadata": {},
   "source": [
    "### 1.2 Model Verification\n",
    "\n",
    "`onnx.checker.check_model()` validates the ONNX model structure to ensure it conforms to the ONNX specification. This is critical before deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8915f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is valid\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    print(\"Model is valid\")\n",
    "except onnx.checker.ValidationError as e:\n",
    "    print(f\"Model is invalid: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5452a84b",
   "metadata": {},
   "source": [
    "### 1.3 Graph Inspection\n",
    "\n",
    "The ONNX graph contains nodes (operations) and initializers (weights/parameters). You can inspect the graph structure to understand the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14fef9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Graph Inputs:\n",
      "  Name: keras_tensor\n",
      "  Shape: [0, 10, 5]\n",
      "  Type: 1\n",
      "\n",
      "Graph Outputs:\n",
      "  Name: Identity:0\n",
      "  Shape: [0, 1]\n",
      "\n",
      "Number of nodes: 13\n",
      "First 5 operations:\n",
      "  1. Transpose: sequential_1/lstm_1/transpose\n",
      "  2. Shape: sequential_1/lstm_1/Shape\n",
      "  3. Cast: sequential_1/lstm_1/Shape__45\n",
      "  4. Slice: sequential_1/lstm_1/strided_slice\n",
      "  5. Concat: sequential_1/lstm_1/zeros/packed_Concat__61\n"
     ]
    }
   ],
   "source": [
    "# Inspect graph inputs\n",
    "print(\"\\nGraph Inputs:\")\n",
    "for input_tensor in onnx_model.graph.input:\n",
    "    print(f\"  Name: {input_tensor.name}\")\n",
    "    shape = [dim.dim_value for dim in input_tensor.type.tensor_type.shape.dim]\n",
    "    print(f\"  Shape: {shape}\")\n",
    "    print(f\"  Type: {input_tensor.type.tensor_type.elem_type}\")\n",
    "\n",
    "# Inspect graph outputs\n",
    "print(\"\\nGraph Outputs:\")\n",
    "for output_tensor in onnx_model.graph.output:\n",
    "    print(f\"  Name: {output_tensor.name}\")\n",
    "    shape = [dim.dim_value for dim in output_tensor.type.tensor_type.shape.dim]\n",
    "    print(f\"  Shape: {shape}\")\n",
    "\n",
    "# Inspect nodes (operations)\n",
    "print(f\"\\nNumber of nodes: {len(onnx_model.graph.node)}\")\n",
    "print(\"First 5 operations:\")\n",
    "for i, node in enumerate(onnx_model.graph.node[:5]):\n",
    "    print(f\"  {i+1}. {node.op_type}: {node.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406c3574",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: ONNX Runtime API\n",
    "\n",
    "ONNX Runtime is a high-performance inference engine for ONNX models. It provides optimized execution across different hardware platforms.\n",
    "\n",
    "### 2.1 Creating an Inference Session\n",
    "\n",
    "`onnxruntime.InferenceSession()` creates a session for running inference. You can specify:\n",
    "- **Execution providers**: CPU, CUDA, TensorRT, DirectML, etc.\n",
    "- **Session options**: Optimization level, thread settings, profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc747178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference session created\n",
      "Providers: ['CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "session = ort.InferenceSession(\n",
    "    'models/demo_model.onnx',\n",
    "    providers=['CPUExecutionProvider']\n",
    ")\n",
    "\n",
    "print(\"Inference session created\")\n",
    "print(f\"Providers: {session.get_providers()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b494233d",
   "metadata": {},
   "source": [
    "### 2.2 Getting Input/Output Metadata\n",
    "\n",
    "The session provides methods to query input and output specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54649d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input name: keras_tensor\n",
      "Input shape: ['unk__71', 10, 5]\n",
      "Input type: tensor(float)\n",
      "\n",
      "Output name: Identity:0\n",
      "Output shape: ['unk__72', 1]\n"
     ]
    }
   ],
   "source": [
    "input_name = session.get_inputs()[0].name\n",
    "input_shape = session.get_inputs()[0].shape\n",
    "input_type = session.get_inputs()[0].type\n",
    "\n",
    "print(f\"\\nInput name: {input_name}\")\n",
    "print(f\"Input shape: {input_shape}\")\n",
    "print(f\"Input type: {input_type}\")\n",
    "\n",
    "output_name = session.get_outputs()[0].name\n",
    "output_shape = session.get_outputs()[0].shape\n",
    "\n",
    "print(f\"\\nOutput name: {output_name}\")\n",
    "print(f\"Output shape: {output_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bf7f95",
   "metadata": {},
   "source": [
    "### 2.3 Running Inference\n",
    "\n",
    "`session.run()` executes the model on input data. It takes:\n",
    "- Output names (or None for all outputs)\n",
    "- Input dictionary mapping input names to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4861492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input shape: (1, 10, 5)\n",
      "Output shape: (1, 1)\n",
      "Prediction: [0.03550987]\n"
     ]
    }
   ],
   "source": [
    "sample_input = np.random.randn(1, 10, 5).astype(np.float32)\n",
    "\n",
    "outputs = session.run([output_name], {input_name: sample_input})\n",
    "\n",
    "print(f\"\\nInput shape: {sample_input.shape}\")\n",
    "print(f\"Output shape: {outputs[0].shape}\")\n",
    "print(f\"Prediction: {outputs[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ff9221",
   "metadata": {},
   "source": [
    "### 2.4 Session Options and Optimization\n",
    "\n",
    "`SessionOptions` allows you to configure optimization levels and execution settings.\n",
    "\n",
    "**Graph Optimization Levels:**\n",
    "- `ORT_DISABLE_ALL`: No optimizations\n",
    "- `ORT_ENABLE_BASIC`: Basic optimizations (constant folding, redundant node elimination)\n",
    "- `ORT_ENABLE_EXTENDED`: Extended optimizations (layout optimizations)\n",
    "- `ORT_ENABLE_ALL`: All optimizations (includes layout transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28372cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized session created\n",
      "Optimization level: ENABLE_ALL\n",
      "Threads: 4\n"
     ]
    }
   ],
   "source": [
    "sess_options = ort.SessionOptions()\n",
    "sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "sess_options.intra_op_num_threads = 4\n",
    "\n",
    "optimized_session = ort.InferenceSession(\n",
    "    'models/demo_model.onnx',\n",
    "    sess_options=sess_options,\n",
    "    providers=['CPUExecutionProvider']\n",
    ")\n",
    "\n",
    "print(\"Optimized session created\")\n",
    "print(f\"Optimization level: ENABLE_ALL\")\n",
    "print(f\"Threads: {sess_options.intra_op_num_threads}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847f1f37",
   "metadata": {},
   "source": [
    "### 2.5 Execution Providers\n",
    "\n",
    "ONNX Runtime supports multiple execution providers for hardware acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e0dcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available Execution Providers:\n",
      "  - TensorrtExecutionProvider\n",
      "  - CUDAExecutionProvider\n",
      "  - CPUExecutionProvider\n",
      "\n",
      "Active provider: CPUExecutionProvider\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;31m2025-12-11 08:09:31.100303120 [E:onnxruntime:Default, provider_bridge_ort.cc:2251 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1844 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "\u001b[m\n",
      "\u001b[0;93m2025-12-11 08:09:31.100392108 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:1013 CreateExecutionProviderFactoryInstance] Failed to create CUDAExecutionProvider. Require cuDNN 9.* and CUDA 12.*. Please install all dependencies as mentioned in the GPU requirements page (https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements), make sure they're in the PATH, and that your GPU is supported.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "available_providers = ort.get_available_providers()\n",
    "print(\"\\nAvailable Execution Providers:\")\n",
    "for provider in available_providers:\n",
    "    print(f\"  - {provider}\")\n",
    "\n",
    "# Create session with preferred provider order\n",
    "# ONNX Runtime will use the first available provider from the list\n",
    "preferred_providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "\n",
    "# Use only providers that are actually available to avoid warnings\n",
    "available_preferred = [p for p in preferred_providers if p in ort.get_available_providers()]\n",
    "if not available_preferred:\n",
    "    available_preferred = ['CPUExecutionProvider']\n",
    "\n",
    "session_gpu = ort.InferenceSession(\n",
    "    'models/demo_model.onnx',\n",
    "    providers=available_preferred\n",
    ")\n",
    "\n",
    "print(f\"\\nActive provider: {session_gpu.get_providers()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75bfbd2",
   "metadata": {},
   "source": [
    "### 2.6 Performance Profiling\n",
    "\n",
    "ONNX Runtime can profile execution to identify bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb29f205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Profiling results saved to: onnxruntime_profile__2025-12-11_08-09-31.json\n"
     ]
    }
   ],
   "source": [
    "sess_options = ort.SessionOptions()\n",
    "sess_options.enable_profiling = True\n",
    "\n",
    "profiled_session = ort.InferenceSession(\n",
    "    'models/demo_model.onnx',\n",
    "    sess_options=sess_options,\n",
    "    providers=['CPUExecutionProvider']\n",
    ")\n",
    "\n",
    "# Run inference with profiling\n",
    "for _ in range(10):\n",
    "    _ = profiled_session.run([output_name], {input_name: sample_input})\n",
    "\n",
    "prof_file = profiled_session.end_profiling()\n",
    "print(f\"\\nProfiling results saved to: {prof_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1729a979",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Model Conversion API\n",
    "\n",
    "### 3.1 TensorFlow to ONNX Conversion\n",
    "\n",
    "`tf2onnx` provides the API for converting TensorFlow models to ONNX format.\n",
    "\n",
    "**Method 1: Using Keras model export (Keras 3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a6df12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harshavardhan-patil/src/umd_classes1/class_project/MSML610/Fall2025/Projects/UmdTask55_Fall2025_ONNX_Time_Series_Forecasting_of_Stock_Prices/.venv/lib/python3.11/site-packages/keras/src/trainers/trainer.py:212: UserWarning: Model doesn't support `jit_compile=True`. Proceeding with `jit_compile=False`.\n",
      "  warnings.warn(\n",
      "/home/harshavardhan-patil/src/umd_classes1/class_project/MSML610/Fall2025/Projects/UmdTask55_Fall2025_ONNX_Time_Series_Forecasting_of_Stock_Prices/.venv/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'adam', because it has 12 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "I0000 00:00:1765458571.292588  130821 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1765458571.428354  130713 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "I0000 00:00:1765458571.428594  130713 single_machine.cc:361] Starting new session\n",
      "I0000 00:00:1765458571.429251  130713 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3582 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "I0000 00:00:1765458571.556093  130713 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3582 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "I0000 00:00:1765458571.600541  130713 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "I0000 00:00:1765458571.600770  130713 single_machine.cc:361] Starting new session\n",
      "I0000 00:00:1765458571.601411  130713 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3582 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "Cannot infer shape for sequential_1/lstm_1/CudnnRNNV3: sequential_1/lstm_1/CudnnRNNV3:3,sequential_1/lstm_1/CudnnRNNV3:4\n",
      "Tensorflow op [sequential_1/lstm_1/CudnnRNNV3: CudnnRNNV3] is not supported\n",
      "Unsupported ops: Counter({'CudnnRNNV3': 1})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'models/keras_export.onnx'.\n",
      "Model exported to ONNX via Keras\n"
     ]
    }
   ],
   "source": [
    "keras_model = keras.models.load_model('models/demo_model.keras')\n",
    "\n",
    "# Call the model once before export (required by Keras 3)\n",
    "dummy_input = np.random.randn(1, 10, 5).astype(np.float32)\n",
    "_ = keras_model(dummy_input)\n",
    "\n",
    "keras_model.export('models/keras_export.onnx', format='onnx')\n",
    "\n",
    "print(\"Model exported to ONNX via Keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfd5ffa",
   "metadata": {},
   "source": [
    "**Method 2: Using tf2onnx programmatically**\n",
    "\n",
    "Note: `tf2onnx.convert.from_keras()` has compatibility issues with Keras 3. Use Method 1 or convert via SavedModel format instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18e6d599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/temp_saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/temp_saved_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'models/temp_saved_model'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 10, 5), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  131828792251536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  131828792260752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  131828792262096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  131828792262480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  131828792261328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Model converted with tf2onnx via SavedModel\n"
     ]
    }
   ],
   "source": [
    "import tf2onnx\n",
    "\n",
    "# First export to SavedModel\n",
    "keras_model.export('models/temp_saved_model')\n",
    "\n",
    "# Then convert SavedModel to ONNX\n",
    "import subprocess\n",
    "result = subprocess.run([\n",
    "    'python', '-m', 'tf2onnx.convert',\n",
    "    '--saved-model', 'models/temp_saved_model',\n",
    "    '--output', 'models/tf2onnx_export.onnx',\n",
    "    '--opset', '13'\n",
    "], capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"Model converted with tf2onnx via SavedModel\")\n",
    "else:\n",
    "    print(\"Conversion failed:\", result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57c7cac",
   "metadata": {},
   "source": [
    "### 3.2 Handling CuDNN Layers\n",
    "\n",
    "CuDNN-optimized LSTM layers in TensorFlow are not directly compatible with ONNX. The solution is to recreate the model with `use_cudnn=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bce3cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1765458578.802209  130713 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "I0000 00:00:1765458578.802415  130713 single_machine.cc:361] Starting new session\n",
      "I0000 00:00:1765458578.803073  130713 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3582 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "I0000 00:00:1765458578.928417  130713 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3582 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "I0000 00:00:1765458578.970338  130713 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "I0000 00:00:1765458578.970616  130713 single_machine.cc:361] Starting new session\n",
      "I0000 00:00:1765458578.971851  130713 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3582 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'models/demo_no_cudnn.onnx'.\n",
      "ONNX model saved: models/demo_no_cudnn.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1765458579.100957  130713 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3582 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "def convert_keras_to_onnx_without_cudnn(model_path, onnx_path):\n",
    "    \"\"\"\n",
    "    Convert Keras model to ONNX by disabling CuDNN optimization.\n",
    "\n",
    "    CuDNN LSTM operations are not supported in ONNX, so we recreate\n",
    "    the model with use_cudnn=False before conversion.\n",
    "    \"\"\"\n",
    "    # Load original model\n",
    "    original_model = keras.models.load_model(model_path, compile=False)\n",
    "    config = original_model.get_config()\n",
    "\n",
    "    # Modify LSTM layers to disable CuDNN\n",
    "    for layer_config in config['layers']:\n",
    "        if layer_config['class_name'] == 'LSTM':\n",
    "            layer_config['config']['use_cudnn'] = False\n",
    "        elif layer_config['class_name'] == 'Bidirectional':\n",
    "            if layer_config['config']['layer']['class_name'] == 'LSTM':\n",
    "                layer_config['config']['layer']['config']['use_cudnn'] = False\n",
    "\n",
    "    # Recreate model from modified config\n",
    "    if isinstance(original_model, keras.Sequential):\n",
    "        new_model = keras.Sequential.from_config(config)\n",
    "    else:\n",
    "        new_model = keras.Model.from_config(config)\n",
    "\n",
    "    # Copy weights\n",
    "    new_model.set_weights(original_model.get_weights())\n",
    "\n",
    "    # Build the model by calling it once\n",
    "    input_shape = new_model.input_shape\n",
    "    dummy_shape = tuple(1 if dim is None else dim for dim in input_shape)\n",
    "    dummy_input = np.zeros(dummy_shape, dtype=np.float32)\n",
    "    new_model(dummy_input)\n",
    "\n",
    "    # Export to ONNX\n",
    "    new_model.export(onnx_path, format='onnx')\n",
    "\n",
    "    return onnx_path\n",
    "\n",
    "# Use the function\n",
    "onnx_path = convert_keras_to_onnx_without_cudnn(\n",
    "    'models/demo_model.keras',\n",
    "    'models/demo_no_cudnn.onnx'\n",
    ")\n",
    "print(f\"ONNX model saved: {onnx_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f3f4bb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Wrapper Utilities\n",
    "\n",
    "To simplify common ONNX operations for time series forecasting, we provide wrapper utilities that abstract away boilerplate code.\n",
    "\n",
    "### 4.1 Conversion Wrapper\n",
    "\n",
    "`convert_to_onnx()` wraps the conversion process with automatic CuDNN handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3d41086",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1765458579.499945  130713 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "I0000 00:00:1765458579.500164  130713 single_machine.cc:361] Starting new session\n",
      "I0000 00:00:1765458579.500813  130713 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3582 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "I0000 00:00:1765458579.627611  130713 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3582 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "I0000 00:00:1765458579.672496  130713 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "I0000 00:00:1765458579.672717  130713 single_machine.cc:361] Starting new session\n",
      "I0000 00:00:1765458579.673772  130713 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3582 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'models/wrapper_demo.onnx'.\n",
      "Converted: models/wrapper_demo.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1765458579.793903  130713 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3582 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "def convert_to_onnx(model_path: str, onnx_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert Keras model to ONNX format.\n",
    "\n",
    "    Automatically handles CuDNN LSTM layers by recreating the model\n",
    "    with use_cudnn=False before conversion.\n",
    "\n",
    "    Args:\n",
    "        model_path: Path to Keras model (.keras or .h5)\n",
    "        onnx_path: Path to save ONNX model\n",
    "\n",
    "    Returns:\n",
    "        Path to saved ONNX model\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(onnx_path), exist_ok=True)\n",
    "\n",
    "    if model_path.endswith('.keras'):\n",
    "        onnx_path = convert_keras_to_onnx_without_cudnn(model_path, onnx_path)\n",
    "\n",
    "    return onnx_path\n",
    "\n",
    "# Usage\n",
    "onnx_model_path = convert_to_onnx(\n",
    "    model_path='models/demo_model.keras',\n",
    "    onnx_path='models/wrapper_demo.onnx'\n",
    ")\n",
    "print(f\"Converted: {onnx_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6ac9f5",
   "metadata": {},
   "source": [
    "### 4.2 Verification Wrapper\n",
    "\n",
    "`verify_onnx()` wraps the verification process and returns structured results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c660908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verification Results:\n",
      "  Valid: True\n",
      "  Opset: 15\n",
      "  Nodes: 13\n"
     ]
    }
   ],
   "source": [
    "def verify_onnx(onnx_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Verify ONNX model and return structured results.\n",
    "\n",
    "    Args:\n",
    "        onnx_path: Path to ONNX model\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with validation results:\n",
    "        - is_valid: bool\n",
    "        - error: str or None\n",
    "        - opset_version: int\n",
    "        - num_nodes: int\n",
    "    \"\"\"\n",
    "    model = onnx.load(onnx_path)\n",
    "\n",
    "    try:\n",
    "        onnx.checker.check_model(model)\n",
    "        is_valid = True\n",
    "        error = None\n",
    "    except Exception as e:\n",
    "        is_valid = False\n",
    "        error = str(e)\n",
    "\n",
    "    return {\n",
    "        'is_valid': is_valid,\n",
    "        'error': error,\n",
    "        'opset_version': model.opset_import[0].version,\n",
    "        'num_nodes': len(model.graph.node)\n",
    "    }\n",
    "\n",
    "# Usage\n",
    "verification = verify_onnx('models/wrapper_demo.onnx')\n",
    "print(\"\\nVerification Results:\")\n",
    "print(f\"  Valid: {verification['is_valid']}\")\n",
    "print(f\"  Opset: {verification['opset_version']}\")\n",
    "print(f\"  Nodes: {verification['num_nodes']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96aef8e",
   "metadata": {},
   "source": [
    "### 4.3 Inference Session Wrapper\n",
    "\n",
    "`ONNXInferenceSession` provides a simplified interface for inference with helpful utility methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30774ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: ['unk__260', 10, 5]\n",
      "Output shape: ['unk__261', 1]\n",
      "Predictions: [0.03550987]\n"
     ]
    }
   ],
   "source": [
    "class ONNXInferenceSession:\n",
    "    \"\"\"\n",
    "    Wrapper around onnxruntime.InferenceSession for simplified inference.\n",
    "\n",
    "    Provides convenient methods for getting input/output shapes and\n",
    "    running predictions without manually managing input/output names.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_path: str, providers=None):\n",
    "        \"\"\"\n",
    "        Initialize inference session.\n",
    "\n",
    "        Args:\n",
    "            model_path: Path to ONNX model\n",
    "            providers: List of execution providers (default: CPUExecutionProvider)\n",
    "        \"\"\"\n",
    "        if providers is None:\n",
    "            providers = ['CPUExecutionProvider']\n",
    "\n",
    "        self.session = ort.InferenceSession(model_path, providers=providers)\n",
    "        self.input_name = self.session.get_inputs()[0].name\n",
    "        self.output_name = self.session.get_outputs()[0].name\n",
    "\n",
    "    def get_input_shape(self):\n",
    "        \"\"\"Get expected input shape.\"\"\"\n",
    "        return self.session.get_inputs()[0].shape\n",
    "\n",
    "    def get_output_shape(self):\n",
    "        \"\"\"Get output shape.\"\"\"\n",
    "        return self.session.get_outputs()[0].shape\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Run inference on input data.\n",
    "\n",
    "        Args:\n",
    "            X: Input numpy array\n",
    "\n",
    "        Returns:\n",
    "            Predictions as numpy array\n",
    "        \"\"\"\n",
    "        X = X.astype(np.float32)\n",
    "        outputs = self.session.run([self.output_name], {self.input_name: X})\n",
    "        return outputs[0]\n",
    "\n",
    "# Usage\n",
    "session = ONNXInferenceSession('models/wrapper_demo.onnx')\n",
    "\n",
    "print(f\"Input shape: {session.get_input_shape()}\")\n",
    "print(f\"Output shape: {session.get_output_shape()}\")\n",
    "\n",
    "predictions = session.predict(sample_input)\n",
    "print(f\"Predictions: {predictions[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7cbad3",
   "metadata": {},
   "source": [
    "### 4.4 Framework Comparison Utility\n",
    "\n",
    "`compare_frameworks_inference()` benchmarks TensorFlow vs ONNX Runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57b0c5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harshavardhan-patil/src/umd_classes1/class_project/MSML610/Fall2025/Projects/UmdTask55_Fall2025_ONNX_Time_Series_Forecasting_of_Stock_Prices/.venv/lib/python3.11/site-packages/keras/src/trainers/trainer.py:212: UserWarning: Model doesn't support `jit_compile=True`. Proceeding with `jit_compile=False`.\n",
      "  warnings.warn(\n",
      "/home/harshavardhan-patil/src/umd_classes1/class_project/MSML610/Fall2025/Projects/UmdTask55_Fall2025_ONNX_Time_Series_Forecasting_of_Stock_Prices/.venv/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'adam', because it has 12 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance Comparison:\n",
      "  TensorFlow: 0.428387s\n",
      "  ONNX:       0.000549s\n",
      "  Speedup:    780.87x\n",
      "  Numerically equivalent: True\n"
     ]
    }
   ],
   "source": [
    "def compare_frameworks_inference(keras_model_path, onnx_model_path, test_input):\n",
    "    \"\"\"\n",
    "    Compare TensorFlow and ONNX Runtime inference performance.\n",
    "\n",
    "    Args:\n",
    "        keras_model_path: Path to Keras model\n",
    "        onnx_model_path: Path to ONNX model\n",
    "        test_input: Input data for inference\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with comparison results:\n",
    "        - tensorflow_time: float (seconds)\n",
    "        - onnx_time: float (seconds)\n",
    "        - speedup: float\n",
    "        - max_difference: float\n",
    "        - mean_difference: float\n",
    "        - numerically_close: bool\n",
    "    \"\"\"\n",
    "    # Load models\n",
    "    tf_model = keras.models.load_model(keras_model_path)\n",
    "    onnx_session = ONNXInferenceSession(onnx_model_path)\n",
    "\n",
    "    test_input = test_input.astype(np.float32)\n",
    "\n",
    "    # TensorFlow inference\n",
    "    start = time.time()\n",
    "    tf_pred = tf_model.predict(test_input, verbose=0)\n",
    "    tf_time = time.time() - start\n",
    "\n",
    "    # ONNX inference\n",
    "    start = time.time()\n",
    "    onnx_pred = onnx_session.predict(test_input)\n",
    "    onnx_time = time.time() - start\n",
    "\n",
    "    # Compare results\n",
    "    max_diff = np.max(np.abs(tf_pred - onnx_pred))\n",
    "    mean_diff = np.mean(np.abs(tf_pred - onnx_pred))\n",
    "    numerically_close = np.allclose(tf_pred, onnx_pred, rtol=1e-5, atol=1e-6)\n",
    "\n",
    "    return {\n",
    "        'tensorflow_time': tf_time,\n",
    "        'onnx_time': onnx_time,\n",
    "        'speedup': tf_time / onnx_time,\n",
    "        'max_difference': max_diff,\n",
    "        'mean_difference': mean_diff,\n",
    "        'numerically_close': numerically_close\n",
    "    }\n",
    "\n",
    "# Usage\n",
    "comparison = compare_frameworks_inference(\n",
    "    keras_model_path='models/demo_model.keras',\n",
    "    onnx_model_path='models/wrapper_demo.onnx',\n",
    "    test_input=np.random.randn(100, 10, 5).astype(np.float32)\n",
    ")\n",
    "\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(f\"  TensorFlow: {comparison['tensorflow_time']:.6f}s\")\n",
    "print(f\"  ONNX:       {comparison['onnx_time']:.6f}s\")\n",
    "print(f\"  Speedup:    {comparison['speedup']:.2f}x\")\n",
    "print(f\"  Numerically equivalent: {comparison['numerically_close']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabc98c7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Native ONNX API\n",
    "\n",
    "| Component | Purpose | Key Methods |\n",
    "|-----------|---------|-------------|\n",
    "| `onnx` | Model loading and inspection | `load()`, `checker.check_model()` |\n",
    "| `onnxruntime` | Optimized inference | `InferenceSession()`, `run()` |\n",
    "| `tf2onnx` | TensorFlow conversion | `convert.from_keras()` |\n",
    "\n",
    "### Wrapper Utilities\n",
    "\n",
    "| Function | Purpose | Benefits |\n",
    "|----------|---------|----------|\n",
    "| `convert_to_onnx()` | Model conversion | Automatic CuDNN handling |\n",
    "| `verify_onnx()` | Model validation | Structured results |\n",
    "| `ONNXInferenceSession` | Simplified inference | Clean API, no name management |\n",
    "| `compare_frameworks_inference()` | Benchmarking | Performance and accuracy validation |\n",
    "\n",
    "### Key Advantages\n",
    "\n",
    "- **Performance**: 2-5x faster inference than native frameworks\n",
    "- **Portability**: Deploy on any platform (cloud, edge, mobile)\n",
    "- **Interoperability**: Convert from any framework\n",
    "- **Optimization**: Hardware-specific optimizations\n",
    "- **Production Ready**: Built-in verification and profiling\n",
    "\n",
    "For a complete end-to-end stock price forecasting example using ONNX, see `onnx_forecasting.example.ipynb`."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
