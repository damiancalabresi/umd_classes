{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME CNN API Tutorial\n",
    "\n",
    "This notebook demonstrates the native APIs for LIME and PyTorch CNN models, along with the wrapper functions provided in `lime_cnn_utils.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "from lime_cnn_utils import (\n",
    "    FastFoodDataset,\n",
    "    create_balanced_subset_from_metadata,\n",
    "    get_data_transforms,\n",
    "    create_cnn_model,\n",
    "    batch_predict,\n",
    "    explain_prediction,\n",
    "    visualize_explanation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Management\n",
    "\n",
    "### 2.1 FastFoodDataset Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastFoodDataset is a custom Dataset class for efficient image loading\n",
      "It's typically created via create_balanced_subset_from_metadata()\n"
     ]
    }
   ],
   "source": [
    "# FastFoodDataset is used internally by create_balanced_subset_from_metadata\n",
    "# It's optimized for loading images from a list of paths\n",
    "\n",
    "# Example usage (typically created via create_balanced_subset_from_metadata):\n",
    "# image_paths_and_labels = [(\"path/to/image1.jpg\", 0), (\"path/to/image2.jpg\", 1), ...]\n",
    "# dataset = FastFoodDataset(\n",
    "#     image_paths_and_labels=image_paths_and_labels,\n",
    "#     transform=train_transform,\n",
    "#     classes=[\"class1\", \"class2\", ...],\n",
    "#     class_to_idx={\"class1\": 0, \"class2\": 1, ...}\n",
    "# )\n",
    "\n",
    "print(\"FastFoodDataset is a custom Dataset class for efficient image loading\")\n",
    "print(\"It's typically created via create_balanced_subset_from_metadata()\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Metadata-Based Subset Creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_balanced_subset_from_metadata() enables fast subset creation\n",
      "Key features: class selection, balanced distribution, metadata-driven loading\n"
     ]
    }
   ],
   "source": [
    "# Create balanced subset from JSON metadata\n",
    "\n",
    "# Example usage:\n",
    "# import json\n",
    "# from pathlib import Path\n",
    "# \n",
    "# meta_dir = Path(\"data/food-101/meta\")\n",
    "# train_meta_path = meta_dir / \"train.json\"\n",
    "# \n",
    "# # Load metadata\n",
    "# with open(train_meta_path, 'r') as f:\n",
    "#     metadata = json.load(f)\n",
    "# all_class_names = sorted(metadata.keys())\n",
    "# \n",
    "# # Create balanced subset\n",
    "# train_dataset = create_balanced_subset_from_metadata(\n",
    "#     metadata_path=train_meta_path,\n",
    "#     data_root=\"data\",\n",
    "#     all_class_names=all_class_names,\n",
    "#     total_samples=1000,\n",
    "#     transform=train_transform,\n",
    "#     selected_classes=None,  # Use all classes\n",
    "#     num_classes_to_use=5,   # Or randomly select N classes\n",
    "#     random_seed=42\n",
    "# )\n",
    "\n",
    "print(\"create_balanced_subset_from_metadata() enables fast subset creation\")\n",
    "print(\"Key features: class selection, balanced distribution, metadata-driven loading\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Native PyTorch/Torchvision API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original ResNet-18 output features: 512\n",
      "Modified ResNet-18: Linear(in_features=512, out_features=101, bias=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/lime/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/lime/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Native PyTorch: Create ResNet-18 model\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "print(f\"Original ResNet-18 output features: {resnet18.fc.in_features}\")\n",
    "\n",
    "# Modify for 101 food classes\n",
    "num_classes = 101\n",
    "resnet18.fc = nn.Linear(resnet18.fc.in_features, num_classes)\n",
    "print(f\"Modified ResNet-18: {resnet18.fc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform pipeline created successfully\n"
     ]
    }
   ],
   "source": [
    "# Native torchvision transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load and transform an image (example - replace with actual path)\n",
    "# image = Image.open(\"data/sample_image.jpg\").convert('RGB')\n",
    "# tensor = transform(image)\n",
    "# print(f\"Image tensor shape: {tensor.shape}\")\n",
    "print(\"Transform pipeline created successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Wrapper Function: Model Creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created: <class 'torchvision.models.resnet.ResNet'>\n"
     ]
    }
   ],
   "source": [
    "# Using wrapper function\n",
    "model = create_cnn_model(\n",
    "    num_classes=101,\n",
    "    architecture='resnet18',\n",
    "    pretrained=True\n",
    ")\n",
    "print(f\"Model created: {type(model)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Wrapper Function: Data Transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train transform: 8 steps\n",
      "Val transform: 4 steps\n"
     ]
    }
   ],
   "source": [
    "# Using wrapper function\n",
    "train_transform, val_transform = get_data_transforms()\n",
    "print(f\"Train transform: {len(train_transform.transforms)} steps\")\n",
    "print(f\"Val transform: {len(val_transform.transforms)} steps\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Native LIME API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIME explainer created: <class 'lime.lime_image.LimeImageExplainer'>\n"
     ]
    }
   ],
   "source": [
    "# Native LIME API\n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "print(f\"LIME explainer created: {type(explainer)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Prediction Function for LIME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape: (1, 101)\n",
      "Sum of probabilities: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Define prediction function (required by LIME)\n",
    "def predict_fn(images):\n",
    "    \"\"\"\n",
    "    LIME requires a function that takes a batch of images\n",
    "    and returns predictions.\n",
    "    \"\"\"\n",
    "    return batch_predict(images, model, device='cpu')  # or 'cuda'\n",
    "\n",
    "# Test with a single image\n",
    "test_image = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)\n",
    "test_batch = np.array([test_image])\n",
    "predictions = predict_fn(test_batch)\n",
    "print(f\"Prediction shape: {predictions.shape}\")\n",
    "print(f\"Sum of probabilities: {predictions.sum():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Generate Explanation (Native API)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIME explanation API demonstrated. Provide image path to generate actual explanation.\n"
     ]
    }
   ],
   "source": [
    "# Example: Generate explanation using native LIME API\n",
    "# Uncomment and provide actual image path to run\n",
    "\n",
    "# image_path = \"data/test_image.jpg\"  # Replace with actual path\n",
    "# image = Image.open(image_path).convert('RGB')\n",
    "# image_array = np.array(image)\n",
    "\n",
    "# # Generate explanation using native LIME API\n",
    "# explanation = explainer.explain_instance(\n",
    "#     image_array,\n",
    "#     predict_fn,\n",
    "#     top_labels=5,\n",
    "#     hide_color=0,\n",
    "#     num_samples=1000\n",
    "# )\n",
    "\n",
    "# print(f\"Top labels: {explanation.top_labels}\")\n",
    "# print(f\"Explanation object: {type(explanation)}\")\n",
    "\n",
    "print(\"LIME explanation API demonstrated. Provide image path to generate actual explanation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Extract Image and Mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get explanation visualization\n",
    "# Uncomment and provide actual explanation object to run\n",
    "\n",
    "# top_label = explanation.top_labels[0]\n",
    "# temp, mask = explanation.get_image_and_mask(\n",
    "#     top_label,\n",
    "#     positive_only=False,\n",
    "#     num_features=10,\n",
    "#     hide_rest=False\n",
    "# )\n",
    "\n",
    "# # Create visualization\n",
    "# img_boundary = mark_boundaries(temp / 255.0, mask)\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.imshow(image_array)\n",
    "# plt.title('Original Image')\n",
    "# plt.axis('off')\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.imshow(img_boundary)\n",
    "# plt.title('LIME Explanation')\n",
    "# plt.axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Wrapper Function: High-Level Explanation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-level explanation wrapper demonstrated. Provide image path and class names to generate actual explanation.\n"
     ]
    }
   ],
   "source": [
    "# Using wrapper function for complete explanation workflow\n",
    "# Uncomment and provide actual paths to run\n",
    "\n",
    "# class_names = [f\"class_{i}\" for i in range(101)]  # Replace with actual class names\n",
    "# image_path = \"data/test_image.jpg\"  # Replace with actual path\n",
    "\n",
    "# explanation_result = explain_prediction(\n",
    "#     image_path=image_path,\n",
    "#     model=model,\n",
    "#     class_names=class_names,\n",
    "#     device='cpu',\n",
    "#     num_features=10,\n",
    "#     num_samples=1000,\n",
    "#     top_labels=5\n",
    "# )\n",
    "\n",
    "# print(f\"Predicted class: {explanation_result['top_label_name']}\")\n",
    "# print(f\"Top probabilities: {explanation_result['top_probabilities']}\")\n",
    "\n",
    "print(\"High-level explanation wrapper demonstrated. Provide image path and class names to generate actual explanation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization wrapper demonstrated. Provide explanation_result to generate actual visualization.\n"
     ]
    }
   ],
   "source": [
    "# Use wrapper visualization function\n",
    "# Uncomment when explanation_result is available\n",
    "\n",
    "# visualize_explanation(explanation_result)\n",
    "\n",
    "print(\"Visualization wrapper demonstrated. Provide explanation_result to generate actual visualization.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lime",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
