---
title: NBA Analysis
colorFrom: red
colorTo: indigo
sdk: gradio
sdk_version: 5.49.1
app_file: app.py
pinned: false
---

# NBA Data Analysis with CrewAI

An intelligent NBA data analysis application powered by CrewAI multi-agent framework. Upload your NBA CSV data and get comprehensive analysis with insights, statistics, and engaging storylines generated by AI agents.

## Features

- **Multi-Agent AI System**: Three specialized agents (Engineer, Analyst, Storyteller) work together
- **Data Engineering**: Automatic data cleaning and preparation
- **Intelligent Analysis**: AI-powered insights and pattern detection
- **Statistical Analysis**: Top performers, trends, and key metrics
- **Semantic Search**: Natural language queries on your data using vector embeddings
- **Storytelling**: Engaging headlines and narratives from data
- **Parallel Processing**: Tasks run in parallel for faster results
- **Web Interface**: Easy-to-use Gradio web app
- **OpenAI Integration**: Powered by GPT-4o for high-quality analysis

## Architecture

The application uses a multi-agent system with the following components:

- **Data Engineer Agent**: Processes and validates data
- **Data Analyst Agent**: Performs statistical analysis and extracts insights
- **Storyteller Agent**: Creates engaging narratives from analysis results

### Tech Stack

- **CrewAI**: Multi-agent AI framework
- **Gradio**: Web interface
- **Pandas**: Data analysis
- **ChromaDB**: Vector database for semantic search
- **Sentence Transformers**: Embeddings for semantic search
- **OpenAI**: LLM provider (GPT-4o)
- **Docker**: Containerized development environment (optional)

## Prerequisites

- Python 3.11 or 3.12
- pip or uv package manager
- OpenAI API key
- (Optional) Docker for containerized development

## Installation

### 1. Clone the Repository

```bash
git clone <your-repo-url>
cd NBA_Analysis
```

### 2. Install Dependencies

**Using uv (recommended):**
```bash
uv sync
```

**Using pip:**
```bash
pip install -r requirements.txt
```

### 3. Prepare Your Data

Place your NBA CSV file in the project directory, or upload it through the web interface.

## Docker Setup (Optional)

For a consistent development environment, you can use Docker.

### Prerequisites
- Docker installed on your system
- OpenAI API key (set as environment variable)

### Build the Docker Image

```bash
./docker_build.sh
```

This creates a Docker image named `nba-analysis:latest` with all dependencies installed.

### Run Docker Container

**Option 1: Start Jupyter Notebook**
```bash
export OPENAI_API_KEY=your-api-key-here
./docker_jupyter.sh
```

Jupyter will be available at `http://localhost:8888`. Check the terminal output for the access token.

**Option 2: Get a Bash Shell**
```bash
export OPENAI_API_KEY=your-api-key-here
./docker_bash.sh
```

This gives you an interactive shell inside the container where you can run Python scripts or notebooks.

### Docker Scripts

- `docker_build.sh` - Build the Docker image
- `docker_bash.sh` - Start an interactive bash shell
- `docker_jupyter.sh` - Start Jupyter notebook server

**Note**: The container mounts your current directory, so changes to files are reflected immediately.

## Configuration

### OpenAI Configuration

The application uses **OpenAI** as the LLM provider.

1. Get an API key from [OpenAI](https://platform.openai.com/api-keys)
2. Set environment variable:
   ```bash
   export OPENAI_API_KEY=your-openai-api-key
   ```

**Default Model**: `gpt-4o` (can be changed via `OPENAI_MODEL` environment variable)

**Available Models:**
- `gpt-4o` (default, best quality)
- `gpt-4` (high quality)
- `gpt-3.5-turbo` (faster, lower cost)

## Usage

### Web Interface (Recommended)

```bash
python app.py
```

Then open your browser to the URL shown (usually `http://localhost:7860`).

**Features:**
- Upload CSV file
- Enter analysis query (or leave blank for comprehensive analysis)
- Click "Analyze Dataset" for full analysis
- Click "Analyze with Question" for quick queries

### Command Line

```bash
python main.py
```

## Example Queries

- "Who are the top 5 three-point shooters?"
- "Show me the best scoring games this season"
- "Which players have the highest field goal percentage?"
- "Analyze team performance trends"
- "Find games with triple doubles"
- "What are the most efficient shooters?"

## Project Structure

```
NBA_Analysis/
├── crewai_utils.py        # Consolidated utility functions
├── crewai.API.md          # API documentation
├── crewai.API.ipynb       # API examples notebook
├── crewai.example.md      # Example documentation
├── crewai.example.ipynb   # Example notebook
├── app.py                 # Gradio web interface
├── main.py                # Command-line entry point
├── config.py              # LLM and configuration settings
├── agents.py              # AI agent definitions
├── crew.py                # CrewAI crew orchestration
├── tasks.py               # Task definitions
├── tools.py               # Data access tools for agents
├── vector_db.py           # Vector database for semantic search
├── requirements.txt       # Python dependencies
├── pyproject.toml        # Project configuration
├── Dockerfile             # Docker configuration
├── docker_build.sh        # Docker build script
├── docker_bash.sh         # Docker bash script
├── docker_jupyter.sh      # Docker Jupyter script
├── EXECUTION_FLOW.md      # Detailed execution flow documentation
├── README.md              # This file
```

## Available Tools

The agents have access to 5 data tools:

1. **read_nba_data**: Read sample rows to understand structure
2. **search_nba_data**: Filter and search CSV data
3. **get_nba_data_summary**: Get comprehensive dataset overview
4. **semantic_search_nba_data**: Natural language semantic search
5. **analyze_nba_data**: Execute pandas operations for advanced analysis

## Deployment

### Hugging Face Spaces (Free)

1. **Get API Key:**
   - OpenAI API key: https://platform.openai.com/api-keys

2. **Create Space:**
   - Go to https://huggingface.co/spaces
   - Create new Space with Gradio SDK
   - Push your code

3. **Set Secrets:**
   - Space Settings → Repository secrets
   - Add `OPENAI_API_KEY` = your OpenAI API key
   - (Optional) Add `OPENAI_MODEL` = your preferred model (default: `gpt-4o`)

4. **Deploy:**
   ```bash
   git remote add hf https://huggingface.co/spaces/yourusername/nba-analysis
   git push hf main
   ```

See `EXECUTION_FLOW.md` for detailed deployment instructions.

## Local Testing

### Quick Test

```bash
# Set your OpenAI API key
export OPENAI_API_KEY=your-api-key-here

# Run the app
python app.py
```

## How It Works

1. **User Input**: Upload CSV + enter query
2. **Crew Creation**: Three agents are initialized with their roles
3. **Parallel Execution**: 
   - Engineer validates data
   - Analyst performs analysis (runs in parallel)
   - Storyteller creates narrative (waits for Analyst)
4. **Tool Execution**: Agents use tools to access and analyze data
5. **LLM Processing**: AI generates insights and responses
6. **Result Aggregation**: All outputs are combined and formatted
7. **Display**: Results shown to user

See `EXECUTION_FLOW.md` for detailed flow documentation.

## Key Features Explained

### Semantic Search
Uses vector embeddings to find semantically similar records. First run indexes the CSV, subsequent runs use cached embeddings.

### Parallel Processing
Engineer and Analyst tasks run simultaneously for faster results. Storyteller waits for Analyst to complete.

### Multi-Agent Collaboration
Each agent has a specialized role:
- **Engineer**: Data quality and structure
- **Analyst**: Statistical analysis and insights
- **Storyteller**: Narrative and presentation

## Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `OPENAI_API_KEY` | OpenAI API key | **Required** |
| `OPENAI_MODEL` | OpenAI model name | `gpt-4o` |

## Troubleshooting

### "ModuleNotFoundError: No module named 'crewai'"
- Install dependencies: `pip install -r requirements.txt` or `uv sync`
- Or use Docker: `./docker_build.sh` then `./docker_bash.sh`

### "OPENAI_API_KEY not set"
- Set your OpenAI API key: `export OPENAI_API_KEY=your-key`
- In Docker: Pass it via `-e OPENAI_API_KEY=your-key` or use `docker_jupyter.sh` which handles it

### "Docker build fails"
- Make sure Docker is installed and running
- Check internet connection (needs to download base image and packages)
- Try: `docker system prune` to free up space

### Slow responses
- OpenAI API calls depend on your internet connection
- Consider using `gpt-3.5-turbo` for faster (but lower quality) responses
- Check OpenAI API status: https://status.openai.com

## License

This project is open source. Check individual dependencies for their licenses.

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## Documentation

- **Execution Flow**: See `EXECUTION_FLOW.md` for detailed flow
- **CrewAI Docs**: https://docs.crewai.com
- **Gradio Docs**: https://gradio.app/docs

## What Was Built

This project demonstrates:
- Multi-agent AI systems with CrewAI
- Parallel task execution
- Semantic search with vector databases
- Integration with OpenAI API
- Web interface with Gradio
- Docker containerization
- Free-tier deployment on Hugging Face Spaces

## Tips

- **First Run**: Vector DB indexing takes time on first use
- **Large Files**: Use semantic search for large datasets
- **Complex Queries**: Use "Analyze with Question" for specific queries
- **Model Selection**: `gpt-4o` = best quality, `gpt-3.5-turbo` = faster/cheaper
- **Docker**: Use Docker for consistent development environment

## Links

- **OpenAI**: https://platform.openai.com
- **CrewAI**: https://docs.crewai.com
- **Gradio**: https://gradio.app

---

**Built with CrewAI and OpenAI**
