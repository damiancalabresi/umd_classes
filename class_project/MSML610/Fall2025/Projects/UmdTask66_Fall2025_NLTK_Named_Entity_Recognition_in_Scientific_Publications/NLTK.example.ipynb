{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2588ac52-ebfb-4409-9f5f-c78a021a8434",
   "metadata": {},
   "source": [
    "# NER on CORD-19 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ab44c8-5e19-4cc5-a757-7f79cc1e59a1",
   "metadata": {},
   "source": [
    "## 1. Import necessary modules and set the environment\n",
    "\n",
    "`setup_nltk` function checks and downloads all necessary NLTK modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "535d001c-117f-4212-8aa5-4cd5de1f1fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download necessary packages\n",
    "# %pip install nltk\n",
    "# %pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93590756-62ba-4c3a-adc4-b3776f63ff95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking NLTK packages...\n",
      "Checking NLTK packages...\n",
      "Downloading spaCy model...\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import tarfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from NLTK_utils import download_dataset, setup_nltk, stream_cord19_data, clean_text, extract_entities_nltk, extract_entities_spacy, calculate_metrics\n",
    "\n",
    "# PROJECT_ROOT = Path(__file__).resolve().parent\n",
    "DATA_DIR = \"data/\"\n",
    "CORD19_FILENAME = 'cord-19_2022-06-02.tar.gz'\n",
    "CORD19_FILE_PATH = DATA_DIR + CORD19_FILENAME\n",
    "CORD19_URL = \"https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2022-06-02.tar.gz\"\n",
    "OUTPUT_DIR = \"output/\"\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "setup_nltk()\n",
    "# download spacy model\n",
    "print(\"Downloading spaCy model...\")\n",
    "spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154ac453-995c-47ac-a5ac-c4585fcc104f",
   "metadata": {},
   "source": [
    "## 2. Download the dataset\n",
    "\n",
    "Since the whole dataset is very big, we implement a reliable download function supporting resumable downloads and streaming processing. The definition of this function is:\n",
    "```python\n",
    "download_dataset(url, dest_path, chunk_size=1024*1024)\n",
    "```\n",
    "\n",
    "Parameters:\n",
    "- `url`: the link to download the dataset.\n",
    "- `dest_path`: the path where dataset is to be stored. Note that the dataset is a compressed file, and therefore the `dest_path` is a compressed folder ending with `.tar.gz`. We use a stream processor `stream_cord19_data` to read this compressed file without uncompressing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d05631d5-ba46-49a0-bf3a-5903a80ac41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists and is complete: data/cord-19_2022-06-02.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# 2. download dataset\n",
    "download_dataset(CORD19_URL, CORD19_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6c632e-1142-4be6-b2e4-c1578c2faae5",
   "metadata": {},
   "source": [
    "## 3. Hyperparameters configuration\n",
    "\n",
    "There are 2 hyperparameters:\n",
    "- **MAX_NUMBER:** indicates the total number of articles that are to be analyzed. The dataset is very big and contains thousands of articles. Therefore, it is nearly impossible to analyze the whole dataset and we only choose the first **MAX_NUMBER** articles to analyze.\n",
    "- **MAX_LENGTH:** indicates where to truncate the `cleaned_text`. Because the main texts of published articles are usually long, it is a useful trick to only analyze the first **MAX_LENGTH** cleaned characters in order to get a quicker demonstration. Besides, if **MAX_LENGTH** is set to `None`, our analysis code would analyze the whole text.\n",
    "\n",
    "You can configure the parameters in the below cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56cae004-468b-4b7a-b1bb-7fcc34da13c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUMBER = 10\n",
    "MAX_LENGTH = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a8d451-3f0c-4b63-ba3b-4904a300d05c",
   "metadata": {},
   "source": [
    "## 4. perform analysis\n",
    "\n",
    "The whole analysis process is integrated in the function `analyze` in the below cell. It invokes 2 separate entities extraction function and 1 performance analysis function. \n",
    "\n",
    "### Entities extraction\n",
    "\n",
    "The entities extraction function of NLTK is defined as:\n",
    "```python\n",
    "extract_entities_nltk(text)\n",
    "```\n",
    "\n",
    "The only parameter is the input text.\n",
    "\n",
    "The entities extraction function of spaCy is defined as:\n",
    "```python\n",
    "extract_entities_spacy(text, nlp_model)\n",
    "```\n",
    "There are two parameters: `text` for the input text; and `nlp_model` for the spaCy model.\n",
    "\n",
    "Both functions above return a list of entities.\n",
    "\n",
    "### Performance analysis\n",
    "\n",
    "The performance analysis uses relative performance to assess the results of NLTK. Since the dataset is unlabeled, there is no ground truth to evaluate the accuracy, recall and F-1 score of NLTK methods. Therefore, we regard the mature spaCy method as the standard, and calculate the relative performance of NLTK to spaCy. The definition of the performance analysis function is: \n",
    "```python\n",
    "calculate_metrics(reference_entities, candidate_entities)\n",
    "```\n",
    "\n",
    "- `reference_entities`: the result list of entities by the standard method that we choose, which is spaCy.\n",
    "- `candidate_entities`: the result list of entities by the method that we are to assess, which is NLTK.\n",
    "\n",
    "This function returns a dictionary that contains all performance metrics, of which the shape is:\n",
    "```python\n",
    "{\n",
    "    \"precision\": round(precision, 4),\n",
    "    \"recall\": round(recall, 4),\n",
    "    \"f1_score\": round(f1, 4),\n",
    "    \"overlap_count\": tp,\n",
    "    \"nltk_only_count\": fp,\n",
    "    \"spacy_only_count\": fn\n",
    "}\n",
    "```\n",
    "\n",
    "Note that the entities extracted per paper and paper-wise performance analysis are stored in `output/` as `.csv` files, in which more details can be inspected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68cb3ae3-9b13-4dee-8236-f9a01e72403a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing models...\n",
      "\n",
      "--- Processing 10 papers from CORD-19 ---\n",
      "Opening dataset: data/cord-19_2022-06-02.tar.gz...\n",
      "Processing [1/10]: d1aafb70c066a2068b02786f8929fd9c900897fb\n",
      "Processing [2/10]: PMC35282\n",
      "Processing [3/10]: 6b0567729c2143a66d737eb0a2f63f2dce2e5a7d\n",
      "Processing [4/10]: PMC59543\n",
      "Processing [5/10]: 06ced00a5fc04215949aa72528f2eeaae1d58927\n",
      "Processing [6/10]: PMC59549\n",
      "Processing [7/10]: 348055649b6b8cf2b9a376498df9bf41f7123605\n",
      "Processing [8/10]: PMC59574\n",
      "Processing [9/10]: 5f48792a5fa08bed9f56016f4981ae2ca6031b32\n",
      "Processing [10/10]: PMC59580\n",
      "\n",
      "[Saved] All extracted entities saved to: output/extracted_entities.csv\n",
      "[Saved] Performance metrics saved to: output/performance_metrics.csv\n",
      "\n",
      "========================================\n",
      "Average Performance (NLTK vs spaCy as Baseline)\n",
      "========================================\n",
      "precision    0.53916\n",
      "recall       0.27565\n",
      "f1_score     0.33390\n",
      "dtype: float64\n",
      "========================================\n",
      "Note: Since CORD-19 is unlabeled, we treat spaCy results as the\n",
      "'Silver Standard' (Ground Truth) to evaluate NLTK's relative performance.\n"
     ]
    }
   ],
   "source": [
    "def analyze():\n",
    "    # initialize models\n",
    "    print(\"Initializing models...\")\n",
    "    \n",
    "    # load spaCy\n",
    "    try:\n",
    "        nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
    "    except OSError:\n",
    "        print(\"SpaCy model not found. Please run: python -m spacy download en_core_web_sm\")\n",
    "        return\n",
    "\n",
    "    # list to store results\n",
    "    all_entities_data = []\n",
    "    performance_metrics = []\n",
    "\n",
    "    # process the first MAX_NUMBER papers\n",
    "    MAX_NUMBER = 10\n",
    "    print(f\"\\n--- Processing {MAX_NUMBER} papers from CORD-19 ---\")\n",
    "    \n",
    "    paper_generator = stream_cord19_data(CORD19_FILE_PATH, limit=MAX_NUMBER)\n",
    "    \n",
    "    for i, paper in enumerate(paper_generator):\n",
    "        paper_id = paper['id']\n",
    "        print(f\"Processing [{i+1}/{MAX_NUMBER}]: {paper_id}\")\n",
    "        \n",
    "        cleaned_text = clean_text(paper['text'])\n",
    "        \n",
    "        # 1. run the models\n",
    "        # notice: in order for fair comparison, we apply same truncation for different methods.\n",
    "        # for quick demonstration, we use the first MAX_LENGTH characters \n",
    "        if MAX_LENGTH:\n",
    "            eval_text = cleaned_text[:MAX_LENGTH]\n",
    "        else:\n",
    "            eval_text = cleaned_text\n",
    "        \n",
    "        ents_nltk = extract_entities_nltk(eval_text)\n",
    "        ents_spacy = extract_entities_spacy(eval_text, nlp_spacy)\n",
    "        \n",
    "        # 2. store the output for following analysis\n",
    "        for ent in ents_nltk:\n",
    "            all_entities_data.append({'paper_id': paper_id, 'model': 'NLTK', 'entity': ent})\n",
    "        for ent in ents_spacy:\n",
    "            all_entities_data.append({'paper_id': paper_id, 'model': 'spaCy', 'entity': ent})\n",
    "            \n",
    "        # 3. compute the performance (use spaCy as the Silver Standard)\n",
    "        metrics = calculate_metrics(reference_entities=ents_spacy, candidate_entities=ents_nltk)\n",
    "        metrics['paper_id'] = paper_id\n",
    "        performance_metrics.append(metrics)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # store and display the results\n",
    "    # ---------------------------------------------------------\n",
    "    \n",
    "    # store lists of entities\n",
    "    df_entities = pd.DataFrame(all_entities_data)\n",
    "    entities_csv_path = OUTPUT_DIR + \"extracted_entities.csv\"\n",
    "    df_entities.to_csv(entities_csv_path, index=False)\n",
    "    print(f\"\\n[Saved] All extracted entities saved to: {entities_csv_path}\")\n",
    "    \n",
    "    # store performances\n",
    "    df_perf = pd.DataFrame(performance_metrics)\n",
    "    perf_csv_path = OUTPUT_DIR + \"performance_metrics.csv\"\n",
    "    df_perf.to_csv(perf_csv_path, index=False)\n",
    "    print(f\"[Saved] Performance metrics saved to: {perf_csv_path}\")\n",
    "    \n",
    "    # print average performances\n",
    "    if not df_perf.empty:\n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(\"Average Performance (NLTK vs spaCy as Baseline)\")\n",
    "        print(\"=\"*40)\n",
    "        print(df_perf[['precision', 'recall', 'f1_score']].mean())\n",
    "        print(\"=\"*40)\n",
    "        print(\"Note: Since CORD-19 is unlabeled, we treat spaCy results as the\")\n",
    "        print(\"'Silver Standard' (Ground Truth) to evaluate NLTK's relative performance.\")\n",
    "\n",
    "# run the analysis\n",
    "analyze()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
