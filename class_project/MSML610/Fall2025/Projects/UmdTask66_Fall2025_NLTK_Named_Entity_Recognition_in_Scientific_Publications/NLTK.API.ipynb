{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef1cfcf2-5d2f-4cb5-a5e3-412927ea9c45",
   "metadata": {},
   "source": [
    "# NLTK API Reference Guide\n",
    "\n",
    "This notebook serves as a concise reference for the NLTK (Natural Language Toolkit) functions used in our Named Entity Recognition (NER) project. We will explore the core pipeline: Tokenization -> POS Tagging -> Named Entity Chunking.\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "First, ensure NLTK and the required data packages are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb8cd5d7-0331-45a0-98df-92cbba15bdb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK setup complete.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download necessary resources\n",
    "required_packages = [\n",
    "    'punkt', \n",
    "    'averaged_perceptron_tagger', \n",
    "    'maxent_ne_chunker', \n",
    "    'words',\n",
    "    'punkt_tab',\n",
    "    'maxent_ne_chunker_tab'\n",
    "]\n",
    "\n",
    "for package in required_packages:\n",
    "    nltk.download(package, quiet=True)\n",
    "\n",
    "print(\"NLTK setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4c9abc-90fb-4add-8e79-0e12a04bc4c5",
   "metadata": {},
   "source": [
    "## 2. Tokenization: nltk.word_tokenize\n",
    "\n",
    "Function: Splits a text string into individual words and punctuation marks (tokens). This is the first step in most NLP pipelines.\n",
    "\n",
    "Usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec277f59-a5cd-4f6f-92a3-1a3cc906c69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Dr. Smith works at the University of Maryland.\n",
      "Tokens:   ['Dr.', 'Smith', 'works', 'at', 'the', 'University', 'of', 'Maryland', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Dr. Smith works at the University of Maryland.\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "print(f\"Original: {text}\")\n",
    "print(f\"Tokens:   {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093d45d6-fb79-443b-8063-f529f18d0291",
   "metadata": {},
   "source": [
    "## 3. Part-of-Speech (POS) Tagging: nltk.pos_tag\n",
    "\n",
    "Function: Assigns a grammatical category (tag) to each token, such as Noun (NN), Verb (VB), or Proper Noun (NNP). These tags are crucial for identifying entities.\n",
    "\n",
    "Common Tags:\n",
    "\n",
    "- NNP: Proper Noun, Singular (e.g., \"Maryland\")\n",
    "- DT: Determiner (e.g., \"the\")\n",
    "- IN: Preposition (e.g., \"at\")\n",
    "\n",
    "Usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f9a50fc-ee2e-4c8b-8a16-5e71082d7c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags:\n",
      "Dr.: NNP\n",
      "Smith: NNP\n",
      "works: VBZ\n",
      "at: IN\n",
      "the: DT\n",
      "University: NNP\n",
      "of: IN\n",
      "Maryland: NNP\n",
      ".: .\n"
     ]
    }
   ],
   "source": [
    "# # download resources\n",
    "# nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "# Requires a list of tokens as input\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "print(\"POS Tags:\")\n",
    "for word, tag in pos_tags:\n",
    "    print(f\"{word}: {tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b89b18-5cfc-4c93-bc67-6ee2cf83e5cd",
   "metadata": {},
   "source": [
    "## 4. Named Entity Chunking: nltk.ne_chunk\n",
    "\n",
    "Function: Takes a list of POS-tagged tokens and groups them into \"chunks\" that represent named entities (like Persons, Organizations, Locations). It returns a Tree structure.\n",
    "\n",
    "Usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17ffbfb5-25e4-4897-80ad-027b9d111d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Tree Structure:\n",
      "(S\n",
      "  Dr./NNP\n",
      "  (PERSON Smith/NNP)\n",
      "  works/VBZ\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION University/NNP)\n",
      "  of/IN\n",
      "  (GPE Maryland/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# Requires POS tags as input\n",
    "chunks = nltk.ne_chunk(pos_tags)\n",
    "\n",
    "print(\"Chunk Tree Structure:\")\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f39d368-aced-47f3-ae58-ade50321ef05",
   "metadata": {},
   "source": [
    "## 5. Extracting Entities from the Tree\n",
    "\n",
    "The output of `ne_chunk` is a tree. To get the actual entity names, we need to traverse this tree and look for subtrees that have a label (like 'PERSON' or 'ORGANIZATION').\n",
    "\n",
    "Usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3d9172a-6ba9-42bd-8fb6-e44b4e841d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Entities:\n",
      "Entity: Smith | Type: PERSON\n",
      "Entity: University | Type: ORGANIZATION\n",
      "Entity: Maryland | Type: GPE\n"
     ]
    }
   ],
   "source": [
    "entities = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    # Check if the chunk is a named entity (has a label)\n",
    "    if hasattr(chunk, 'label'):\n",
    "        label = chunk.label()\n",
    "        # Rejoin the tokens to form the full entity name\n",
    "        entity_name = ' '.join(c[0] for c in chunk)\n",
    "        entities.append((entity_name, label))\n",
    "\n",
    "print(\"Extracted Entities:\")\n",
    "for name, label in entities:\n",
    "    print(f\"Entity: {name} | Type: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e76c4da-f8fa-4971-996a-5bfd4d07631f",
   "metadata": {},
   "source": [
    "## Summary of the Pipeline\n",
    "1. **Input**: Raw Text String\n",
    "2. `word_tokenize` -> List of Strings (Tokens)\n",
    "3. `pos_tag` -> List of Tuples (Word, Tag)\n",
    "4. `ne_chunk` -> Tree Object\n",
    "5. `Traversal` -> List of Entities"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
