{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98e14d41",
   "metadata": {},
   "source": [
    "# Tetrad Example\n",
    "\n",
    "This script will be an example of how to use Tetrad to analyze causal relationships with regard to user posts taken from Twitter and associated stock market data over time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6449a137",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T22:37:11.251580Z",
     "start_time": "2025-12-05T22:37:05.954962Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/cmu-phil/py-tetrad\n",
      "  Cloning https://github.com/cmu-phil/py-tetrad to /tmp/pip-req-build-5o9dvfo6\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/cmu-phil/py-tetrad /tmp/pip-req-build-5o9dvfo6\n",
      "  Resolved https://github.com/cmu-phil/py-tetrad to commit 275850d12bb29e392580ee96bff633928a8296bb\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.4.1)\n",
      "Requirement already satisfied: JPype1 in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.3.5)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (1.2.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.7.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.3.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!sudo pip install -U --break-system-packages datasets JPype1 git+https://github.com/cmu-phil/py-tetrad\n",
    "#!sudo pip install -U --break-system-packages torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0252c5d",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6536ecb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T22:38:33.289927Z",
     "start_time": "2025-12-05T22:38:32.663266Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import pytetrad.tools.translate as ptt\n",
    "import pytetrad.tools.TetradSearch as ts\n",
    "import edu.cmu.tetrad.util as util\n",
    "import edu.cmu.tetrad.data as td\n",
    "import edu.cmu.tetrad.algcomparison.simulation as sim\n",
    "import edu.cmu.tetrad.algcomparison.algorithm.multi as multi\n",
    "import java.util as jutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a25e4f",
   "metadata": {},
   "source": [
    "## Import and Clean Tweet Sentiment Data\n",
    "Using the huggingface dataset emad12/stock_tweets_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39573b92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T22:38:41.208626Z",
     "start_time": "2025-12-05T22:38:34.714202Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "splits = {'train': 'data/train-00000-of-00001-49baa0648effea14.parquet', 'test': 'data/test-00000-of-00001-cb0233e05c1cc1c9.parquet'}\n",
    "twt_df = pd.read_parquet(\"hf://datasets/emad12/stock_tweets_sentiment/\" + splits[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99d769b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T22:38:42.651619Z",
     "start_time": "2025-12-05T22:38:42.301703Z"
    }
   },
   "outputs": [],
   "source": [
    "tweet_df = twt_df.copy(deep=True)\n",
    "\n",
    "# Function for standardizing the various date formats in the dataset\n",
    "def fix_dates(r):\n",
    "    first_str = r['post_date'].split(None, 1)[0]\n",
    "    if '-' in first_str:\n",
    "        return first_str\n",
    "    elif first_str.isdigit():\n",
    "        return datetime.fromtimestamp(int(first_str))\n",
    "    else:\n",
    "        return datetime.strptime(r['post_date'], '%a %b %d %X %z %Y').strftime('%Y-%m-%d')\n",
    "\n",
    "# Normalize date format\n",
    "tweet_df['post_date'] = tweet_df.apply(fix_dates, axis=1)\n",
    "tweet_df['post_date'] = pd.to_datetime(tweet_df['post_date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Filter for only those ticker symbols that have a large amount of data\n",
    "tweet_df = tweet_df[tweet_df['ticker_symbol'].isin(['AMZN', 'AAPL', 'TSLA', 'GOOG', 'MSFT'])]\n",
    "\n",
    "# Drop columns we won't be using\n",
    "tweet_df = tweet_df.drop(columns=['Unnamed: 0', 'tweet', 'tweet_cleaned', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'])\n",
    "\n",
    "# Rename columns for later when we combine this dataset with the stock price dataset\n",
    "tweet_df = tweet_df.rename(columns={'post_date': 'Date', 'sentiment': 'Sentiment', 'ticker_symbol': 'Ticker'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c494987a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T22:38:43.628726Z",
     "start_time": "2025-12-05T22:38:43.613355Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticker\n",
      "TSLA    34440\n",
      "AAPL    21281\n",
      "AMZN    10026\n",
      "GOOG     6132\n",
      "MSFT     2563\n",
      "Name: count, dtype: int64\n",
      "2015-01-01\n",
      "2019-12-31\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-08-26</td>\n",
       "      <td>0</td>\n",
       "      <td>AMZN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-09-10</td>\n",
       "      <td>1</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-05-16</td>\n",
       "      <td>0</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-01-24</td>\n",
       "      <td>1</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017-08-04</td>\n",
       "      <td>-1</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Sentiment Ticker\n",
       "2  2015-08-26          0   AMZN\n",
       "3  2019-09-10          1   AAPL\n",
       "4  2019-05-16          0   AAPL\n",
       "5  2018-01-24          1   TSLA\n",
       "6  2017-08-04         -1   TSLA"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tweet_df['Ticker'].value_counts())\n",
    "print(tweet_df['Date'].min())\n",
    "print(tweet_df['Date'].max())\n",
    "tweet_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6c0cf4",
   "metadata": {},
   "source": [
    "## Import and Clean Historical Stock Data for Selected Stocks\n",
    "Using the huggingface dataset no-ry/world-stock-prices-daily-updating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd6d2aa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T22:38:48.743582Z",
     "start_time": "2025-12-05T22:38:46.860208Z"
    }
   },
   "outputs": [],
   "source": [
    "st_df = pd.read_csv(\"hf://datasets/no-ry/world-stock-prices-daily-updating/World-Stock-Prices-Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e606e2e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T22:38:50.156175Z",
     "start_time": "2025-12-05T22:38:49.434465Z"
    }
   },
   "outputs": [],
   "source": [
    "stock_df = st_df.copy(deep=True)\n",
    "# Convert dates to a standardized format\n",
    "stock_df['Date'] = pd.to_datetime(stock_df['Date'], utc=True).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Drop columns we will not be using\n",
    "stock_df = stock_df.drop(columns=['Industry_Tag', 'Country', 'Dividends', 'Stock Splits', 'Capital Gains', 'Brand_Name'])\n",
    "\n",
    "# Keep only those ticker symbols for which we have sufficient sentiment data\n",
    "stock_df = stock_df[stock_df['Ticker'].isin(['AMZN', 'AAPL', 'TSLA', 'GOOG', 'MSFT'])]\n",
    "\n",
    "# Remove duplicate records \n",
    "stock_df = stock_df.drop_duplicates(subset=['Date', 'Ticker'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b9529ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T22:38:50.659768Z",
     "start_time": "2025-12-05T22:38:50.649814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000-01-03\n",
      "2025-07-03\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>2025-07-03</td>\n",
       "      <td>212.149994</td>\n",
       "      <td>214.649994</td>\n",
       "      <td>211.809998</td>\n",
       "      <td>213.550003</td>\n",
       "      <td>34955800.0</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2025-07-03</td>\n",
       "      <td>493.809998</td>\n",
       "      <td>500.130005</td>\n",
       "      <td>493.440002</td>\n",
       "      <td>498.839996</td>\n",
       "      <td>13984800.0</td>\n",
       "      <td>MSFT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2025-07-03</td>\n",
       "      <td>221.820007</td>\n",
       "      <td>224.009995</td>\n",
       "      <td>221.360001</td>\n",
       "      <td>223.410004</td>\n",
       "      <td>29632400.0</td>\n",
       "      <td>AMZN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2025-07-03</td>\n",
       "      <td>317.989990</td>\n",
       "      <td>318.450012</td>\n",
       "      <td>312.760010</td>\n",
       "      <td>315.350006</td>\n",
       "      <td>58042300.0</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>2025-07-02</td>\n",
       "      <td>219.729996</td>\n",
       "      <td>221.600006</td>\n",
       "      <td>219.059998</td>\n",
       "      <td>219.919998</td>\n",
       "      <td>30840800.0</td>\n",
       "      <td>AMZN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        Open        High         Low       Close      Volume  \\\n",
       "86   2025-07-03  212.149994  214.649994  211.809998  213.550003  34955800.0   \n",
       "96   2025-07-03  493.809998  500.130005  493.440002  498.839996  13984800.0   \n",
       "97   2025-07-03  221.820007  224.009995  221.360001  223.410004  29632400.0   \n",
       "99   2025-07-03  317.989990  318.450012  312.760010  315.350006  58042300.0   \n",
       "111  2025-07-02  219.729996  221.600006  219.059998  219.919998  30840800.0   \n",
       "\n",
       "    Ticker  \n",
       "86    AAPL  \n",
       "96    MSFT  \n",
       "97    AMZN  \n",
       "99    TSLA  \n",
       "111   AMZN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(stock_df['Date'].min())\n",
    "print(stock_df['Date'].max())\n",
    "stock_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9be5e38",
   "metadata": {},
   "source": [
    "## Combine the Datasets \n",
    "Before we can analyze this data with Tetrad, we need to combine these two datasets, keeping relevant columns and creating new aggregated fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06913e33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T22:38:52.825035Z",
     "start_time": "2025-12-05T22:38:52.802445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-01-02\n",
      "2019-12-31\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet_Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>28.086000</td>\n",
       "      <td>26.805332</td>\n",
       "      <td>27.888666</td>\n",
       "      <td>154285500.0</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>-0.057143</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>92.099998</td>\n",
       "      <td>92.663002</td>\n",
       "      <td>91.611504</td>\n",
       "      <td>92.391998</td>\n",
       "      <td>50130000.0</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>70.707799</td>\n",
       "      <td>71.622344</td>\n",
       "      <td>70.607807</td>\n",
       "      <td>71.615028</td>\n",
       "      <td>100805600.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>151.376144</td>\n",
       "      <td>152.341738</td>\n",
       "      <td>151.067147</td>\n",
       "      <td>152.274139</td>\n",
       "      <td>18369400.0</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>153.519779</td>\n",
       "      <td>153.548745</td>\n",
       "      <td>151.337527</td>\n",
       "      <td>152.167938</td>\n",
       "      <td>16348400.0</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date        Open        High         Low       Close       Volume  \\\n",
       "0  2019-12-31   27.000000   28.086000   26.805332   27.888666  154285500.0   \n",
       "1  2019-12-31   92.099998   92.663002   91.611504   92.391998   50130000.0   \n",
       "2  2019-12-31   70.707799   71.622344   70.607807   71.615028  100805600.0   \n",
       "3  2019-12-31  151.376144  152.341738  151.067147  152.274139   18369400.0   \n",
       "4  2019-12-30  153.519779  153.548745  151.337527  152.167938   16348400.0   \n",
       "\n",
       "  Ticker  Sentiment  Tweet_Volume  \n",
       "0   TSLA  -0.057143          35.0  \n",
       "1   AMZN   0.266667          15.0  \n",
       "2   AAPL   0.176471          17.0  \n",
       "3   MSFT  -1.000000           1.0  \n",
       "4   MSFT   0.166667           6.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combining the two datasets on the date and ticker symbol\n",
    "agg_tweet_df = tweet_df.groupby(['Date', 'Ticker'], as_index=False).agg(\n",
    "    Sentiment=pd.NamedAgg(column='Sentiment', aggfunc='mean'),\n",
    "    Tweet_Volume=pd.NamedAgg(column='Sentiment', aggfunc='count'))\n",
    "merged_df = stock_df.merge(agg_tweet_df, on=['Date', 'Ticker'])\n",
    "merged_df['Tweet_Volume'] = merged_df['Tweet_Volume'].astype(float)\n",
    "print(merged_df['Date'].min())\n",
    "print(merged_df['Date'].max())\n",
    "merged_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9214f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T02:53:40.262724Z",
     "start_time": "2025-11-24T02:53:40.257060Z"
    }
   },
   "source": [
    "## Split the Dataset by Ticker Symbol, Normalize, and Create Features\n",
    "Since each ticker symbol has its own magnitudes of data, we will normalize the \"Volume\" column and calculate deltas after splitting the dataset by ticker symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "848162b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T22:38:54.752637Z",
     "start_time": "2025-12-05T22:38:54.728689Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stock_Volume</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet_Volume</th>\n",
       "      <th>Daily_Stock_Close_Delta</th>\n",
       "      <th>Daily_Stock_Low_Delta</th>\n",
       "      <th>Daily_Stock_High_Delta</th>\n",
       "      <th>EMA_Delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4582</th>\n",
       "      <td>0.123139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.015973</td>\n",
       "      <td>-0.043119</td>\n",
       "      <td>0.001705</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4578</th>\n",
       "      <td>0.141477</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.020788</td>\n",
       "      <td>-0.034444</td>\n",
       "      <td>0.009089</td>\n",
       "      <td>0.015512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4574</th>\n",
       "      <td>0.168598</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.005808</td>\n",
       "      <td>-0.027849</td>\n",
       "      <td>0.019709</td>\n",
       "      <td>0.019588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4569</th>\n",
       "      <td>0.083010</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.010291</td>\n",
       "      <td>-0.013157</td>\n",
       "      <td>0.004652</td>\n",
       "      <td>0.003749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4565</th>\n",
       "      <td>0.120221</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.010818</td>\n",
       "      <td>-0.018955</td>\n",
       "      <td>0.005074</td>\n",
       "      <td>0.013825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4564</th>\n",
       "      <td>0.159139</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.004137</td>\n",
       "      <td>-0.018715</td>\n",
       "      <td>0.006993</td>\n",
       "      <td>0.027372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4560</th>\n",
       "      <td>0.114423</td>\n",
       "      <td>-0.315789</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.004574</td>\n",
       "      <td>-0.011853</td>\n",
       "      <td>0.021100</td>\n",
       "      <td>0.016799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4555</th>\n",
       "      <td>0.329185</td>\n",
       "      <td>-0.157895</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.036915</td>\n",
       "      <td>-0.004467</td>\n",
       "      <td>0.050422</td>\n",
       "      <td>0.073477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4554</th>\n",
       "      <td>0.136863</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.013471</td>\n",
       "      <td>-0.023086</td>\n",
       "      <td>0.006479</td>\n",
       "      <td>0.016890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4551</th>\n",
       "      <td>0.087889</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.012428</td>\n",
       "      <td>-0.005506</td>\n",
       "      <td>0.019874</td>\n",
       "      <td>0.024515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Stock_Volume  Sentiment  Tweet_Volume  Daily_Stock_Close_Delta  \\\n",
       "4582      0.123139   0.000000           5.0                -0.015973   \n",
       "4578      0.141477   0.000000           4.0                -0.020788   \n",
       "4574      0.168598   0.333333           3.0                 0.005808   \n",
       "4569      0.083010  -0.333333           3.0                -0.010291   \n",
       "4565      0.120221  -0.750000           4.0                -0.010818   \n",
       "4564      0.159139   0.166667           6.0                -0.004137   \n",
       "4560      0.114423  -0.315789          19.0                 0.004574   \n",
       "4555      0.329185  -0.157895          19.0                 0.036915   \n",
       "4554      0.136863   0.333333           3.0                -0.013471   \n",
       "4551      0.087889  -1.000000           2.0                 0.012428   \n",
       "\n",
       "      Daily_Stock_Low_Delta  Daily_Stock_High_Delta  EMA_Delta  \n",
       "4582              -0.043119                0.001705   0.000000  \n",
       "4578              -0.034444                0.009089   0.015512  \n",
       "4574              -0.027849                0.019709   0.019588  \n",
       "4569              -0.013157                0.004652   0.003749  \n",
       "4565              -0.018955                0.005074   0.013825  \n",
       "4564              -0.018715                0.006993   0.027372  \n",
       "4560              -0.011853                0.021100   0.016799  \n",
       "4555              -0.004467                0.050422   0.073477  \n",
       "4554              -0.023086                0.006479   0.016890  \n",
       "4551              -0.005506                0.019874   0.024515  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A function to help normalize a column\n",
    "def normalize_column(column, dataframe):\n",
    "    return (dataframe[column] - dataframe[column].min()) \\\n",
    "        / (dataframe[column].max() - dataframe[column].min())\n",
    "\n",
    "dfs = {}\n",
    "symbols = merged_df['Ticker'].unique()\n",
    "for symbol in symbols:\n",
    "    # Filter per ticker symbol\n",
    "    dfs[symbol] = merged_df[merged_df['Ticker'] == symbol].sort_values('Date').copy(deep=True)\n",
    "    \n",
    "    # Normalize volume\n",
    "    dfs[symbol]['Volume'] = normalize_column('Volume', dfs[symbol])\n",
    "    \n",
    "    # Create delta columns as a daily price movement indicators\n",
    "    dfs[symbol]['Daily_Stock_Close_Delta'] = (dfs[symbol]['Close'] - dfs[symbol]['Open']) / dfs[symbol]['Open']\n",
    "    dfs[symbol]['Daily_Stock_Low_Delta'] = (dfs[symbol]['Low'] - dfs[symbol]['Open']) / dfs[symbol]['Open']\n",
    "    dfs[symbol]['Daily_Stock_High_Delta'] = (dfs[symbol]['High'] - dfs[symbol]['Open']) / dfs[symbol]['Open']\n",
    "\n",
    "    # Create exponential moving average to capture a bit of history\n",
    "    dfs[symbol]['EMA'] = dfs[symbol]['Open'].ewm(span=5).mean()\n",
    "    dfs[symbol]['EMA_Delta'] = (dfs[symbol]['EMA'] - dfs[symbol]['Open']) / dfs[symbol]['Open']\n",
    "    \n",
    "    # 5 day of opens against open\n",
    "    dfs[symbol] = dfs[symbol].rename(columns={'Volume': 'Stock_Volume'})\n",
    "    dfs[symbol] = dfs[symbol].drop(columns=['Ticker', 'Date', 'High', 'Low', 'Open', 'Close', 'EMA'])\n",
    "    dfs[symbol] = dfs[symbol].fillna(0)\n",
    "\n",
    "dfs['TSLA'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41af67a",
   "metadata": {},
   "source": [
    "## Set the Prior Knowledge\n",
    "Before running the TetradSearch algorithm, we can apply our prior knowledge to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4360f2f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T22:38:56.320337Z",
     "start_time": "2025-12-05T22:38:56.312453Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set knowledge tiers\n",
    "kno_tier0 = ['EMA_Delta']\n",
    "kno_tier1 = ['Daily_Stock_Low_Delta', 'Daily_Stock_High_Delta', 'Sentiment', 'Stock_Volume', 'Tweet_Volume']\n",
    "kno_tier2 = ['Daily_Stock_Close_Delta']\n",
    "\n",
    "knowledge = td.Knowledge()\n",
    "for col in kno_tier0:\n",
    "    knowledge.addToTier(0, col)\n",
    "for col in kno_tier1:\n",
    "    knowledge.addToTier(1, col)\n",
    "for col in kno_tier2:\n",
    "    knowledge.addToTier(2, col)\n",
    "    \n",
    "# Create a required edge\n",
    "knowledge.setRequired('EMA_Delta', 'Sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baa1544",
   "metadata": {},
   "source": [
    "## Run the FGES Search Algorithm\n",
    "We will use the FGES algorithm to determine the Completed Partially Directed Acyclic Graph (CPDAG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7d3c350",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T22:38:58.052727Z",
     "start_time": "2025-12-05T22:38:57.627794Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dec 05, 2025 10:38:57 PM java.util.prefs.FileSystemPreferences$6 run\n",
      "WARNING: Prefs file removed in background /root/.java/.userPrefs/prefs.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error computing BIC: Graph must not be null.\n",
      "here\n",
      "Graph Nodes:\n",
      "Stock_Volume;Sentiment;Tweet_Volume;Daily_Stock_Close_Delta;Daily_Stock_Low_Delta;Daily_Stock_High_Delta;EMA_Delta;Stock_Volume:1;Sentiment:1;Tweet_Volume:1;Daily_Stock_Close_Delta:1;Daily_Stock_Low_Delta:1;Daily_Stock_High_Delta:1;EMA_Delta:1\n",
      "\n",
      "Graph Edges:\n",
      "1. Daily_Stock_Close_Delta:1 --> Daily_Stock_Close_Delta\n",
      "2. Daily_Stock_Close_Delta:1 --> Daily_Stock_High_Delta:1\n",
      "3. Daily_Stock_Close_Delta:1 --> Daily_Stock_Low_Delta\n",
      "4. Daily_Stock_Close_Delta:1 --> Daily_Stock_Low_Delta:1\n",
      "5. Daily_Stock_Close_Delta:1 --> EMA_Delta\n",
      "6. Daily_Stock_High_Delta --> Daily_Stock_Close_Delta\n",
      "7. Daily_Stock_High_Delta --> Daily_Stock_Low_Delta\n",
      "8. Daily_Stock_High_Delta --> EMA_Delta\n",
      "9. Daily_Stock_High_Delta:1 --> Daily_Stock_Low_Delta\n",
      "10. Daily_Stock_High_Delta:1 --> Stock_Volume:1\n",
      "11. Daily_Stock_Low_Delta --> Daily_Stock_Close_Delta\n",
      "12. Daily_Stock_Low_Delta:1 --> Daily_Stock_High_Delta:1\n",
      "13. Daily_Stock_Low_Delta:1 --> Daily_Stock_Low_Delta\n",
      "14. Daily_Stock_Low_Delta:1 --> Stock_Volume:1\n",
      "15. EMA_Delta:1 --> Daily_Stock_Low_Delta\n",
      "16. EMA_Delta:1 --> Daily_Stock_Low_Delta:1\n",
      "17. EMA_Delta:1 --> EMA_Delta\n",
      "18. Stock_Volume --> Daily_Stock_High_Delta\n",
      "19. Stock_Volume --> Daily_Stock_Low_Delta\n",
      "20. Stock_Volume:1 --> Daily_Stock_Low_Delta\n",
      "21. Stock_Volume:1 --> Stock_Volume\n",
      "22. Stock_Volume:1 --> Tweet_Volume\n",
      "23. Stock_Volume:1 --> Tweet_Volume:1\n",
      "24. Tweet_Volume --> Daily_Stock_Low_Delta\n",
      "25. Tweet_Volume --> Sentiment\n",
      "26. Tweet_Volume --- Stock_Volume\n",
      "27. Tweet_Volume:1 --> Daily_Stock_High_Delta\n",
      "28. Tweet_Volume:1 --> Sentiment:1\n",
      "29. Tweet_Volume:1 --> Stock_Volume\n",
      "30. Tweet_Volume:1 --> Tweet_Volume\n",
      "\n",
      "Graph Attributes:\n",
      "Score: 38975.368161\n",
      "\n",
      "Graph Node Attributes:\n",
      "Score: [Stock_Volume: 2371.0124160105465;Sentiment: -801.7578219680973;Tweet_Volume: -10204.897089177863;Daily_Stock_Close_Delta: 8007.8420155378935;Daily_Stock_Low_Delta: 7640.313961789668;Daily_Stock_High_Delta: 6819.12885276761;EMA_Delta: 7285.325149461144;Stock_Volume:1: 2097.1513003351106;Sentiment:1: -801.7606184130212;Tweet_Volume:1: -10716.961658766762;Daily_Stock_Close_Delta:1: 5786.383723813893;Daily_Stock_Low_Delta:1: 7828.1868400691;Daily_Stock_High_Delta:1: 8163.037577886969;EMA_Delta:1: 5502.363511397162]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a TetradSearch object, set its knowledge, indicate tests to be used\n",
    "fges_search = ts.TetradSearch(dfs['TSLA'])\n",
    "fges_search.set_knowledge(knowledge)\n",
    "fges_search.use_sem_bic()\n",
    "fges_search.use_fisher_z(alpha=0.05)\n",
    "fges_search.set_time_lag(1)\n",
    "\n",
    "\n",
    "# Run the FGES algorithm\n",
    "fges_result = fges_search.run_fges()\n",
    "print(fges_search.get_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26e681d",
   "metadata": {},
   "source": [
    "The error displayed in the above output, \"Error computing BIC,\" is an issue with the py-tetrad library that appears on any FGES-based search. The score attributes indicate the strength of the graph and its constituent nodes, with higher values indicating higher confidence in the determined graph. The individual nodes' scores represent how that node's relationship within the broader graph contributes to the strength of the graph as a whole. \n",
    "\n",
    "These graph edges indicate some degree causality determined by the search algorithm given the data. For example, \"Tweet_Volume --> Sentiment\" indicates that tweet volume has some causal effect on sentiment within this dataset. Similarly, \"Tweet_Volume --- Stock_Volume\" indicates that there is some unoriented relationship between tweet volume and stock volume due to a lack of a collider in the DAGs that comprise this CPDAG. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f2f7e2",
   "metadata": {},
   "source": [
    "## Run the FCI Search Algorithm \n",
    "We will use the FCI algorithm to determine a Partial Ancestral Graph (PAG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3689d4de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T22:39:00.014470Z",
     "start_time": "2025-12-05T22:38:59.825424Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing possible dsep search.\n",
      "Removed Sentiment:1 o-> Tweet_Volume:1 by possible dsep\n",
      "Removed Sentiment:1 o-o EMA_Delta:1 by possible dsep\n",
      "Graph Nodes:\n",
      "Stock_Volume;Sentiment;Tweet_Volume;Daily_Stock_Close_Delta;Daily_Stock_Low_Delta;Daily_Stock_High_Delta;EMA_Delta;Stock_Volume:1;Sentiment:1;Tweet_Volume:1;Daily_Stock_Close_Delta:1;Daily_Stock_Low_Delta:1;Daily_Stock_High_Delta:1;EMA_Delta:1\n",
      "\n",
      "Graph Edges:\n",
      "1. Daily_Stock_Close_Delta:1 --> Daily_Stock_High_Delta:1\n",
      "2. Daily_Stock_Close_Delta:1 o-> Daily_Stock_Low_Delta:1\n",
      "3. Daily_Stock_Close_Delta:1 o-> EMA_Delta\n",
      "4. Daily_Stock_High_Delta --> Daily_Stock_Close_Delta\n",
      "5. Daily_Stock_High_Delta <-> Daily_Stock_Low_Delta\n",
      "6. Daily_Stock_High_Delta:1 --> Tweet_Volume:1\n",
      "7. Daily_Stock_Low_Delta --> Daily_Stock_Close_Delta\n",
      "8. Daily_Stock_Low_Delta:1 --> Daily_Stock_High_Delta:1\n",
      "9. Daily_Stock_Low_Delta:1 --> Tweet_Volume:1\n",
      "10. EMA_Delta --> Sentiment\n",
      "11. Sentiment o-o Sentiment:1\n",
      "12. EMA_Delta:1 o-> Daily_Stock_Low_Delta:1\n",
      "13. EMA_Delta:1 o-> EMA_Delta\n",
      "14. Sentiment:1 o-> Tweet_Volume\n",
      "15. Stock_Volume --> Daily_Stock_High_Delta\n",
      "16. Stock_Volume <-> Daily_Stock_Low_Delta\n",
      "17. Stock_Volume <-> Stock_Volume:1\n",
      "18. Stock_Volume --> Tweet_Volume\n",
      "19. Stock_Volume:1 --> Daily_Stock_High_Delta:1\n",
      "20. Stock_Volume:1 --> Daily_Stock_Low_Delta:1\n",
      "21. Stock_Volume:1 --> Tweet_Volume:1\n",
      "22. Tweet_Volume <-> Stock_Volume:1\n",
      "23. Tweet_Volume:1 --> Tweet_Volume\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a TetradSearch object, set its knowledge, indicate tests to be used\n",
    "fci_search = ts.TetradSearch(dfs['TSLA'])\n",
    "fci_search.set_knowledge(knowledge)\n",
    "fci_search.use_sem_bic()\n",
    "fci_search.use_fisher_z(alpha=0.05)\n",
    "fci_search.set_time_lag(1)\n",
    "\n",
    "# Run the FCI algorithm\n",
    "graph = fci_search.run_fci()\n",
    "print(fci_search.get_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86ea277",
   "metadata": {},
   "source": [
    "These edges in the produced PAG represent relationships between the nodes that can and cannot be determined by the search algorithm. For instance, \"EMA_Delta --> Sentiment\" indicates that EMA_Delta is a cause of Sentiment, though it may be direct or indirect and there may exist and unmeasured confounder. \n",
    "\n",
    "\"EMA_Delta:1 o-> EMA_Delta\" indicates that the previous day's exponential moving average causes that day's exponential moving average, that there is an unmeasured variable that is a cause of both, or that both of the previous statements are true. \n",
    "\n",
    "\"Tweet_Volume <-> Stock_Volume:1\" indicates that neither causes the other and that there exists an unmeasured variable that directly or indirectly causes both.\n",
    "\n",
    "Lastly, \"Sentiment o-o Sentiment:1\" indicates some causal relationship between the two variables, or that there is an unmeasured variable that causes both variables, or a combination of the two. Since we know from the prior knowledge that a given day's sentiment cannot cause the previous day's sentiment, we can rule out that possibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c4cf2d",
   "metadata": {},
   "source": [
    "## Simulation and Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "998a09f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T22:39:38.509827Z",
     "start_time": "2025-12-05T22:39:06.391934Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hereError computing BIC: Graph must not be null.\n",
      "\n",
      "Graph Nodes:\n",
      "Stock_Volume;Sentiment;Tweet_Volume;Daily_Stock_Close_Delta;Daily_Stock_Low_Delta;Daily_Stock_High_Delta;EMA_Delta;Stock_Volume:1;Sentiment:1;Tweet_Volume:1;Daily_Stock_Close_Delta:1;Daily_Stock_Low_Delta:1;Daily_Stock_High_Delta:1;EMA_Delta:1\n",
      "\n",
      "Graph Edges:\n",
      "1. Daily_Stock_Close_Delta --> Daily_Stock_High_Delta:1\n",
      "2. Daily_Stock_Close_Delta:1 --> Daily_Stock_High_Delta:1\n",
      "3. Daily_Stock_Close_Delta:1 --> Daily_Stock_Low_Delta:1\n",
      "4. Daily_Stock_Close_Delta:1 --> EMA_Delta\n",
      "5. Daily_Stock_High_Delta --> Daily_Stock_Close_Delta\n",
      "6. Daily_Stock_High_Delta --> Daily_Stock_Low_Delta\n",
      "7. Daily_Stock_High_Delta --> Daily_Stock_Low_Delta:1\n",
      "8. Daily_Stock_High_Delta --> EMA_Delta\n",
      "9. Daily_Stock_High_Delta --> Sentiment:1\n",
      "10. Daily_Stock_High_Delta --> Stock_Volume\n",
      "11. Daily_Stock_High_Delta --> Tweet_Volume:1\n",
      "12. Daily_Stock_Low_Delta --> Daily_Stock_Close_Delta\n",
      "13. Daily_Stock_Low_Delta --> Sentiment\n",
      "14. Daily_Stock_Low_Delta --> Daily_Stock_Low_Delta:1\n",
      "15. Daily_Stock_Low_Delta:1 --> Daily_Stock_Close_Delta\n",
      "16. Daily_Stock_Low_Delta:1 --> Daily_Stock_High_Delta:1\n",
      "17. EMA_Delta --> Daily_Stock_Low_Delta:1\n",
      "18. EMA_Delta:1 --> Daily_Stock_High_Delta:1\n",
      "19. EMA_Delta:1 --> Daily_Stock_Low_Delta\n",
      "20. EMA_Delta:1 --> Daily_Stock_Low_Delta:1\n",
      "21. EMA_Delta:1 --> EMA_Delta\n",
      "22. Sentiment:1 --> Stock_Volume\n",
      "23. Sentiment:1 --- Tweet_Volume:1\n",
      "24. Stock_Volume --> Daily_Stock_Close_Delta\n",
      "25. Stock_Volume --> Daily_Stock_Low_Delta\n",
      "26. Stock_Volume --> Daily_Stock_Low_Delta:1\n",
      "27. Stock_Volume --> EMA_Delta\n",
      "28. Stock_Volume --> Sentiment\n",
      "29. Stock_Volume --> EMA_Delta:1\n",
      "30. Stock_Volume --> Stock_Volume:1\n",
      "31. Stock_Volume:1 --> Daily_Stock_Close_Delta\n",
      "32. Stock_Volume:1 --> Daily_Stock_High_Delta:1\n",
      "33. Stock_Volume:1 --> Daily_Stock_Low_Delta\n",
      "34. Stock_Volume:1 --> Sentiment\n",
      "35. Stock_Volume:1 --> Daily_Stock_Close_Delta:1\n",
      "36. Stock_Volume:1 --> Daily_Stock_Low_Delta:1\n",
      "37. Tweet_Volume --> Daily_Stock_Low_Delta\n",
      "38. Tweet_Volume --> Sentiment\n",
      "39. Tweet_Volume --> Daily_Stock_Low_Delta:1\n",
      "40. Tweet_Volume --> EMA_Delta:1\n",
      "41. Tweet_Volume --> Sentiment:1\n",
      "42. Tweet_Volume --> Stock_Volume:1\n",
      "43. Tweet_Volume --> Tweet_Volume:1\n",
      "44. Tweet_Volume:1 --> Daily_Stock_Close_Delta\n",
      "45. Tweet_Volume:1 --> Daily_Stock_Low_Delta\n",
      "46. Tweet_Volume:1 --> EMA_Delta\n",
      "47. Tweet_Volume:1 --> Sentiment\n",
      "48. Tweet_Volume:1 --> EMA_Delta:1\n",
      "49. Tweet_Volume:1 --> Stock_Volume:1\n",
      "\n",
      "Graph Attributes:\n",
      "Score: -215392.211938\n",
      "\n",
      "Graph Node Attributes:\n",
      "Score: [Stock_Volume: -17973.769561735295;Sentiment: -25781.000667396722;Tweet_Volume: -20268.449638531067;Daily_Stock_Close_Delta: -8218.228247089772;Daily_Stock_Low_Delta: -20028.0107884915;Daily_Stock_High_Delta: -29539.047683882174;EMA_Delta: -8680.09203019704;Stock_Volume:1: -14622.629945939476;Sentiment:1: -8418.186111879731;Tweet_Volume:1: 7228.8036905809595;Daily_Stock_Close_Delta:1: -27203.021770599316;Daily_Stock_Low_Delta:1: -20815.99177316262;Daily_Stock_High_Delta:1: -15391.516029799997;EMA_Delta:1: -5681.07138036194]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pytetrad.tools.cpn as cpn\n",
    "import torch.nn as nn\n",
    "\n",
    "# Setup a CausalPerceptronNetwork to simulate data from the PAG\n",
    "noise_distributions = {}\n",
    "for node in fges_search.java.getNodes():\n",
    "    noise_distributions[node] = cpn.NoiseDistribution(distribution_type=\"normal\", mean=0, std=1)\n",
    "\n",
    "cpn = cpn.CausalPerceptronNetwork(\n",
    "    graph=fges_search.java,\n",
    "    num_samples=10000,\n",
    "    noise_distributions=noise_distributions,  # Function to generate noise\n",
    "    hidden_dimensions=[50, 50, 50, 50, 50],\n",
    "    input_scale=1,\n",
    "    activation_module=nn.LeakyReLU(),\n",
    "    nonlinearity='leaky_relu',\n",
    "    discrete_prob=0,  # No discrete variables\n",
    "    seed=None # Random to show how sensitive the model is on repeated runs\n",
    ")\n",
    "\n",
    "# Simulate the data from the CPN that was created from the PAG\n",
    "simulated_df = cpn.generate_data()\n",
    "\n",
    "# Reconstruct the CPDAG from this simulated Data\n",
    "search = ts.TetradSearch(simulated_df)\n",
    "search.set_knowledge(knowledge)\n",
    "search.use_sem_bic()\n",
    "search.use_fisher_z(alpha=0.05)\n",
    "search.set_time_lag(1)\n",
    "\n",
    "# Run the FGES algorithm to try to reproduce the CPDAG\n",
    "graph = search.run_fges()\n",
    "print(search.get_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb22832",
   "metadata": {},
   "source": [
    "Comparing this graph's score to the original graph's score, we can note that this reconstructed graph is a weaker representation. There are 49 edges remaining in this graph, compared to the 30 in the original graph, because the search algorithm was less able to determine independence between variables to eliminate edges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901d584a",
   "metadata": {},
   "source": [
    "## An Alternative Algorithm: IMaGES\n",
    "The IMaGES algorithm is effective at building a combined CPDAG from multiple separate but similarly structured datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e222d541",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T22:39:55.408406Z",
     "start_time": "2025-12-05T22:39:55.331419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Nodes:\n",
      "Stock_Volume;Sentiment;Tweet_Volume;Daily_Stock_Close_Delta;Daily_Stock_Low_Delta;Daily_Stock_High_Delta;EMA_Delta;Stock_Volume:1;Sentiment:1;Tweet_Volume:1;Daily_Stock_Close_Delta:1;Daily_Stock_Low_Delta:1;Daily_Stock_High_Delta:1;EMA_Delta:1\n",
      "\n",
      "Graph Edges:\n",
      "1. Daily_Stock_Close_Delta:1 --- Daily_Stock_Low_Delta:1\n",
      "2. Daily_Stock_High_Delta --> Daily_Stock_Close_Delta\n",
      "3. Daily_Stock_High_Delta:1 --- Daily_Stock_Close_Delta:1\n",
      "4. EMA_Delta --> Sentiment\n",
      "5. EMA_Delta --- Sentiment:1\n",
      "6. EMA_Delta:1 --- EMA_Delta\n",
      "7. Stock_Volume --- Daily_Stock_High_Delta\n",
      "8. Stock_Volume --- Daily_Stock_Low_Delta\n",
      "9. Stock_Volume --- Tweet_Volume\n",
      "10. Stock_Volume:1 --- Stock_Volume\n",
      "11. Stock_Volume:1 --- Tweet_Volume:1\n",
      "\n",
      "Graph Attributes:\n",
      "Score: 43235.130487\n",
      "\n",
      "Graph Node Attributes:\n",
      "Score: [Stock_Volume: 1806.561269494074;Sentiment: -944.0424844068725;Tweet_Volume: -8139.990429853084;Daily_Stock_Close_Delta: 7273.004519313469;Daily_Stock_Low_Delta: 7354.210952229297;Daily_Stock_High_Delta: 7398.16768772352;EMA_Delta: 6070.902973406199;Stock_Volume:1: 2375.127623413962;Sentiment:1: -935.6743572618241;Tweet_Volume:1: -8139.672791950432;Daily_Stock_Close_Delta:1: 6367.797869918945;Daily_Stock_Low_Delta:1: 7980.599311392103;Daily_Stock_High_Delta:1: 8166.00897238;EMA_Delta:1: 6602.129371299577]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the Images algorithm \n",
    "alg = multi.Images()\n",
    "# Set knowledge\n",
    "alg.setKnowledge(knowledge)\n",
    "# Set parameters. Images uses SEM BIC for scoring by default\n",
    "params = util.Parameters()\n",
    "params.set(util.Params.PENALTY_DISCOUNT, 2)\n",
    "params.set(util.Params.TIME_LAG, 1)\n",
    "# Add data to a list, keeping separated by ticker symbol\n",
    "data_list = jutil.ArrayList()\n",
    "for symbol in symbols:\n",
    "    data_list.add(ptt.pandas_data_to_tetrad(dfs[symbol]))\n",
    "\n",
    "# Run and print resultss\n",
    "cpdag = alg.search(data_list, params)\n",
    "ptt.print_java(cpdag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a7e721",
   "metadata": {},
   "source": [
    "Comparing this result to the original graph, we can see that it has a slightly higher score but far fewer oriented edges. This indicates that the search algorithm was less able to orient the remaining edges but that it was able to determine more independence between variables, thus eliminating edges. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
