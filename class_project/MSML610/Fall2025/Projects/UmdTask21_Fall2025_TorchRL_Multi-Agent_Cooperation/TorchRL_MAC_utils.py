"""
MAC (Multi-Agent Communication) Utilities for PettingZoo MPE simple_reference with TorchRL.

Actor and Critic Neural Net Model Setup

"""

from dataclasses import dataclass, asdict
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Categorical, Normal
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
import json
from pathlib import Path
import matplotlib.pyplot as plt

# ============================================================================
# Configuration Setup
# ============================================================================

@dataclass
class MacConfig:
    """Configuration for Multi Agent training and evaluation."""
    
    # Environment
    env_name: str = "simple_reference"
    num_envs: int = 4
    max_cycles: int = 25
    continuous_actions: bool = False
    
    # Network architecture
    hidden_dim: int = 128
    actor_layers: int = 2
    critic_layers: int = 2
    
    # Training
    num_iters: int = 1000
    rollout_len: int = 16
    lr: float = 3e-4
    gamma: float = 0.99
    entropy_coef: float = 0.01
    value_coef: float = 0.5
    max_grad_norm: float = 0.5
    
    # Evaluation
    eval_episodes: int = 100
    success_threshold: float = -150.0  # Reward threshold for success
    
    # System
    seed: int = 42
    device: str = "cpu"
    checkpoint_dir: str = "./checkpoints"
    log_interval: int = 10
    
    def to_dict(self) -> dict:
        """Convert config to dictionary."""
        return asdict(self)


def default_cfg() -> MacConfig:
    return MacConfig()


# ============================================================================
# Environment Creation
# ============================================================================

def make_env(cfg: MacConfig):
    """
    Create PettingZoo MPE simple_reference environment wrapped with TorchRL.
    
    Args:
        cfg: MacConfig instance with environment settings.
        
    Returns:
        TorchRL-wrapped PettingZoo environment.
        
    Raises:
        ImportError: If TorchRL PettingZoo wrapper not found.
    """
    # seed for reproducibility
    np.random.seed(cfg.seed)
    torch.manual_seed(cfg.seed)
    
    torchrl_wrapper = None
    try:
        from torchrl.envs.libs.pettingzoo import PettingZooEnv
        torchrl_wrapper = PettingZooEnv
    except ImportError:
        try:
            from torchrl.envs.libs.pettingzoo import PettingZooWrapper
            torchrl_wrapper = PettingZooWrapper
        except ImportError:
            raise ImportError(
                "Could not import TorchRL PettingZoo wrapper. "
                "Expected one of:\n"
                "  - torchrl.envs.libs.pettingzoo.PettingZooEnv\n"
                "  - torchrl.envs.libs.pettingzoo.PettingZooWrapper\n"
                "Please install torchrl with PettingZoo support."
            )
    
    env_version = "v3"
    try:
        from pettingzoo.mpe import simple_reference_v3
    except (ImportError, AttributeError):
        try:
            from pettingzoo.mpe import simple_reference_v2
            env_version = "v2"
        except (ImportError, AttributeError):
            raise ImportError(
                "Could not import simple_reference from pettingzoo.mpe. "
                "Please install pettingzoo: pip install pettingzoo[mpe]"
            )
    
    task_name = f"mpe/simple_reference_{env_version}"
    
    try:
        env = torchrl_wrapper(
            task=task_name,
            parallel=True,
            categorical_actions=not cfg.continuous_actions,
            seed=cfg.seed,
            done_on_any=False,
            max_cycles=cfg.max_cycles,
            continuous_actions=cfg.continuous_actions
        )
    except (TypeError, RuntimeError) as e:
        # Fallback: create environment manually without TorchRL wrapper
        if env_version == "v3":
            from pettingzoo.mpe import simple_reference_v3
            env = simple_reference_v3.parallel_env(
                max_cycles=cfg.max_cycles, 
                continuous_actions=cfg.continuous_actions
            )
        else:
            from pettingzoo.mpe import simple_reference_v2
            env = simple_reference_v2.parallel_env(
                max_cycles=cfg.max_cycles, 
                continuous_actions=cfg.continuous_actions
            )
    
    return env


def get_agent_names(env) -> List[str]:
    """
    Get list of agent names from environment.
    
    Args:
        env: TorchRL-wrapped PettingZoo environment.
        
    Returns:
        List of agent names (e.g., ['speaker_0', 'listener_0']).
    """
    if hasattr(env, '_env'):
        pz_env = env._env
    elif hasattr(env, 'env'):
        pz_env = env.env
    else:
        pz_env = env
    
    # Agents list
    if hasattr(pz_env, 'possible_agents'):
        return pz_env.possible_agents
    elif hasattr(pz_env, 'agents'):
        return pz_env.agents
    else:
        # Fallback for simple_reference
        return ['speaker_0', 'listener_0']


# ============================================================================
# Neural Network Modules
# ============================================================================

def orthogonal_init(layer, gain=1.0):
    if isinstance(layer, nn.Linear):
        nn.init.orthogonal_(layer.weight, gain=gain)
        nn.init.constant_(layer.bias, 0)

class MLPNetwork(nn.Module):
    """
    Enhanced Multi-layer perceptron network.
    Features: GELU activation, Layer Normalization, and Orthogonal Initialization.
    """
    
    def __init__(self, input_dim: int, output_dim: int, hidden_dim: int, num_layers: int):
        super().__init__()
        layers = []
        
        # Input layer
        fc1 = nn.Linear(input_dim, hidden_dim)
        orthogonal_init(fc1)
        layers.append(fc1)
        layers.append(nn.LayerNorm(hidden_dim))
        layers.append(nn.GELU())
        
        # Hidden layers (Deeper and stabilized)
        for _ in range(num_layers - 1):
            fc = nn.Linear(hidden_dim, hidden_dim)
            orthogonal_init(fc)
            layers.append(fc)
            layers.append(nn.LayerNorm(hidden_dim))
            layers.append(nn.GELU())
        
        # Output layer
        fc_out = nn.Linear(hidden_dim, output_dim)
        orthogonal_init(fc_out, gain=0.01) 
        layers.append(fc_out)
        
        self.network = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.network(x)


class DiscreteActor(nn.Module):
    """Actor network for discrete action spaces (Categorical policy)."""
    
    def __init__(self, obs_dim: int, action_dim: int, hidden_dim: int, num_layers: int):
        super().__init__()
        self.mlp = MLPNetwork(obs_dim, action_dim, hidden_dim, num_layers)
    
    def forward(self, obs):
        logits = self.mlp(obs)
        return Categorical(logits=logits)
    
    def evaluate_actions(self, obs, action):
        """Evaluate actions for PPO update."""
        dist = self.forward(obs)
        log_prob = dist.log_prob(action.squeeze(-1)).unsqueeze(-1)
        entropy = dist.entropy().unsqueeze(-1)
        return log_prob, entropy

    def get_null_action(self, batch_size: int, device: str):
        return torch.zeros(batch_size, dtype=torch.long, device=device)


class ContinuousActor(nn.Module):
    """Actor network for continuous action spaces (Gaussian policy)."""
    
    def __init__(self, obs_dim: int, action_dim: int, hidden_dim: int, num_layers: int, action_bounds: Tuple[float, float]):
        super().__init__()
        self.mlp = MLPNetwork(obs_dim, 2 * action_dim, hidden_dim, num_layers)
        self.action_dim = action_dim
        self.action_low = action_bounds[0]
        self.action_high = action_bounds[1]
        self.action_scale = (self.action_high - self.action_low) / 2.0
        self.action_bias = (self.action_high + self.action_low) / 2.0
    
    def forward(self, obs):
        output = self.mlp(obs)
        mean, log_std = output.chunk(2, dim=-1)
        log_std = torch.clamp(log_std, -20, 2)
        std = log_std.exp()
        return Normal(mean, std)
    
    def get_action(self, obs, deterministic: bool = False):
        dist = self.forward(obs)
        if deterministic:
            action_raw = dist.mean
        else:
            action_raw = dist.rsample()
        
        action = torch.tanh(action_raw)
        
        log_prob = dist.log_prob(action_raw)
        log_prob = log_prob - torch.log(1 - action.pow(2) + 1e-6)
        log_prob = log_prob.sum(dim=-1, keepdim=True)
        
        action_scaled = action * self.action_scale + self.action_bias
        entropy = dist.entropy().sum(dim=-1, keepdim=True)
        
        return action_scaled, log_prob, entropy

    def evaluate_actions(self, obs, action):
        """Evaluate actions for PPO update (reverse the scaling)."""
        # Unscale action to [-1, 1] range for tanh
        action_norm = (action - self.action_bias) / (self.action_scale + 1e-8)
        action_norm = torch.clamp(action_norm, -0.999, 0.999)
        action_raw = torch.atanh(action_norm)

        dist = self.forward(obs)
        log_prob = dist.log_prob(action_raw)
        log_prob = log_prob - torch.log(1 - action_norm.pow(2) + 1e-6)
        log_prob = log_prob.sum(dim=-1, keepdim=True)
        entropy = dist.entropy().sum(dim=-1, keepdim=True)
        
        return log_prob, entropy
    
    def get_null_action(self, batch_size: int, device: str):
        return torch.zeros(batch_size, self.action_dim, device=device)


class CentralizedCritic(nn.Module):
    def __init__(self, state_dim: int, hidden_dim: int, num_layers: int):
        super().__init__()
        self.mlp = MLPNetwork(state_dim, 1, hidden_dim, num_layers)
    
    def forward(self, state):
        return self.mlp(state)

def build_modules(cfg, env) -> Tuple[Dict[str, nn.Module], nn.Module, dict]:
    """
    Build actor networks and centralized critic using the enhanced MLP classes.
    """
    # Get agent names
    if hasattr(env, 'agents'):
        agent_names = env.agents
    else:
        # Fallback for some wrappers
        agent_names = env.unwrapped.agents

    # Access underlying PettingZoo environment to get spaces
    if hasattr(env, '_env'):
        pz_env = env._env
    elif hasattr(env, 'env'):
        pz_env = env.env
    else:
        pz_env = env
    
    specs = {
        'agent_names': agent_names,
        'obs_dims': {},
        'action_dims': {},
        'action_types': {},
        'action_bounds': {},
    }
    
    total_obs_dim = 0
    for agent in agent_names:
        obs_space = pz_env.observation_space(agent)
        action_space = pz_env.action_space(agent)
        
        # Obs dim
        if hasattr(obs_space, 'shape') and len(obs_space.shape) > 0:
            obs_dim = int(np.prod(obs_space.shape))
        else:
            obs_dim = int(obs_space.n)
        specs['obs_dims'][agent] = obs_dim
        total_obs_dim += obs_dim
        
        # Action dim
        if hasattr(action_space, 'n'):
            action_dim = int(action_space.n)
            specs['action_types'][agent] = 'discrete'
            specs['action_bounds'][agent] = None
        else:
            action_dim = int(np.prod(action_space.shape))
            specs['action_types'][agent] = 'continuous'
            specs['action_bounds'][agent] = (float(action_space.low[0]), float(action_space.high[0]))
        specs['action_dims'][agent] = action_dim
    
    actors = {}
    for agent in agent_names:
        obs_dim = specs['obs_dims'][agent]
        action_dim = specs['action_dims'][agent]
        action_type = specs['action_types'][agent]
        
        if action_type == 'discrete':
            actor = DiscreteActor(
                obs_dim=obs_dim,
                action_dim=action_dim,
                hidden_dim=cfg.hidden_dim,
                num_layers=cfg.actor_layers
            )
        else:
            action_bounds = specs['action_bounds'][agent]
            actor = ContinuousActor(
                obs_dim=obs_dim,
                action_dim=action_dim,
                hidden_dim=cfg.hidden_dim,
                num_layers=cfg.actor_layers,
                action_bounds=action_bounds
            )
        
        actors[agent] = actor.to(cfg.device)
    
    critic = CentralizedCritic(
        state_dim=total_obs_dim,
        hidden_dim=cfg.hidden_dim,
        num_layers=cfg.critic_layers
    ).to(cfg.device)
    
    return actors, critic, specs

# ============================================================================
# Rollout Collection
# ============================================================================
def collect_rollout(cfg: MacConfig, env, actors: Dict[str, nn.Module], critic: Optional[nn.Module] = None, device: Optional[str] = None) -> dict:
    """
    Collect rollout from environment using current policies.
    
    Args:
        cfg: MacConfig instance.
        env: TorchRL-wrapped environment.
        actors: Dict of actor networks per agent.
        critic: Centralized critic (optional).
        device: Device to store tensors on.
        
    Returns:
        batch: Dictionary containing:
            - obs[agent][t]: observations
            - actions[agent][t]: actions
            - logp[agent][t]: log probabilities
            - entropy[agent][t]: entropies
            - reward[t]: team rewards
            - done[t]: done flags
            - state[t]: global states
            - values[t]: state values (if critic provided)
    """
    if device is None:
        device = cfg.device
    
    agent_names = get_agent_names(env)
    
    # Access underlying PettingZoo environment
    if hasattr(env, '_env'):
        pz_env = env._env
    elif hasattr(env, 'env'):
        pz_env = env.env
    else:
        pz_env = env
    
    # Initialize storage
    batch = {
        'obs': {agent: [] for agent in agent_names},
        'actions': {agent: [] for agent in agent_names},
        'logp': {agent: [] for agent in agent_names},
        'entropy': {agent: [] for agent in agent_names},
        'reward': [],
        'done': [],
        'state': [],
    }
    if critic is not None:
        batch['values'] = []
    
    # Collect rollouts from multiple environments in parallel (simple loop for academic implementation)
    for env_idx in range(cfg.num_envs):
        # Reset environment
        reset_result = pz_env.reset(seed=cfg.seed + env_idx)
        
        # Handle TorchRL TensorDict or plain dict
        if isinstance(reset_result, tuple):
            obs_dict = reset_result[0] if len(reset_result) > 0 else reset_result
        else:
            obs_dict = reset_result
        
        # Collect rollout_len steps
        for step in range(cfg.rollout_len):
            # Convert observations to tensors
            obs_tensors = {}
            for agent in agent_names:
                # Handle TensorDict from TorchRL
                if hasattr(obs_dict, 'get'):
                    obs = obs_dict.get(agent, obs_dict.get(('agents', agent)))
                else:
                    obs = obs_dict[agent] if isinstance(obs_dict, dict) else obs_dict
                
                if not isinstance(obs, torch.Tensor):
                    obs = torch.FloatTensor(obs).flatten()
                else:
                    obs = obs.flatten()
                obs_tensors[agent] = obs.unsqueeze(0).to(device)
            
            # Create global state (concatenate all observations)
            state = torch.cat([obs_tensors[agent] for agent in agent_names], dim=-1)
            
            # Get actions and log probs from actors
            actions_dict = {}
            logp_dict = {}
            entropy_dict = {}
            
            for agent in agent_names:
                actor = actors[agent]
                obs = obs_tensors[agent]
                
                with torch.no_grad():
                    if isinstance(actor, DiscreteActor):
                        dist = actor(obs)
                        action = dist.sample()
                        logp = dist.log_prob(action)
                        entropy = dist.entropy()
                        actions_dict[agent] = action.cpu().numpy()[0]
                    else:  # ContinuousActor
                        action, logp, entropy = actor.get_action(obs)
                        actions_dict[agent] = action.cpu().numpy()[0]
                    
                    logp_dict[agent] = logp
                    entropy_dict[agent] = entropy
            
            # Get value estimate
            if critic is not None:
                with torch.no_grad():
                    value = critic(state)
            
            # Step environment
            step_result = pz_env.step(actions_dict)
            
            # Handle TorchRL TensorDict or plain tuple
            if len(step_result) == 5:
                next_obs_dict, rewards_dict, dones_dict, truncs_dict, infos_dict = step_result
            elif isinstance(step_result, tuple) and len(step_result) == 1:
                # TorchRL returns a single TensorDict
                td = step_result[0]
                next_obs_dict = {agent: td.get(agent, td.get(('agents', agent))) for agent in agent_names}
                rewards_dict = {agent: td.get(('next', 'reward', agent), td.get(('reward', agent), 0)) for agent in agent_names}
                dones_dict = {agent: td.get(('next', 'done', agent), td.get(('done', agent), False)) for agent in agent_names}
                truncs_dict = {agent: False for agent in agent_names}
                infos_dict = {agent: {} for agent in agent_names}
            else:
                next_obs_dict, rewards_dict, dones_dict, truncs_dict, infos_dict = step_result[0], step_result[1], step_result[2], step_result[3], step_result[4]
            
            # Compute team reward (sum of individual rewards)
            team_reward = sum(rewards_dict.values()) if isinstance(rewards_dict, dict) else sum(rewards_dict)
            
            # Check if episode done
            done_vals = dones_dict.values() if isinstance(dones_dict, dict) else dones_dict
            trunc_vals = truncs_dict.values() if isinstance(truncs_dict, dict) else truncs_dict
            done = any(done_vals) or any(trunc_vals)
            
            # Store transition
            for agent in agent_names:
                batch['obs'][agent].append(obs_tensors[agent].cpu())
                if isinstance(actors[agent], DiscreteActor):
                    batch['actions'][agent].append(torch.LongTensor([actions_dict[agent]]))
                else:
                    batch['actions'][agent].append(torch.FloatTensor([actions_dict[agent]]))
                batch['logp'][agent].append(logp_dict[agent].cpu())
                batch['entropy'][agent].append(entropy_dict[agent].cpu())
            
            batch['reward'].append(torch.FloatTensor([team_reward]))
            batch['done'].append(torch.FloatTensor([float(done)]))
            batch['state'].append(state.cpu())
            if critic is not None:
                batch['values'].append(value.cpu())
            
            # Update observation
            obs_dict = next_obs_dict
            
            # Reset if done
            if done:
                reset_result = pz_env.reset(seed=cfg.seed + env_idx + step * 1000)
                if isinstance(reset_result, tuple):
                    obs_dict = reset_result[0] if len(reset_result) > 0 else reset_result
                else:
                    obs_dict = reset_result
    
    # Stack tensors
    for agent in agent_names:
        batch['obs'][agent] = torch.cat(batch['obs'][agent], dim=0)
        batch['actions'][agent] = torch.cat(batch['actions'][agent], dim=0)
        batch['logp'][agent] = torch.cat(batch['logp'][agent], dim=0)
        batch['entropy'][agent] = torch.cat(batch['entropy'][agent], dim=0)
    
    batch['reward'] = torch.cat(batch['reward'], dim=0)
    batch['done'] = torch.cat(batch['done'], dim=0)
    batch['state'] = torch.cat(batch['state'], dim=0)
    if critic is not None:
        batch['values'] = torch.cat(batch['values'], dim=0)
    
    return batch

# ============================================================================
# Loss Computation
# ============================================================================

def compute_loss(cfg, batch: dict, actors: Dict[str, nn.Module], critic: nn.Module) -> Tuple[torch.Tensor, dict]:
    """
    Compute PPO loss with GAE.
    """
    device = cfg.device
    agent_names = list(actors.keys())
    
    # 1. Prepare Data
    # ----------------
    rewards = batch['reward'].to(device) # [T, 1]
    dones = batch['done'].to(device)     # [T, 1]
    states = batch['state'].to(device)   # [T, state_dim]
    old_values = batch['values'].to(device).detach() # [T, 1]
    
    # Flatten batch for PPO updates
    T = rewards.shape[0]


    # Ensure rewards and dones are 1D
    if rewards.dim() > 1:
        rewards = rewards.squeeze()
    if dones.dim() > 1:
        dones = dones.squeeze()

    # 2. Compute GAE (Generalized Advantage Estimation)
    # ----------------
    advantages = torch.zeros_like(rewards)
    lastgaelam = 0
    
    # Bootstrap value
    with torch.no_grad():
        if dones[-1] < 0.5:
            next_value = critic(states[-1:]).squeeze().item()
        else:
            next_value = 0.0
            
    # Calculate GAE backwards
    for t in reversed(range(T)):
        if t == T - 1:
            next_non_terminal = 1.0 - dones[t].item()
            next_val = next_value
        else:
            next_non_terminal = 1.0 - dones[t].item()
            next_val = old_values[t+1].item()
            
        delta = rewards[t].item() + cfg.gamma * next_val * next_non_terminal - old_values[t].item()
        lastgaelam = delta + cfg.gamma * 0.95 * next_non_terminal * lastgaelam # Using hardcoded lambda=0.95
        advantages[t] = lastgaelam
        
    # returns = advantages + old_values
    # Ensure old_values is 1D for proper addition
    old_values_flat = old_values.squeeze() if old_values.dim() > 1 else old_values
    returns = advantages + old_values_flat

    # Normalize advantages
    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

    # 3. PPO Update Loop
    # ------------------
    # Config defaults if not present
    ppo_epochs = getattr(cfg, 'ppo_epochs', 4)
    clip_param = getattr(cfg, 'clip_param', 0.2)
    mini_batch_size = T // 4  # Default to 4 minibatches
    
    total_actor_loss = 0
    total_critic_loss = 0
    total_entropy_loss = 0
    
    # Create indices for shuffling
    indices = np.arange(T)
    
    for _ in range(ppo_epochs):
        np.random.shuffle(indices)
        
        for start in range(0, T, mini_batch_size):
            end = start + mini_batch_size
            mb_inds = indices[start:end]
            
            # Get mini-batch data for shared Critic
            mb_states = states[mb_inds]
            mb_returns = returns[mb_inds]
            mb_advantages = advantages[mb_inds]
            
            # Ensure all tensors are 1D for this mini-batch
            if mb_returns.dim() > 1:
                mb_returns = mb_returns.squeeze()
            if mb_advantages.dim() > 1:
                mb_advantages = mb_advantages.squeeze()

            # --- Critic Update ---
            # new_values = critic(mb_states)
            # critic_loss = F.mse_loss(new_values, mb_returns)
            new_values = critic(mb_states).squeeze()  # Should be [mini_batch_size]
            critic_loss = F.mse_loss(new_values, mb_returns)

            
            # --- Actor Update (Per Agent) ---
            actor_loss_sum = 0
            entropy_sum = 0
            
            for agent in agent_names:
                # Get agent specific mini-batch data
                mb_obs = batch['obs'][agent].to(device)[mb_inds]
                mb_actions = batch['actions'][agent].to(device)[mb_inds]
                mb_old_logp = batch['logp'][agent].to(device)[mb_inds].detach()
                
                actor = actors[agent]
                
                # Evaluate current policy
                new_logp, dist_entropy = actor.evaluate_actions(mb_obs, mb_actions)
                
                # PPO Ratio
                # ratio = torch.exp(new_logp - mb_old_logp)
                ratio = torch.exp(new_logp - mb_old_logp).squeeze(-1)
                # Surrogate Loss
                surr1 = ratio * mb_advantages
                surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * mb_advantages
                
                # Maximize objective => Minimize negative loss
                agent_actor_loss = -torch.min(surr1, surr2).mean()
                
                actor_loss_sum += agent_actor_loss
                entropy_sum += dist_entropy.mean()
            
            # Aggregate losses
            loss = actor_loss_sum + cfg.value_coef * critic_loss - cfg.entropy_coef * entropy_sum
            
            # Track metrics
            total_actor_loss += actor_loss_sum.item()
            total_critic_loss += critic_loss.item()
            total_entropy_loss += entropy_sum.item()
    
    # --- Single Pass Calculation for Gradient Graph ---
    final_critic_loss = F.mse_loss(critic(states), returns)
    final_actor_loss = 0
    final_entropy = 0
    
    for agent in agent_names:
        obs = batch['obs'][agent].to(device)
        actions = batch['actions'][agent].to(device)
        old_logp = batch['logp'][agent].to(device).detach()
        
        new_logp, entropy = actors[agent].evaluate_actions(obs, actions)
        ratio = torch.exp(new_logp - old_logp)
        surr1 = ratio * advantages
        surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantages
        final_actor_loss += -torch.min(surr1, surr2).mean()
        final_entropy += entropy.mean()
        
    final_loss = final_actor_loss + cfg.value_coef * final_critic_loss - cfg.entropy_coef * final_entropy
    
    info = {
        'loss': final_loss.item(),
        'actor_loss': final_actor_loss.item(),
        'critic_loss': final_critic_loss.item(),
        'entropy': final_entropy.item(),
        'mean_return': returns.mean().item(),
        'mean_value': old_values.mean().item(),
    }
    
    return final_loss, info


# ============================================================================
# Training
# ============================================================================

def train(cfg: MacConfig) -> Tuple[str, dict]:
    """
    Train multi-agent system with centralized A3C.
    
    Args:
        cfg: MacConfig instance.
        
    Returns:
        checkpoint_path: Path to saved checkpoint.
        stats: Training statistics.
    """
    print(f"Starting training with config:\n{cfg}")
    
    # Create environment
    env = make_env(cfg)
    
    # Build modules
    actors, critic, specs = build_modules(cfg, env)
    
    # Optimizer
    params = list(critic.parameters())
    for actor in actors.values():
        params.extend(actor.parameters())
    optimizer = torch.optim.Adam(params, lr=cfg.lr)
    
    # Training loop
    stats = {
        'losses': [],
        'returns': [],
        'entropies': [],
    }
    
    for iteration in range(cfg.num_iters):
        # Collect rollout
        batch = collect_rollout(cfg, env, actors, critic, device=cfg.device)
        
        # Compute loss
        loss, info = compute_loss(cfg, batch, actors, critic)
        
        # Optimize
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(params, cfg.max_grad_norm)
        optimizer.step()
        
        # Log
        stats['losses'].append(info['loss'])
        stats['returns'].append(info['mean_return'])
        stats['entropies'].append(info['entropy'])
        
        if (iteration + 1) % cfg.log_interval == 0:
            # positive_return = -info['mean_return']
            print(f"Iter {iteration + 1}/{cfg.num_iters} | "
                  f"Loss: {info['loss']:.3f} | "
                  f"Return: {info['mean_return']:.3f} | "
                  f"Entropy: {info['entropy']:.3f}")
    
    # Save checkpoint
    os.makedirs(cfg.checkpoint_dir, exist_ok=True)
    checkpoint_path = os.path.join(cfg.checkpoint_dir, f"mac_checkpoint_iter{cfg.num_iters}.pt")
    
    checkpoint = {
        'actors_state': {agent: actor.state_dict() for agent, actor in actors.items()},
        'critic_state': critic.state_dict(),
        'cfg': cfg.to_dict(),
        'specs': specs,
        'iteration': cfg.num_iters,
    }
    
    torch.save(checkpoint, checkpoint_path)
    # print(f"\nTraining complete! Checkpoint saved to: {checkpoint_path}")
    # Print final statistics with positive scores
    final_positive_score = -info['mean_return']
    print(f"\nTraining complete! Checkpoint saved to: {checkpoint_path}")
    print(f"\nâœ“ Training complete!")
    print(f"  Checkpoint: {checkpoint_path}")
    print(f"  Final loss: {info['loss']:.3f}")
    print(f"  Final success score: {final_positive_score:.3f}")
    print(f"  Final entropy: {info['entropy']:.3f}")

    return checkpoint_path, stats


# ============================================================================
# Checkpoint Loading
# ============================================================================

def load_checkpoint(path: str, device: Optional[str] = None) -> dict:
    """
    Load checkpoint from disk.
    """
    if device is None:
        device = 'cpu'
    
    checkpoint = torch.load(path, map_location=device)
    
    # Reconstruct config
    cfg = MacConfig(**checkpoint['cfg'])
    cfg.device = device
    
    # Reconstruct environment to get specs
    env = make_env(cfg)
    
    # Rebuild modules
    actors, critic, specs = build_modules(cfg, env)
    
    # Load state dicts
    for agent, actor in actors.items():
        actor.load_state_dict(checkpoint['actors_state'][agent])
        actor.eval()
    
    critic.load_state_dict(checkpoint['critic_state'])
    critic.eval()
    
    return {
        'actors': actors,
        'critic': critic,
        'cfg': cfg,
        'specs': specs,
        'env': env,
        'iteration': checkpoint.get('iteration', 0),
    }


# ============================================================================
# Evaluation
# ============================================================================

def evaluate(cfg: MacConfig, ckpt_path: str, mode: str = "normal") -> dict:
    """
    Evaluate trained policy.
    
    Args:
        cfg: MacConfig instance (used for eval_episodes, etc).
        ckpt_path: Path to checkpoint.
        mode: "normal" for normal eval, "no_comm" to silence speaker.
        
    Returns:
        metrics: Dict with success_rate, comm_cost, comm_gain, comm_efficiency.
    """
    assert mode in ["normal", "no_comm"], f"Invalid mode: {mode}"
    
    print(f"\nEvaluating in mode: {mode}")
    
    # Load checkpoint
    ckpt = load_checkpoint(ckpt_path, device=cfg.device)
    actors = ckpt['actors']
    env = ckpt['env']
    agent_names = ckpt['specs']['agent_names']
    
    speaker_name = None
    for agent in agent_names:
        if 'speaker' in agent.lower():
            speaker_name = agent
            break
    if speaker_name is None:
        speaker_name = agent_names[0]  
    
    # Access underlying PettingZoo environment
    if hasattr(env, '_env'):
        pz_env = env._env
    elif hasattr(env, 'env'):
        pz_env = env.env
    else:
        pz_env = env
    
    # Run evaluation episodes
    successes = []
    comm_costs = []
    
    for ep in range(cfg.eval_episodes):
        reset_result = pz_env.reset(seed=cfg.seed + ep)
        if isinstance(reset_result, tuple):
            obs_dict = reset_result[0] if len(reset_result) > 0 else reset_result
        else:
            obs_dict = reset_result
        
        done = False
        episode_reward = 0
        episode_comm_cost = 0
        step_count = 0
        
        while not done and step_count < cfg.max_cycles:
            # Get actions
            actions_dict = {}
            
            for agent in agent_names:
                # Handle TensorDict from TorchRL
                if hasattr(obs_dict, 'get'):
                    obs = obs_dict.get(agent, obs_dict.get(('agents', agent)))
                else:
                    obs = obs_dict[agent] if isinstance(obs_dict, dict) else obs_dict
                if not isinstance(obs, torch.Tensor):
                    obs = torch.FloatTensor(obs).flatten()
                else:
                    obs = obs.flatten()
                obs_tensor = obs.unsqueeze(0).to(cfg.device)
                
                actor = actors[agent]
                
                with torch.no_grad():
                    if isinstance(actor, DiscreteActor):
                        if mode == "no_comm" and agent == speaker_name:
                            # Force null action (action 0)
                            action = 0
                        else:
                            dist = actor(obs_tensor)
                            action = dist.sample().item()
                        actions_dict[agent] = action
                        
                        # Track comm cost for discrete (nonzero action fraction)
                        if agent == speaker_name:
                            episode_comm_cost += float(action != 0)
                    else:  # ContinuousActor
                        if mode == "no_comm" and agent == speaker_name:
                            # Force null action (zeros)
                            action = actor.get_null_action(1, cfg.device).cpu().numpy()[0]
                        else:
                            action, _, _ = actor.get_action(obs_tensor, deterministic=True)
                            action = action.cpu().numpy()[0]
                        actions_dict[agent] = action
                        
                        # Track comm cost for continuous (L2 norm)
                        if agent == speaker_name:
                            episode_comm_cost += float(np.linalg.norm(action))
            
            # Environment step
            step_result = pz_env.step(actions_dict)
            
            # Handle TorchRL TensorDict
            if len(step_result) == 5:
                next_obs_dict, rewards_dict, dones_dict, truncs_dict, infos_dict = step_result
            elif isinstance(step_result, tuple) and len(step_result) == 1:
                # TorchRL returns a single TensorDict
                td = step_result[0]
                next_obs_dict = {agent: td.get(agent, td.get(('agents', agent))) for agent in agent_names}
                rewards_dict = {agent: td.get(('next', 'reward', agent), td.get(('reward', agent), 0)) for agent in agent_names}
                dones_dict = {agent: td.get(('next', 'done', agent), td.get(('done', agent), False)) for agent in agent_names}
                truncs_dict = {agent: False for agent in agent_names}
                infos_dict = {agent: {} for agent in agent_names}
            else:
                next_obs_dict, rewards_dict, dones_dict, truncs_dict, infos_dict = step_result[0], step_result[1], step_result[2], step_result[3], step_result[4]
            
            episode_reward += sum(rewards_dict.values()) if isinstance(rewards_dict, dict) else sum(rewards_dict)
            done_vals = dones_dict.values() if isinstance(dones_dict, dict) else dones_dict
            trunc_vals = truncs_dict.values() if isinstance(truncs_dict, dict) else truncs_dict
            done = any(done_vals) or any(trunc_vals)
            obs_dict = next_obs_dict
            step_count += 1
        
        # Determine success
        success = False
        for agent_info in infos_dict.values():
            if isinstance(agent_info, dict):
                if 'is_success' in agent_info:
                    success = agent_info['is_success']
                    break
                elif 'success' in agent_info:
                    success = agent_info['success']
                    break
        
        # Fallback: infer from reward threshold
        if not success:
            # success = episode_reward > -cfg.success_threshold
            success = episode_reward >= cfg.success_threshold
        
        successes.append(float(success))
        
        # Average comm cost per step
        avg_comm_cost = episode_comm_cost / max(step_count, 1)
        comm_costs.append(avg_comm_cost)
    
    # Compute metrics
    success_rate = np.mean(successes)
    comm_cost = np.mean(comm_costs)
    # Calculate average episode reward for display
    avg_reward = np.mean([episode_reward]) if 'episode_reward' in locals() else 0
    positive_score = -avg_reward  # Transform to positive

    metrics = {
        'success_rate': success_rate,
        'comm_cost': comm_cost,
        'positive_score': positive_score,
    }
    
    print(f"  Success Rate: {success_rate:.3f}")
    print(f"  Comm Cost: {comm_cost:.4f}")
    print(f"  Positive Score: {positive_score:.3f}")
    
    # If this is normal mode, we can't compute gain/efficiency yet
    if mode == "normal":
        metrics['comm_gain'] = 0.0
        metrics['comm_efficiency'] = 0.0
    else:
        # For no_comm mode, we compute gain/efficiency if we have normal results
        # This is a bit awkward - in practice you'd call evaluate twice and compare
        # For now, set to 0
        metrics['comm_gain'] = 0.0
        metrics['comm_efficiency'] = 0.0
    
    return metrics


def evaluate_with_comparison(cfg: MacConfig, ckpt_path: str) -> dict:
    """
    Evaluate in both normal and no_comm modes and compute full metrics.
    
    Args:
        cfg: MacConfig instance.
        ckpt_path: Path to checkpoint.
        
    Returns:
        metrics: Dict with all metrics including comm_gain and comm_efficiency.
    """
    # Evaluate normal mode
    normal_metrics = evaluate(cfg, ckpt_path, mode="normal")
    
    # Evaluate no_comm mode
    no_comm_metrics = evaluate(cfg, ckpt_path, mode="no_comm")
    
    # Compute gain and efficiency
    success_normal = normal_metrics['success_rate']
    success_no_comm = no_comm_metrics['success_rate']
    comm_cost = normal_metrics['comm_cost']
    
    comm_gain = success_normal - success_no_comm
    comm_efficiency = comm_gain / (comm_cost + 1e-8)
    
    metrics = {
        'success_rate': success_normal,
        'comm_cost': comm_cost,
        'comm_gain': comm_gain,
        'comm_efficiency': comm_efficiency,
        'success_rate_no_comm': success_no_comm,
    }
    
    print(f"\n=== Final Metrics ===")
    print(f"Success Rate (normal): {success_normal:.3f}")
    print(f"Success Rate (no_comm): {success_no_comm:.3f}")
    print(f"Comm Cost: {comm_cost:.4f}")
    print(f"Comm Gain: {comm_gain:.3f}")
    print(f"Comm Efficiency: {comm_efficiency:.3f}")
    
    return metrics


# ============================================================================
# Main (for testing)
# ============================================================================

if __name__ == "__main__":
    # Quick test
    print("Testing MAC utilities...")
    
    cfg = default_cfg()
    cfg.num_iters = 2
    cfg.rollout_len = 8
    cfg.num_envs = 2
    
    print("\n1. Creating environment...")
    env = make_env(cfg)
    print(f"   Agents: {get_agent_names(env)}")
    
    print("\n2. Building modules...")
    actors, critic, specs = build_modules(cfg, env)
    print(f"   Actor networks: {list(actors.keys())}")
    print(f"   Obs dims: {specs['obs_dims']}")
    print(f"   Action dims: {specs['action_dims']}")
    
    print("\n3. Collecting rollout...")
    batch = collect_rollout(cfg, env, actors, critic)
    print(f"   Collected {batch['reward'].shape[0]} transitions")
    
    print("\n4. Computing loss...")
    loss, info = compute_loss(cfg, batch, actors, critic)
    print(f"   Loss: {loss.item():.3f}")
    
    print("\n5. Running short training...")
    ckpt_path, stats = train(cfg)
    
    print("\n6. Loading checkpoint...")
    ckpt = load_checkpoint(ckpt_path)
    print(f"   Loaded checkpoint from iteration {ckpt['iteration']}")
    
    print("\n7. Evaluating...")
    cfg.eval_episodes = 5
    metrics = evaluate_with_comparison(cfg, ckpt_path)
    
    print("\n All tests passed!")

def train_wrapper(cfg):
    checkpoint_path, stats = train(cfg)

    # Plot training curves
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))

    # Loss
    axes[0].plot(stats['losses'], 'b-', linewidth=2)
    axes[0].set_xlabel('Iteration')
    axes[0].set_ylabel('Loss')
    axes[0].set_title('Training Loss')
    axes[0].grid(True, alpha=0.3)

    # Returns
    axes[1].plot(stats['returns'], 'g-', linewidth=2)
    axes[1].set_xlabel('Iteration')
    axes[1].set_ylabel('Mean Return')
    axes[1].set_title('Episode Returns')
    axes[1].grid(True, alpha=0.3)

    # Entropy
    axes[2].plot(stats['entropies'], 'r-', linewidth=2)
    axes[2].set_xlabel('Iteration')
    axes[2].set_ylabel('Mean Entropy')
    axes[2].set_title('Policy Entropy')
    axes[2].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('mac_training_curves.png', dpi=100, bbox_inches='tight')
    plt.show()

    print("Training curves saved to: mac_training_curves.png")