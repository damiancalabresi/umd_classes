{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fade619e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPEFT_Sentiment_Analysis_on_Movie_Reviews.API\\n\\nThis notebook demonstrates:\\n1. The native HuggingFace APIs (tokenizer, model, dataset)\\n2. The PEFT LoRA API (LoraConfig, get_peft_model)\\n3. How our utils wrapper functions expose a simpler high-level API\\n\\nThis notebook *does not* train a model.\\nIt is purely for understanding the API surface.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "PEFT_Sentiment_Analysis - API Documentation Notebook\n",
    "\n",
    "This notebook demonstrates the core APIs and tools used in PEFT (Parameter-Efficient Fine-Tuning):\n",
    "\n",
    "1. HuggingFace Transformers APIs (tokenizer, model, dataset)\n",
    "2. PEFT LoRA API (LoraConfig, get_peft_model)\n",
    "3. How to use these APIs for any sentiment analysis or text classification task\n",
    "\n",
    "This notebook is tool-focused and does NOT contain project-specific implementation.\n",
    "For a complete project example, see PEFT_Sentiment_Analysis.example.ipynb\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f76429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import core HuggingFace and PEFT libraries\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "# These are the main APIs we'll demonstrate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd33bd7",
   "metadata": {},
   "source": [
    "## 1. Tokenization API\n",
    "\n",
    "The tokenizer converts raw text into token IDs that the model can process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb89bfe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 44820, 268, 146, 234, 21992, 203, 3013, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load RoBERTa tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Example: Tokenize a simple text\n",
    "sample_text = \"This movie was excellent and very entertaining!\"\n",
    "\n",
    "# Tokenize with padding and truncation\n",
    "encoded = tokenizer(\n",
    "    sample_text,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=20\n",
    ")\n",
    "\n",
    "print(\"Input IDs:\", encoded[\"input_ids\"][:10], \"...\")\n",
    "print(\"Attention Mask:\", encoded[\"attention_mask\"][:10], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17807c6",
   "metadata": {},
   "source": [
    "## 2. Dataset API\n",
    "\n",
    "HuggingFace's Dataset class provides efficient data handling for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c45cf39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a simple dataset from dictionary\n",
    "dataset = Dataset.from_dict({\n",
    "    \"text\": [\"I loved this movie\", \"This was terrible\", \"Great acting and plot\"],\n",
    "    \"label\": [1, 0, 1]  # 1=positive, 0=negative\n",
    "})\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Features: {dataset.features}\")\n",
    "print(f\"\\nFirst example: {dataset[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5cbb06",
   "metadata": {},
   "source": [
    "## 3. Model API\n",
    "\n",
    "Load a pre-trained RoBERTa model for sequence classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088305ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load RoBERTa model for binary classification\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=2  # Binary classification\n",
    ")\n",
    "\n",
    "print(f\"Model type: {type(model).__name__}\")\n",
    "print(f\"Number of parameters: {model.num_parameters():,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01a2bfc",
   "metadata": {},
   "source": [
    "## 4. PEFT LoRA API\n",
    "\n",
    "LoRA (Low-Rank Adaptation) enables parameter-efficient fine-tuning by adding small trainable adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22de2e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 887,042 || all params: 125,534,212 || trainable%: 0.7066\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRA parameters\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # Sequence classification\n",
    "    r=8,                          # Rank of adaptation matrices\n",
    "    lora_alpha=16,                # Scaling factor\n",
    "    lora_dropout=0.1,             # Dropout for regularization\n",
    "    bias=\"none\",                  # Don't adapt bias terms\n",
    "    target_modules=[\"query\", \"value\"]  # Apply LoRA to Q and V attention matrices\n",
    ")\n",
    "\n",
    "# Wrap the model with LoRA adapters\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Show the dramatic reduction in trainable parameters\n",
    "peft_model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb7893d",
   "metadata": {},
   "source": [
    "## 5. Training API\n",
    "\n",
    "The Trainer API simplifies the training loop with automatic batching, optimization, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b7b488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  label  \n",
       "0  December 31, 2017      1  \n",
       "1  December 31, 2017      1  \n",
       "2  December 30, 2017      1  \n",
       "3  December 29, 2017      1  \n",
       "4  December 25, 2017      1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example TrainingArguments configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01d7c6b",
   "metadata": {},
   "source": [
    "## 6. Tokenizing Datasets\n",
    "\n",
    "Apply the tokenizer to your dataset using the `.map()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091d94b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>donald trump wish american happy new year leav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>house intelligence committee chairman devin nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>friday revealed former milwaukee sheriff david...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>christmas day donald trump announced would bac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>pope francis used annual christmas day message...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...   \n",
       "1  House Intelligence Committee Chairman Devin Nu...   \n",
       "2  On Friday, it was revealed that former Milwauk...   \n",
       "3  On Christmas day, Donald Trump announced that ...   \n",
       "4  Pope Francis used his annual Christmas Day mes...   \n",
       "\n",
       "                                          text_final  \n",
       "0  donald trump wish american happy new year leav...  \n",
       "1  house intelligence committee chairman devin nu...  \n",
       "2  friday revealed former milwaukee sheriff david...  \n",
       "3  christmas day donald trump announced would bac...  \n",
       "4  pope francis used annual christmas day message...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(f\"Tokenized dataset columns: {tokenized_dataset.column_names}\")\n",
    "print(f\"Example tokenized input shape: {len(tokenized_dataset[0]['input_ids'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237dd5d9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the key APIs for PEFT-based sentiment analysis:\n",
    "\n",
    "- **Tokenizer**: Converts text to model inputs\n",
    "- **Dataset**: Efficient data management\n",
    "- **Model**: Pre-trained transformers for classification\n",
    "- **LoRA/PEFT**: Parameter-efficient fine-tuning\n",
    "- **Trainer**: Simplified training workflow\n",
    "\n",
    "For a complete end-to-end project implementation, see `PEFT_Sentiment_Analysis.example.ipynb`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
