{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis: Training & Evaluation\n",
    "\n",
    "This notebook trains a sentiment classifier on Twitter airline tweets.\n",
    "\n",
    "**Workflow:**\n",
    "1. Load and explore data\n",
    "2. Text preprocessing\n",
    "3. Split train/val/test\n",
    "4. Train TF-IDF + LogisticRegression\n",
    "5. Evaluate and analyze\n",
    "6. Save model artifacts\n",
    "\n",
    "**Dataset:** 14,640 airline tweets (negative, neutral, positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T20:20:37.775145Z",
     "start_time": "2025-12-14T20:20:37.492709Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/data')\n",
    "import sentiment_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T20:20:37.884063Z",
     "start_time": "2025-12-14T20:20:37.776069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports OK\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "import importlib\n",
    "import sentiment_utils\n",
    "importlib.reload(sentiment_utils)\n",
    "\n",
    "from sentiment_utils import (\n",
    "    load_data,\n",
    "    preprocess_dataframe,\n",
    "    split_data,\n",
    "    vectorize_and_train,\n",
    "    evaluate_model,\n",
    "    get_feature_importance,\n",
    "    LABEL_MAP,\n",
    ")\n",
    "\n",
    "from joblib import dump\n",
    "\n",
    "DATA_PATH = \"data/Tweets.csv\"\n",
    "VECT_PATH = \"tfidf_vectorizer.joblib\"\n",
    "MODEL_PATH = \"logreg_sentiment_model.joblib\"\n",
    "\n",
    "print(\"✓ Imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T20:20:37.923106Z",
     "start_time": "2025-12-14T20:20:37.884781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 14640 tweets from data/Tweets.csv\n",
      "Shape: (14640, 2)\n",
      "\n",
      "First few rows:\n",
      "  airline_sentiment                                               text\n",
      "0           neutral                @VirginAmerica What @dhepburn said.\n",
      "1          positive  @VirginAmerica plus you've added commercials t...\n",
      "2           neutral  @VirginAmerica I didn't today... Must mean I n...\n",
      "3          negative  @VirginAmerica it's really aggressive to blast...\n",
      "4          negative  @VirginAmerica and it's a really big bad thing...\n"
     ]
    }
   ],
   "source": [
    "df_raw = load_data(DATA_PATH)\n",
    "print(f\"Shape: {df_raw.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA with YData Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T20:20:41.090844Z",
     "start_time": "2025-12-14T20:20:39.541598Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <ins><a href=\"https://ydata.ai/register\">Upgrade to ydata-sdk</a></ins>\n",
       "                <p>\n",
       "                    Improve your data and profiling with ydata-sdk, featuring data quality scoring, redundancy detection, outlier identification, text validation, and synthetic data generation.\n",
       "                </p>\n",
       "            </div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating profiling report...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarize dataset:   0%|                                                                                                                                               | 0/7 [00:00<?, ?it/s, Describe variable: text]\n",
      "Summarize dataset:  29%|██████████████████████████████████████▌                                                                                                | 2/7 [00:00<00:00,  7.63it/s, Describe variable: text]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  9.53it/s]\u001b[A\n",
      "Summarize dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 28.53it/s, Completed]\n",
      "Generate report structure: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.43it/s]\n",
      "Render HTML: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.36it/s]\n",
      "Export report to file: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 461.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Report saved to airline_sentiment_profile.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "print(\"Generating profiling report...\")\n",
    "profile = ProfileReport(\n",
    "    df_raw,\n",
    "    title=\"Airline Sentiment - EDA\",\n",
    "    minimal=False\n",
    ")\n",
    "profile.to_file(\"airline_sentiment_profile.html\")\n",
    "print(\"✓ Report saved to airline_sentiment_profile.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T20:20:45.507886Z",
     "start_time": "2025-12-14T20:20:45.459174Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Imbalance:\n",
      "  Negative: 62.7%\n",
      "  Neutral:  21.2%\n",
      "  Positive: 16.1%\n",
      "  Ratio (neg/pos): 3.9x\n"
     ]
    }
   ],
   "source": [
    "raw_counts = df_raw['airline_sentiment'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "colors = ['#d32f2f', '#f57c00', '#388e3c']\n",
    "raw_counts.plot(kind='bar', color=colors, ax=ax)\n",
    "ax.set_title('Label Distribution', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Sentiment')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
    "\n",
    "for i, v in enumerate(raw_counts):\n",
    "    ax.text(i, v + 100, f'{v}\\n({v/len(df_raw)*100:.1f}%)', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClass Imbalance:\")\n",
    "print(f\"  Negative: {raw_counts['negative']/len(df_raw)*100:.1f}%\")\n",
    "print(f\"  Neutral:  {raw_counts['neutral']/len(df_raw)*100:.1f}%\")\n",
    "print(f\"  Positive: {raw_counts['positive']/len(df_raw)*100:.1f}%\")\n",
    "print(f\"  Ratio (neg/pos): {raw_counts['negative']/raw_counts['positive']:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T20:20:48.078121Z",
     "start_time": "2025-12-14T20:20:47.990818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Preprocessed - Label distribution:\n",
      "  negative  :  9178 ( 62.7%)\n",
      "  neutral   :  3099 ( 21.2%)\n",
      "  positive  :  2363 ( 16.1%)\n",
      "\n",
      "Cleaned shape: (14640, 4)\n",
      "\n",
      "Sample cleaning:\n",
      "\n",
      "[Original]\n",
      "  @VirginAmerica What @dhepburn said....\n",
      "[Cleaned]\n",
      "  what said\n",
      "\n",
      "[Original]\n",
      "  @VirginAmerica plus you've added commercials to the experience... tacky....\n",
      "[Cleaned]\n",
      "  plus you ve added commercials to the experience tacky\n",
      "\n",
      "[Original]\n",
      "  @VirginAmerica I didn't today... Must mean I need to take another trip!...\n",
      "[Cleaned]\n",
      "  i didn t today must mean i need to take another trip\n"
     ]
    }
   ],
   "source": [
    "df_clean = preprocess_dataframe(df_raw)\n",
    "\n",
    "print(f\"\\nCleaned shape: {df_clean.shape}\")\n",
    "print(f\"\\nSample cleaning:\")\n",
    "for idx in range(3):\n",
    "    print(f\"\\n[Original]\")\n",
    "    print(f\"  {df_clean.iloc[idx]['text'][:100]}...\")\n",
    "    print(f\"[Cleaned]\")\n",
    "    print(f\"  {df_clean.iloc[idx]['clean_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T20:20:52.632905Z",
     "start_time": "2025-12-14T20:20:52.526787Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats:\n",
      "  Avg length: 85 chars\n",
      "  Avg words: 16.5\n"
     ]
    }
   ],
   "source": [
    "df_clean['text_len'] = df_clean['clean_text'].str.len()\n",
    "df_clean['word_count'] = df_clean['clean_text'].str.split().str.len()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "df_clean['text_len'].hist(bins=50, ax=axes[0], color='skyblue', edgecolor='black')\n",
    "axes[0].set_title('Character Length', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Length')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "df_clean['word_count'].hist(bins=50, ax=axes[1], color='lightcoral', edgecolor='black')\n",
    "axes[1].set_title('Word Count', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Words')\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nStats:\")\n",
    "print(f\"  Avg length: {df_clean['text_len'].mean():.0f} chars\")\n",
    "print(f\"  Avg words: {df_clean['word_count'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T20:20:54.800332Z",
     "start_time": "2025-12-14T20:20:54.777773Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Data split:\n",
      "  Train: 11712 ( 80.0%)\n",
      "  Val:     366 (  2.5%)\n",
      "  Test:   2562 ( 17.5%)\n",
      "\n",
      "Class distribution in splits:\n",
      "\n",
      "Train:\n",
      "  negative  :  7343 ( 62.7%)\n",
      "  neutral   :  2479 ( 21.2%)\n",
      "  positive  :  1890 ( 16.1%)\n",
      "\n",
      "Val:\n",
      "  negative  :   229 ( 62.6%)\n",
      "  neutral   :    78 ( 21.3%)\n",
      "  positive  :    59 ( 16.1%)\n",
      "\n",
      "Test:\n",
      "  negative  :  1606 ( 62.7%)\n",
      "  neutral   :   542 ( 21.2%)\n",
      "  positive  :   414 ( 16.2%)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(df_clean)\n",
    "\n",
    "print(f\"\\nClass distribution in splits:\")\n",
    "for split_name, y_split in [('Train', y_train), ('Val', y_val), ('Test', y_test)]:\n",
    "    print(f\"\\n{split_name}:\")\n",
    "    for label_id in range(3):\n",
    "        count = (y_split == label_id).sum()\n",
    "        pct = count / len(y_split) * 100\n",
    "        print(f\"  {LABEL_MAP[label_id]:10s}: {count:5d} ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T20:20:58.757919Z",
     "start_time": "2025-12-14T20:20:57.940595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Vectorizing...\n",
      "  Vocab: 7576 | Shape: (11712, 7576)\n",
      "✓ Training model...\n",
      "  Done\n"
     ]
    }
   ],
   "source": [
    "vectorizer, model = vectorize_and_train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T20:21:00.834235Z",
     "start_time": "2025-12-14T20:21:00.802740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EVALUATION: Validation\n",
      "==================================================\n",
      "Accuracy:  0.7951\n",
      "Precision: 0.7533\n",
      "Recall:    0.7730\n",
      "F1-Score:  0.7583\n",
      "\n",
      "Confusion Matrix:\n",
      "[[188  32   9]\n",
      " [ 14  60   4]\n",
      " [  6  10  43]]\n",
      "\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.82      0.86       229\n",
      "     neutral       0.59      0.77      0.67        78\n",
      "    positive       0.77      0.73      0.75        59\n",
      "\n",
      "    accuracy                           0.80       366\n",
      "   macro avg       0.75      0.77      0.76       366\n",
      "weighted avg       0.81      0.80      0.80       366\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_metrics = evaluate_model(vectorizer, model, X_val, y_val, \"Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T20:21:03.088345Z",
     "start_time": "2025-12-14T20:21:03.048130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EVALUATION: Test\n",
      "==================================================\n",
      "Accuracy:  0.7760\n",
      "Precision: 0.7249\n",
      "Recall:    0.7444\n",
      "F1-Score:  0.7312\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1311  235   60]\n",
      " [ 107  383   52]\n",
      " [  54   66  294]]\n",
      "\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.82      0.85      1606\n",
      "     neutral       0.56      0.71      0.62       542\n",
      "    positive       0.72      0.71      0.72       414\n",
      "\n",
      "    accuracy                           0.78      2562\n",
      "   macro avg       0.72      0.74      0.73      2562\n",
      "weighted avg       0.79      0.78      0.78      2562\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_metrics = evaluate_model(vectorizer, model, X_test, y_test, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T20:21:05.592114Z",
     "start_time": "2025-12-14T20:21:05.442850Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "cm = test_metrics['confusion_matrix']\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=[LABEL_MAP[i] for i in range(3)],\n",
    "            yticklabels=[LABEL_MAP[i] for i in range(3)])\n",
    "axes[0].set_title('Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('True')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='Greens', ax=axes[1],\n",
    "            xticklabels=[LABEL_MAP[i] for i in range(3)],\n",
    "            yticklabels=[LABEL_MAP[i] for i in range(3)])\n",
    "axes[1].set_title('Normalized', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('True')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T20:21:07.860186Z",
     "start_time": "2025-12-14T20:21:07.807380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Misclassifications: 574/2562 (22.4%)\n",
      "\n",
      "Breakdown by true label:\n",
      "\n",
      "  When true=negative:\n",
      "    Predicted neutral: 235x\n",
      "    Predicted positive: 60x\n",
      "\n",
      "  When true=neutral:\n",
      "    Predicted negative: 107x\n",
      "    Predicted positive: 52x\n",
      "\n",
      "  When true=positive:\n",
      "    Predicted negative: 54x\n",
      "    Predicted neutral: 66x\n",
      "\n",
      "Example errors:\n",
      "\n",
      "1. Text: when you ve got to get there we ve got excuses usairwaysfail...\n",
      "   True: negative, Pred: neutral\n",
      "\n",
      "2. Text: i managed to get sorted out over the phone good luck dealing with the snow in te...\n",
      "   True: positive, Pred: neutral\n",
      "\n",
      "3. Text: call gate d in clt and get me on this flight...\n",
      "   True: neutral, Pred: negative\n"
     ]
    }
   ],
   "source": [
    "X_test_vec = vectorizer.transform(X_test)\n",
    "y_pred = model.predict(X_test_vec)\n",
    "\n",
    "errors = y_test != y_pred\n",
    "error_count = errors.sum()\n",
    "total = len(y_test)\n",
    "\n",
    "print(f\"\\nMisclassifications: {error_count}/{total} ({error_count/total*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nBreakdown by true label:\")\n",
    "for true_id in range(3):\n",
    "    true_label = LABEL_MAP[true_id]\n",
    "    mask = (y_test == true_id) & errors\n",
    "    if mask.sum() > 0:\n",
    "        print(f\"\\n  When true={true_label}:\")\n",
    "        for pred_id in range(3):\n",
    "            pred_label = LABEL_MAP[pred_id]\n",
    "            count = ((y_test == true_id) & (y_pred == pred_id) & errors).sum()\n",
    "            if count > 0:\n",
    "                print(f\"    Predicted {pred_label}: {count}x\")\n",
    "\n",
    "# Show a few examples\n",
    "error_idx = np.where(errors)[0]\n",
    "if len(error_idx) > 0:\n",
    "    print(f\"\\nExample errors:\")\n",
    "    for i, idx in enumerate(error_idx[:3]):\n",
    "        true_label = LABEL_MAP[int(y_test.iloc[idx])]\n",
    "        pred_label = LABEL_MAP[int(y_pred[idx])]\n",
    "        text = X_test.iloc[idx]\n",
    "        \n",
    "        print(f\"\\n{i+1}. Text: {text[:80]}...\")\n",
    "        print(f\"   True: {true_label}, Pred: {pred_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T20:21:10.269166Z",
     "start_time": "2025-12-14T20:21:10.247998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top words per class:\n",
      "\n",
      "NEGATIVE:\n",
      "   1. not             ( 3.833)\n",
      "   2. no              ( 3.441)\n",
      "   3. delayed         ( 3.378)\n",
      "   4. hours           ( 3.277)\n",
      "   5. worst           ( 2.878)\n",
      "   6. your            ( 2.631)\n",
      "   7. hour            ( 2.614)\n",
      "   8. cancelled       ( 2.580)\n",
      "   9. delay           ( 2.501)\n",
      "  10. nothing         ( 2.424)\n",
      "\n",
      "NEUTRAL:\n",
      "   1. is there        ( 2.167)\n",
      "   2. can you         ( 2.141)\n",
      "   3. hi              ( 2.123)\n",
      "   4. is your         ( 1.677)\n",
      "   5. do you          ( 1.653)\n",
      "   6. destinationdragons ( 1.585)\n",
      "   7. do              ( 1.522)\n",
      "   8. need            ( 1.520)\n",
      "   9. any             ( 1.514)\n",
      "  10. can             ( 1.507)\n",
      "\n",
      "POSITIVE:\n",
      "   1. great           ( 5.634)\n",
      "   2. thanks          ( 4.890)\n",
      "   3. awesome         ( 4.078)\n",
      "   4. love            ( 3.797)\n",
      "   5. amazing         ( 3.360)\n",
      "   6. thank           ( 3.180)\n",
      "   7. thx             ( 2.918)\n",
      "   8. thank you       ( 2.744)\n",
      "   9. best            ( 2.696)\n",
      "  10. the best        ( 2.659)\n"
     ]
    }
   ],
   "source": [
    "importance = get_feature_importance(vectorizer, model, top_n=15)\n",
    "\n",
    "print(\"\\nTop words per class:\")\n",
    "for label, features in importance.items():\n",
    "    print(f\"\\n{label.upper()}:\")\n",
    "    for i, (word, coef) in enumerate(list(features.items())[:10], 1):\n",
    "        print(f\"  {i:2d}. {word:15s} ({coef:6.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T20:21:12.586286Z",
     "start_time": "2025-12-14T20:21:12.494141Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "colors_map = {'negative': '#d32f2f', 'neutral': '#f57c00', 'positive': '#388e3c'}\n",
    "\n",
    "for idx, (label, features) in enumerate(importance.items()):\n",
    "    words = list(features.keys())[:10]\n",
    "    coefs = list(features.values())[:10]\n",
    "    \n",
    "    axes[idx].barh(words, coefs, color=colors_map[label])\n",
    "    axes[idx].set_title(f'{label.upper()}', fontweight='bold')\n",
    "    axes[idx].set_xlabel('Coefficient')\n",
    "    axes[idx].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T20:21:15.101166Z",
     "start_time": "2025-12-14T20:21:14.939710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Vectorizer -> tfidf_vectorizer.joblib\n",
      "✓ Model -> logreg_sentiment_model.joblib\n",
      "\n",
      "✓ Ready for inference!\n"
     ]
    }
   ],
   "source": [
    "dump(vectorizer, VECT_PATH)\n",
    "dump(model, MODEL_PATH)\n",
    "\n",
    "print(f\"✓ Vectorizer -> {VECT_PATH}\")\n",
    "print(f\"✓ Model -> {MODEL_PATH}\")\n",
    "print(f\"\\n✓ Ready for inference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T20:21:18.366197Z",
     "start_time": "2025-12-14T20:21:18.360420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "╔════════════════════════════════════════╗\n",
      "║  TRAINING COMPLETE                     ║\n",
      "╚════════════════════════════════════════╝\n",
      "\n",
      "Dataset: 14,640 airline tweets\n",
      "Classes: negative (63%), neutral (21%), positive (16%)\n",
      "\n",
      "Model: Logistic Regression + TF-IDF\n",
      "- Vocab size: 20,000\n",
      "- N-grams: unigrams + bigrams\n",
      "- Class weights: balanced\n",
      "\n",
      "Results (Test Set):\n",
      "- Accuracy:  0.776\n",
      "- Precision: 0.725\n",
      "- Recall:    0.744\n",
      "- F1-Score:  0.731\n",
      "\n",
      "Artifacts saved:\n",
      "✓ tfidf_vectorizer.joblib\n",
      "✓ logreg_sentiment_model.joblib\n",
      "✓ airline_sentiment_profile.html\n",
      "\n",
      "Next: Open sentiment.API.ipynb for inference\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "╔════════════════════════════════════════╗\n",
    "║  TRAINING COMPLETE                     ║\n",
    "╚════════════════════════════════════════╝\n",
    "\n",
    "Dataset: 14,640 airline tweets\n",
    "Classes: negative (63%), neutral (21%), positive (16%)\n",
    "\n",
    "Model: Logistic Regression + TF-IDF\n",
    "- Vocab size: 20,000\n",
    "- N-grams: unigrams + bigrams\n",
    "- Class weights: balanced\n",
    "\n",
    "Results (Test Set):\n",
    "- Accuracy:  {test_metrics['accuracy']:.3f}\n",
    "- Precision: {test_metrics['precision_macro']:.3f}\n",
    "- Recall:    {test_metrics['recall_macro']:.3f}\n",
    "- F1-Score:  {test_metrics['f1_macro']:.3f}\n",
    "\n",
    "Artifacts saved:\n",
    "✓ {VECT_PATH}\n",
    "✓ {MODEL_PATH}\n",
    "✓ airline_sentiment_profile.html\n",
    "\n",
    "Next: Open sentiment.API.ipynb for inference\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
