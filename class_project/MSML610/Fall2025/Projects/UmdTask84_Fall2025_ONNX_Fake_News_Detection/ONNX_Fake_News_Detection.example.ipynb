{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "427b4e85",
   "metadata": {},
   "source": [
    "# ONNX Fake News Detection — End-to-End Example\n",
    "\n",
    "## Project Goal\n",
    "\n",
    "This notebook demonstrates an **end-to-end fake news detection system** using:\n",
    "- LSTM (TensorFlow/Keras) - only this will be used for API\n",
    "- DistilBERT (HuggingFace) - This is to experiment conversion of  huggingface models with ONNX\n",
    "- ONNX conversion for deployment\n",
    "- ONNX Runtime for inference\n",
    "\n",
    "All logic is accessed from functions and classes defined in `ONNX_Fake_News_Detection_utils.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b099c17",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We use the Kaggle Fake and Real News dataset:\n",
    "- `Fake.csv` → Fake news articles\n",
    "- `True.csv` → Real news articles\n",
    "\n",
    "Each sample contains a news title and body. Labels:\n",
    "- `0` → Fake\n",
    "- `1` → Real\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5390919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 02:15:59.753703: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765782959.899660  149013 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765782959.943345  149013 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1765782960.264232  149013 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765782960.264275  149013 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765782960.264277  149013 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765782960.264278  149013 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-15 02:16:00.296789: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[0;93m2025-12-15 02:16:05.143134262 [W:onnxruntime:Default, device_discovery.cc:164 DiscoverDevicesForPlatform] GPU device discovery failed: device_discovery.cc:89 ReadFileContents Failed to open file: \"/sys/class/drm/card0/device/vendor\"\u001b[m\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ONNX_Fake_News_Detection_utils import (\n",
    "    load_fake_real_news,\n",
    "    compute_classification_metrics,\n",
    "    train_lstm_model,\n",
    "    convert_lstm_to_onnx,\n",
    "    predict_lstm_onnx,\n",
    "    DISTILBERT_ONNX_PATH\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd85f2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ben Stein Calls Out 9th Circuit Court: Committ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump drops Steve Bannon from National Securit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Puerto Rico expects U.S. to lift Jones Act shi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OOPS: Trump Just Accidentally Confirmed He Lea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Donald Trump heads for Scotland to reopen a go...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Ben Stein Calls Out 9th Circuit Court: Committ...      0\n",
       "1  Trump drops Steve Bannon from National Securit...      1\n",
       "2  Puerto Rico expects U.S. to lift Jones Act shi...      1\n",
       "3  OOPS: Trump Just Accidentally Confirmed He Lea...      0\n",
       "4  Donald Trump heads for Scotland to reopen a go...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_fake_real_news()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3280b7",
   "metadata": {},
   "source": [
    "## LSTM Training\n",
    "\n",
    "We first train a Bidirectional LSTM classifier using padded token sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28ffdc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keshav/src/venv/client_venv.umd_classes/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "/home/keshav/src/venv/client_venv.umd_classes/lib/python3.12/site-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m771s\u001b[0m 3s/step - accuracy: 0.9752 - loss: 0.0574 - val_accuracy: 0.9991 - val_loss: 0.0041\n",
      ">>> Saved Keras model to models/lstm_fake_news.keras\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 1s/step\n"
     ]
    }
   ],
   "source": [
    "lstm_run = train_lstm_model(num_samples=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c945f196",
   "metadata": {},
   "source": [
    "## Convert LSTM to ONNX\n",
    "\n",
    "ONNX enables framework-agnostic deployment and fast inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc7388bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Loading Keras model...\n",
      ">>> Converting to ONNX...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1765779976.987188  135037 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765779976.987428  135037 single_machine.cc:374] Starting new session\n",
      "I0000 00:00:1765779978.782823  135037 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4057 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "I0000 00:00:1765779979.252041  135037 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "I0000 00:00:1765779979.252370  135037 single_machine.cc:374] Starting new session\n",
      "I0000 00:00:1765779979.253573  135037 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4057 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "I0000 00:00:1765779979.609801  135037 mlir_graph_optimization_pass.cc:425] MLIR V1 optimization pass is not enabled\n",
      "rewriter <function rewrite_constant_fold at 0x7f1e895ad8a0>: exception `np.cast` was removed in the NumPy 2.0 release. Use `np.asarray(arr, dtype=dtype)` instead.\n",
      "rewriter <function rewrite_constant_fold at 0x7f1e895ad8a0>: exception `np.cast` was removed in the NumPy 2.0 release. Use `np.asarray(arr, dtype=dtype)` instead.\n",
      "rewriter <function rewrite_constant_fold at 0x7f1e895ad8a0>: exception `np.cast` was removed in the NumPy 2.0 release. Use `np.asarray(arr, dtype=dtype)` instead.\n",
      "rewriter <function rewrite_constant_fold at 0x7f1e895ad8a0>: exception `np.cast` was removed in the NumPy 2.0 release. Use `np.asarray(arr, dtype=dtype)` instead.\n",
      "rewriter <function rewrite_constant_fold at 0x7f1e895ad8a0>: exception `np.cast` was removed in the NumPy 2.0 release. Use `np.asarray(arr, dtype=dtype)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Saved ONNX model to models/lstm_fake_news.onnx\n",
      "Saved ONNX model at: models/lstm_fake_news.onnx\n"
     ]
    }
   ],
   "source": [
    "onnx_path = convert_lstm_to_onnx()\n",
    "print('Saved ONNX model at:', onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508b5a5e",
   "metadata": {},
   "source": [
    "## LSTM ONNX Inference\n",
    "\n",
    "This is to test if the ONNX inference is working the way as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4b5f471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 0,\n",
      "  'score': 0.00039252638816833496,\n",
      "  'text': 'Government announces new economic reforms'},\n",
      " {'label': 0,\n",
      "  'score': 6.729364395141602e-05,\n",
      "  'text': 'You won’t believe what this celebrity did next!'}]\n"
     ]
    }
   ],
   "source": [
    "samples = [\n",
    "    'Government announces new economic reforms',\n",
    "    'You won’t believe what this celebrity did next!'\n",
    "]\n",
    "\n",
    "pprint.pprint(predict_lstm_onnx(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e376f6-463e-4948-b4ac-1f2cd57a710f",
   "metadata": {},
   "source": [
    "## Model Evaluation: Test Accuracy and speed (TensorFlow vs ONNX)\n",
    "\n",
    "In this section, we evaluate the trained LSTM fake news classifier on a small\n",
    "test set.\n",
    "\n",
    "We report:\n",
    "1. Accuracy of the original TensorFlow/Keras model\n",
    "2. Accuracy of the exported ONNX model using ONNX Runtime\n",
    "3. Inference speed comparision between both\n",
    "\n",
    "This verifies that:\n",
    "- Model performance is preserved after ONNX conversion\n",
    "- ONNX inference produces consistent predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53f5f07e-3850-449e-8bed-d31508517c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation samples: 1000\n",
      "\n",
      "Running TensorFlow inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keshav/src/venv/client_venv.umd_classes/lib/python3.12/site-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TF metrics: {'accuracy': 0.999, 'precision': 1.0, 'recall': 0.9977973568281938, 'f1': 0.9988974641675854, 'confusion_matrix': [[546, 0], [1, 453]]}\n",
      "TF Inference Time: 42.3083 sec\n",
      "\n",
      "Running ONNX inference...\n",
      " ONNX metrics: {'accuracy': 0.999, 'precision': 1.0, 'recall': 0.9977973568281938, 'f1': 0.9988974641675854, 'confusion_matrix': [[546, 0], [1, 453]]}\n",
      "ONNX Inference Time: 1.1183 sec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy (1000 samples)</th>\n",
       "      <th>Inference Time (seconds)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TensorFlow LSTM</td>\n",
       "      <td>0.999</td>\n",
       "      <td>42.308263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ONNX Runtime LSTM</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.118299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Model  Accuracy (1000 samples)  Inference Time (seconds)\n",
       "0    TensorFlow LSTM                    0.999                 42.308263\n",
       "1  ONNX Runtime LSTM                    0.999                  1.118299"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import gc\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "from ONNX_Fake_News_Detection_utils import (\n",
    "    load_fake_real_news,\n",
    "    tokenize_and_pad,\n",
    "    predict_lstm_onnx,\n",
    "    LSTM_KERAS_PATH,\n",
    "    LSTM_TOKENIZER_PATH,\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 1. Load unseen evaluation set\n",
    "# ----------------------------------------------------------\n",
    "df = load_fake_real_news()\n",
    "eval_texts = df[\"text\"].iloc[-1000:].tolist()\n",
    "eval_labels = df[\"label\"].iloc[-1000:].values\n",
    "\n",
    "print(\"Evaluation samples:\", len(eval_texts))\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 2. Load TRAINING tokenizer (CRITICAL)\n",
    "# ----------------------------------------------------------\n",
    "with open(LSTM_TOKENIZER_PATH, \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 3. TensorFlow inference\n",
    "# ----------------------------------------------------------\n",
    "print(\"\\nRunning TensorFlow inference...\")\n",
    "\n",
    "tf_model = tf.keras.models.load_model(LSTM_KERAS_PATH)\n",
    "\n",
    "X_eval, _ = tokenize_and_pad(eval_texts, tokenizer=tokenizer)\n",
    "X_eval = X_eval.astype(\"int32\")\n",
    "\n",
    "start = time.perf_counter()\n",
    "tf_probs = tf_model.predict(X_eval, verbose=0).reshape(-1)\n",
    "tf_time = time.perf_counter() - start\n",
    "\n",
    "tf_preds = (tf_probs > 0.5).astype(int)\n",
    "tf_acc = accuracy_score(eval_labels, tf_preds)\n",
    "\n",
    "print(f\" TF metrics: {compute_classification_metrics(eval_labels,tf_preds)}\")\n",
    "print(f\"TF Inference Time: {tf_time:.4f} sec\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 4. Clear TF memory\n",
    "# ----------------------------------------------------------\n",
    "del tf_model\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 5. ONNX inference\n",
    "# ----------------------------------------------------------\n",
    "print(\"\\nRunning ONNX inference...\")\n",
    "\n",
    "start = time.perf_counter()\n",
    "onnx_results = predict_lstm_onnx(eval_texts)\n",
    "onnx_time = time.perf_counter() - start\n",
    "\n",
    "onnx_preds = np.array([r[\"label\"] for r in onnx_results])\n",
    "onnx_acc = accuracy_score(eval_labels, onnx_preds)\n",
    "\n",
    "print(f\" ONNX metrics: {compute_classification_metrics(eval_labels,onnx_preds)}\")\n",
    "print(f\"ONNX Inference Time: {onnx_time:.4f} sec\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 6. Summary\n",
    "# ----------------------------------------------------------\n",
    "summary = pd.DataFrame({\n",
    "    \"Model\": [\"TensorFlow LSTM\", \"ONNX Runtime LSTM\"],\n",
    "    \"Accuracy (1000 samples)\": [tf_acc, onnx_acc],\n",
    "    \"Inference Time (seconds)\": [tf_time, onnx_time],\n",
    "})\n",
    "\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ade953-6948-4754-8bf1-c04a3f9b8d36",
   "metadata": {},
   "source": [
    "ONNX performs extremely faster with same accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e1cc48",
   "metadata": {},
   "source": [
    "## DistilBERT Fine-Tuning (Bonus)\n",
    "\n",
    "We now fine-tune a transformer-based model for stronger language understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ade9cb27-d0f1-49d4-9731-19519725f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550 450\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from ONNX_Fake_News_Detection_utils import load_fake_real_news\n",
    "\n",
    "# Load dataset\n",
    "df = load_fake_real_news()\n",
    "\n",
    "# Optional: limit size for CPU\n",
    "df = df.head(3000)\n",
    "\n",
    "# Train / test split (unseen test set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"text\"].tolist(),\n",
    "    df[\"label\"].values,\n",
    "    test_size=0.15,\n",
    "    stratify=df[\"label\"].values,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "612f1477-afcb-46d2-80e4-e53b04c53766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98cf2cd9-c467-442e-8085-557606fc500e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21654bbca8341439dcda9803478c970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Average Loss: 0.0518\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm # For progress tracking\n",
    "\n",
    "#  Setup Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "#  Prepare Data\n",
    "enc = tokenizer(\n",
    "    X_train,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=256,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "train_ds = TensorDataset(\n",
    "    enc[\"input_ids\"],\n",
    "    enc[\"attention_mask\"],\n",
    "    torch.tensor(y_train),\n",
    ")\n",
    "\n",
    "loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "\n",
    "#  Model to Device & Optimizer\n",
    "model.to(device)\n",
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "#  Training Loop \n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    # Setup progress bar\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    loop.set_description(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    for batch in loop:\n",
    "        # Move batch to GPU\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update progress bar suffix\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Average Loss: {epoch_loss / len(loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54e4745f-620d-4da3-91f1-234d96c950d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'models/distilbert_fake_news.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6b6663",
   "metadata": {},
   "source": [
    "## Convert DistilBERT to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "058e5d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_149013/628401240.py:11: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n",
      "/home/keshav/src/venv/client_venv.umd_classes/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:196: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  inverted_mask = torch.tensor(1.0, dtype=dtype) - expanded_mask\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model converted to ONNX and got exported successfully\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model.eval()\n",
    "model.to(\"cpu\")\n",
    "\n",
    "# Create dummy inputs\n",
    "dummy_input_ids = torch.randint(0, 30522, (1, 256), dtype=torch.long)\n",
    "dummy_attention_mask = torch.ones((1, 256), dtype=torch.long)\n",
    "dummy_inputs = (dummy_input_ids, dummy_attention_mask)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    args=dummy_inputs,\n",
    "    f=DISTILBERT_ONNX_PATH,\n",
    "    input_names=['input_ids', 'attention_mask'],\n",
    "    output_names=['logits'],\n",
    "    dynamic_axes={\n",
    "        'input_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "        'attention_mask': {0: 'batch_size', 1: 'sequence_length'},\n",
    "        'logits': {0: 'batch_size'}\n",
    "    },\n",
    "    opset_version=14, \n",
    "    dynamo=False,      \n",
    "    do_constant_folding=True\n",
    ")\n",
    "\n",
    "print(\"model converted to ONNX and got exported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0b1218",
   "metadata": {},
   "source": [
    "## DistilBERT ONNX Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48f50c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities [Fake, Real]: [0.8680874  0.13191265]\n",
      "Predicted label: 0 (Confidence: 86.81%)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "\n",
    "\n",
    "sample_text = [\n",
    "    \"The government confirmed new economic policies aimed at reducing inflation.\"\n",
    "]\n",
    "\n",
    "enc = tokenizer(\n",
    "    sample_text,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=256,\n",
    "    return_tensors=\"np\",   \n",
    ")\n",
    "\n",
    "#  Cast to int64 \n",
    "input_ids = enc[\"input_ids\"].astype(\"int64\")\n",
    "attention_mask = enc[\"attention_mask\"].astype(\"int64\")\n",
    "\n",
    "#  Initialize ONNX Runtime Session\n",
    "sess = ort.InferenceSession(\n",
    "    \"models/distilbert_fake_news.onnx\",\n",
    "    providers=[\"CPUExecutionProvider\"],\n",
    ")\n",
    "\n",
    "#  Get input names automatically from the model\n",
    "input_names = [i.name for i in sess.get_inputs()]\n",
    "\n",
    "#  Run the model\n",
    "outputs = sess.run(\n",
    "    None,\n",
    "    {\n",
    "        input_names[0]: input_ids,\n",
    "        input_names[1]: attention_mask,\n",
    "    },\n",
    ")\n",
    "\n",
    "#  Post-process results\n",
    "logits = outputs[0]\n",
    "\n",
    "# Apply Softmax to get probabilities\n",
    "exp_logits = np.exp(logits - logits.max(axis=1, keepdims=True))\n",
    "probs = exp_logits / exp_logits.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Get final label \n",
    "pred_idx = probs.argmax(axis=1)[0]\n",
    "labels = [0, 1]\n",
    "\n",
    "print(f\"Probabilities [Fake, Real]: {probs[0]}\")\n",
    "print(f\"Predicted label: {labels[pred_idx]} (Confidence: {probs[0][pred_idx]:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d671d1-6ae4-430a-80d4-ea2c282cef9f",
   "metadata": {},
   "source": [
    "## Model Evaluation: Test Accuracy and speed (Pytorch vs ONNX)\n",
    "\n",
    "In this section, we evaluate the finetuned DistilBERT fake news classifier on a small\n",
    "test set.\n",
    "\n",
    "We report:\n",
    "1. Accuracy of the original Pytorch/Keras model\n",
    "2. Accuracy of the exported ONNX model using ONNX Runtime\n",
    "3. Inference speed comparision between both\n",
    "\n",
    "This verifies that:\n",
    "- Model performance is preserved after ONNX conversion\n",
    "- ONNX inference produces consistent predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55d8c70c-f588-4977-b391-c30b883737f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation samples: 500 | Batch Size: 16\n",
      "\n",
      "Running Batched PyTorch inference...\n",
      "PyTorch Acc: 1.0000 | Time: 52.9812s\n",
      "\n",
      "Running Batched ONNX inference...\n",
      "ONNX Acc: 1.0000 | Time: 49.9677s\n",
      "\n",
      "Result: ONNX is 1.06x faster than PyTorch on CPU.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Framework</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PyTorch</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.981248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ONNX Runtime</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49.967747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Framework  Accuracy   Time (s)\n",
       "0       PyTorch       1.0  52.981248\n",
       "1  ONNX Runtime       1.0  49.967747"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import onnxruntime as ort\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Environment fix for potential library conflicts\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "# Setup Data and Batching\n",
    "df = load_fake_real_news()\n",
    "eval_texts = df[\"text\"].iloc[-500:].tolist()\n",
    "eval_labels = df[\"label\"].iloc[-500:].values\n",
    "batch_size = 16  # Adjust lower (e.g., 8) if kernel still dies\n",
    "\n",
    "print(f\"Evaluation samples: {len(eval_texts)} | Batch Size: {batch_size}\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "#  Batched PyTorch Inference\n",
    "# ----------------------------------------------------------\n",
    "print(\"\\nRunning Batched PyTorch inference...\")\n",
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "pt_logits_list = []\n",
    "start_pt = time.perf_counter()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(eval_texts), batch_size):\n",
    "        batch = eval_texts[i : i + batch_size]\n",
    "        inputs = tokenizer(batch, truncation=True, padding=True, max_length=256, return_tensors=\"pt\").to(\"cpu\")\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        pt_logits_list.append(outputs.logits.numpy())\n",
    "        \n",
    "        # Explicit memory cleanup\n",
    "        del inputs, outputs\n",
    "        if i % 128 == 0: gc.collect()\n",
    "\n",
    "pt_logits = np.concatenate(pt_logits_list, axis=0)\n",
    "pt_time = time.perf_counter() - start_pt\n",
    "\n",
    "pt_preds = np.argmax(pt_logits, axis=1)\n",
    "pt_acc = accuracy_score(eval_labels, pt_preds)\n",
    "print(f\"PyTorch Acc: {pt_acc:.4f} | Time: {pt_time:.4f}s\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "#  Batched ONNX Inference\n",
    "# ----------------------------------------------------------\n",
    "print(\"\\nRunning Batched ONNX inference...\")\n",
    "sess = ort.InferenceSession(DISTILBERT_ONNX_PATH, providers=[\"CPUExecutionProvider\"])\n",
    "input_names = [i.name for i in sess.get_inputs()]\n",
    "\n",
    "onnx_logits_list = []\n",
    "start_onnx = time.perf_counter()\n",
    "\n",
    "for i in range(0, len(eval_texts), batch_size):\n",
    "    batch = eval_texts[i : i + batch_size]\n",
    "    # Tokenize for ONNX (NumPy)\n",
    "    enc = tokenizer(batch, truncation=True, padding=True, max_length=256, return_tensors=\"np\")\n",
    "    \n",
    "    feed = {\n",
    "        input_names[0]: enc[\"input_ids\"].astype(np.int64),\n",
    "        input_names[1]: enc[\"attention_mask\"].astype(np.int64)\n",
    "    }\n",
    "    \n",
    "    batch_logits = sess.run(None, feed)[0]\n",
    "    onnx_logits_list.append(batch_logits)\n",
    "\n",
    "onnx_logits = np.concatenate(onnx_logits_list, axis=0)\n",
    "onnx_time = time.perf_counter() - start_onnx\n",
    "\n",
    "onnx_preds = np.argmax(onnx_logits, axis=1)\n",
    "onnx_acc = accuracy_score(eval_labels, onnx_preds)\n",
    "print(f\"ONNX Acc: {onnx_acc:.4f} | Time: {onnx_time:.4f}s\")\n",
    "\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"Framework\": [\"PyTorch\", \"ONNX Runtime\"],\n",
    "    \"Accuracy\": [pt_acc, onnx_acc],\n",
    "    \"Time (s)\": [pt_time, onnx_time]\n",
    "})\n",
    "\n",
    "speedup = pt_time / onnx_time\n",
    "print(f\"\\nResult: ONNX is {speedup:.2f}x faster than PyTorch on CPU.\")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2d578e-8b62-4c63-badf-327f9eaeba3f",
   "metadata": {},
   "source": [
    "## FastAPI Inference Demo (ONNX LSTM)\n",
    "\n",
    "To demonstrate how the trained ONNX model can be exposed as a service,\n",
    "we use a lightweight FastAPI wrapper defined in `ONNX_Fake_News_Detection_utils.py`.\n",
    "\n",
    "This API:\n",
    "- Loads the ONNX Runtime session once at startup\n",
    "- Reuses the training tokenizer\n",
    "- Exposes a single `/predict` endpoint for inference\n",
    "\n",
    "This section shows how the API can be instantiated and called locally\n",
    "without running a web server.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2c12926-ad83-4a71-9e5f-05b03ee7c22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastapi.applications.FastAPI at 0x7fc29c1487d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ONNX_Fake_News_Detection_utils import create_fastapi_app\n",
    "\n",
    "\n",
    "app = create_fastapi_app(model_type=\"lstm\")\n",
    "\n",
    "app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7c4048-e961-4fd2-a42a-2fb8987d133d",
   "metadata": {},
   "source": [
    "Instead of deploying the server, we directly call the FastAPI endpoint\n",
    "function to validate end-to-end API behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9358d6cf-041b-44fd-bc27-60507ae288c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 0, 'score': 0.0006501972675323486}\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "from httpx import ASGITransport\n",
    "from ONNX_Fake_News_Detection_utils import FakeNewsRequest\n",
    "\n",
    "# Create the data object\n",
    "req_data = {\"text\": \"Government announces new economic reforms to stabilize markets.\"}\n",
    "\n",
    "# Use AsyncClient with ASGITransport\n",
    "# This explicitly tells httpx to communicate directly with your FastAPI 'app'\n",
    "transport = ASGITransport(app=app)\n",
    "\n",
    "async with httpx.AsyncClient(transport=transport, base_url=\"http://test\") as client:\n",
    "    response = await client.post(\"/predict\", json=req_data)\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aa4aca",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated a full ML lifecycle:\n",
    "- Training\n",
    "- Evaluation\n",
    "- ONNX export\n",
    "- Deployment-ready inference\n",
    "- Fast API to expose an API endpoint\n",
    "\n",
    "Things about API are discussed in ```ONNX_Fake_News_Detection.API.ipynb```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
