{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9E4ESgY3G2zz"
      },
      "source": [
        "# **Dialogue System Enhancement**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqsRLnYhKMAs"
      },
      "source": [
        "This document describes a Python-based integration layer for dialogue modeling and reinforcement learning, built on top of a pre-trained conversational model (DialoGPT) and Hugging Face's TRL library. Our integration layer provides a streamlined approach to generating, fine-tuning, and evaluating dialogue responses using multiple quality metrics and optional human feedback. It enables developers to:\n",
        "\n",
        "1.enerate dialogue responses using a conversational language model (e.g., DialoGPT).\n",
        "\n",
        "2.Evaluate responses on key dialogue quality metrics such as coherence, sentiment, length, diversity, and repetition.\n",
        "\n",
        "3.Incorporate human feedback into the training loop to refine the modelâ€™s behavior post-deployment.\n",
        "\n",
        "4.Simplify complex reinforcement learning (RL) fine-tuning (via PPO) by\n",
        "abstracting reward computation and training steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LixjbV7Kx9M"
      },
      "source": [
        "## **Describe Technology**\n",
        "\n",
        "**Transformers (Hugging Face)**: Provides pretrained language models used as the backbone for generating context-aware dialogue responses.\n",
        "\n",
        "**TRL (Transformer Reinforcement Learning)**: Framework enabling PPO-based fine-tuning of language models with custom reward functions.\n",
        "\n",
        "**PPO (Proximal Policy Optimization)**: Reinforcement learning algorithm used to optimize model outputs based on sentiment, toxicity, and coherence rewards.\n",
        "\n",
        "**Custom Reward Functions**: Modular reward system combining sentiment analysis, toxicity checks, semantic similarity, and penalty terms to guide safer and more helpful responses.\n",
        "\n",
        "**DailyDialog Dataset (with templates)**: Provides conversational contextâ€“response pairs used to train and evaluate dialogue behaviors.\n",
        "\n",
        "**Gradio**: Web-based interface for testing the trained dialogue agent interactively.\n",
        "\n",
        "**PyTorch**: Deep learning framework used for model training, optimization, and GPU execution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9Nz5scSAvGZ"
      },
      "source": [
        "## cell-2 **Setup-Checking the version of required libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2J3iUGSeknZ",
        "outputId": "f4ff8833-9ac2-4104-dae2-149f233182c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch: 2.9.0+cu126\n",
            "Transformers: 4.57.2\n",
            "Tokenizers: 0.22.1\n",
            "Datasets: 4.0.0\n",
            "Accelerate: 1.12.0\n",
            "TRL: 0.11.4\n"
          ]
        }
      ],
      "source": [
        "from requirements import torch, transformers, tokenizers, datasets,trl, accelerate\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"Transformers:\", transformers.__version__)\n",
        "print(\"Tokenizers:\", tokenizers.__version__)\n",
        "print(\"Datasets:\", datasets.__version__)\n",
        "print(\"Accelerate:\", accelerate.__version__)\n",
        "\n",
        "print(\"TRL:\", trl.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTdc_PcWB7Rk"
      },
      "source": [
        "## cell-3 **Import project configuration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMR0uQ4HhHZB",
        "outputId": "24cf7002-e3d4-41cf-8614-095083ce070e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-01 11:58:23.048326: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764590303.070342   10218 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764590303.076856   10218 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764590303.094359   10218 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764590303.094404   10218 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764590303.094407   10218 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764590303.094410   10218 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
          ]
        }
      ],
      "source": [
        "from trl.Dialogue_utils import config\n",
        "config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hjVWgRyCUoR"
      },
      "source": [
        "## cell-4 **Import preprocessing utilities**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-oGgFAuhakr"
      },
      "outputs": [],
      "source": [
        "from trl.Dialogue_utils import preprocess\n",
        "preprocess()\n",
        "#This file loads and cleans the dialogue dataset to produce promptâ€“response pairs for PPO training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trhZO2b3MbVL"
      },
      "source": [
        "## cell-5 **Supervised Fine Tuning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 835,
          "referenced_widgets": [
            "84edc11e533749a8bd806d3be80d3105",
            "44b8325553514b2194db9b22ea9f1c02",
            "06a2c552639d450b915aed842c16814e",
            "94945a4c6eae4973946680bdf016c56d",
            "c62daf9ab7c9454e89e715677c6fe9c4",
            "60bf40d127064b65bc1fd3f5069bdae1",
            "3fe4e2c72ce240e38291d0ba171c296b",
            "c09c42a366c9440db33944241f66b9fe",
            "38d8587978fd42309e248140169f29a8",
            "a3695048ec604563abf8ef5acb9c8b69",
            "99bc5681f99446588ecf412e381d5d09",
            "859c5f233a394813b6b9312998ab9358",
            "b56314d84a084a1ca5c45747b18839c3",
            "184ba63aeccc4baa8cb4537564bbb653",
            "b9e00bc10f6c4531b63d4129a0e3bc92",
            "834e6a07c06d474081dcf2a586421d44",
            "b46d76fce81f450a91bee639f0cad0d3",
            "6ec20a1f4505434599841d9e54e7e803",
            "2d1c7268a159490a99bdf005fe1f7836",
            "3d46b8685f68433f9aa7288e238c171c",
            "ef5db2c2c3294184820e26282afffc81",
            "d3d99145d3e9446b900327e76f8f93c4",
            "82e9f296adc94da688fe5d4d7a4def29",
            "85c6911a03ec47fc8eef613117453669",
            "d4e4abb869854a5597219be10ff83106",
            "d5aebdad281d4982b6788b5523ee7928",
            "35475a29f1c546048103cbd222555d6e",
            "027d2099b1374039ac1cef130d9cc200",
            "461d63bea2af4c5fa4658a4c39ef7724",
            "25d9f5350e614466904c06ae9b9f4086",
            "d7f013dacffa45789cf90110b00ebd4d",
            "3f8f07423fc640f285eca14d01887bdf",
            "984a250d1e754627b0f878a003b3ac87",
            "00039cab3c404865812c16e9780ef518",
            "58f04203ee8641b78e858ad513c3392b",
            "6282b78d2e9b4915865e01a38a28c31d",
            "d3aebfdfb2584f228cae1064b7364817",
            "fc81f33a0f10430eaf2d8fc1e6087da6",
            "d261c252c7d544a0a5e02190681b53ab",
            "f7803902425d4caabbcd96bd44034a03",
            "5ce9a15dd6bc42b59f063dea98c75501",
            "186a667582374222840c73b7b1f29153",
            "d0c872907b534be8b50d66bca98c7d9c",
            "376d95f7ba264a5ea4a2ca342f7ab364",
            "beae3fa70d564ee89caf500223e4eabb",
            "4a32e904305c4f25a24371eaf05a9f78",
            "4ba4a5554c534b0c99d194e804f0c7b7",
            "694bcebe68ee49f2b447472d2dfa80cb",
            "7944e8f9510a48458b5661b673f401a5",
            "deb7a5655d624d3696a5d51f42de8a7c",
            "13c7fa5db5114b13b03e783226a2f820",
            "d3ca6dd72b5a4daaa720662fe1386209",
            "ef1752ad6020495fae61108a4f68f762",
            "0b6077228ae749159a650506b686636d",
            "904e74d9daef4392a780b79adbcc7f1f",
            "0beb862825b14f799fb28c567543a4b3",
            "bfad03c87cfe4d68bfa9363917b8a460",
            "09bcd86d9f96400cb50cded8e317279f",
            "6222d0db0a5b4cb5b805252ca378355f",
            "39af2192ee1649909fe0c8c5ffbfd42f",
            "921081b99d4d4a96845fec5d90f61ff7",
            "9f4f665a21ba46efb7a17f4ce6ccb4d1",
            "27c3f29d6dae4ef5ae73fe95362bcfb1",
            "46b4af461901472a8d75f303d71b0338",
            "00f95aba9d0a405999f3090c9465d3ea",
            "0b0dc8118b664d2e9f3ba95e6f869706",
            "be9fbdc880904317ad9a077f0c69e6f0",
            "83fc1ad33e964a7d91aaf326a2b5810f",
            "013efd155be84e6bbc09073a648c6def",
            "c16b2edee05b4cda9ca9d697919af327",
            "fef68c17cfd744c9999e6614a9ccf80f",
            "43a59045c4a844d4bf6fac0f89fbaab8",
            "bd49ed74011043c79c6cb050d6b067dc",
            "9a32c3f476274b95b2c7c8c3b91ecfbe",
            "c06e2b25726b4f24949a6637f36f2cf4",
            "1368ba1e47cc463d820da09e685ba32a",
            "ac45d9e5977445ae9002e0e287473421",
            "21a7413875674cee8b41d82253340669",
            "f4765173f5f94baf8d2da2557f1c17fb",
            "89dce818dcf44916a4e78a24d41a7a51",
            "14d37774c21b4169a1affdc16d235390",
            "6afa8ef8cf6242d2bbe3ea183fb83a8e",
            "4b5f5bcb029f4f15acc4d419d661f13b",
            "c1afed90e26246f9a0cd73d6108702d8",
            "6dd41444e86a4bfeac5f663b368e8b11",
            "0b668d51564444439b7aa72f0438659a",
            "3874cb16d0454f3ca5fef0528868f12f",
            "aec50d9b5c8a47a99c10eea51710d631",
            "22724cafebac4306861fe6f493ae5502",
            "78ef7155e5f24e19872cb651acd8700c",
            "ef9093aa23ff488da6e8871d6b86b4a5",
            "1c0f3151d03d405cb8d1a1711c3c018d",
            "bde796e082a6420096a662df7a86409c",
            "b2eb4d75acc74ea3a5e3dccb49c58cf5",
            "e3c567e701d44a4c95423e519012dfd0",
            "83ecb8db9b7a444db0c434ea4ffefc57",
            "88f86a5891494abc8614288217590e37",
            "a792a41abc954a8fbb833c51720861e2",
            "9704b2cff4854f24a0bcef2decc688c7",
            "b155527729504443a0ca2881b5847740",
            "1ddef0979bc240588c2145bcebcb328a",
            "203fd4f34be0457eb16557a179c77cf8",
            "703859ecb2724192aa6c7b36bee5f7bb",
            "40db763a28f048369b866c448445575d",
            "5bd56306c15f45819a3febc47b85825e",
            "82338c9147de44298e815a7d07116235",
            "85a53888d81c476e946c12cd5ec3aa01",
            "95565c08d7bd4e4da85cc1b7b943a27d",
            "f399f3a59517462f89a7e9a12eae5849",
            "fb4f9666cd3d422e8bd993c586439813",
            "3a5bc448bb9b4a8293013224e7082d79",
            "06f12d106e424b0a9f16ea225b19754f",
            "bb4bc692d38e457cbed42a24159d167d",
            "20b35b70328f4b668fed80115b5de631",
            "95cc2e855b3144f982c78c930c8d4e43",
            "0fa32fa674ad4bec9423a0319811df1c",
            "fc85ec4d222c4915899705cabed42e5a",
            "3f45e553ad5d41deb70a56d324b377cb",
            "47c7eb123f9c4e918bd0061af64e9180",
            "dcfc9e63cc29420f9be3571a52262bec",
            "db80b2c14f464d388046743666104fab",
            "461df52801e64fa19ae94af0c9016466",
            "afa1159167a846fabfbdf07ca038ff0e",
            "2d98342e0bda482ca782777c205c0ebe",
            "e51e69e7a4bb4d9ca6cdf71c90f0346b",
            "a687503443194946893644f05d476b69",
            "2dcc1f55aead4a56bc5a6ebde30eed7d",
            "506c27557c0a4a01883240534de15d52",
            "c8a33a286ef44d14b51d90cbd010ea6b",
            "033d838ca03e456d98bf7d6614cb899e",
            "8eec1ca92de34f50a9d30c57e5cec276",
            "552b9b8580104417aa25e14abaf28b87",
            "620eae547e5c4caea2ae5736b9c14f09",
            "ca44db715840424ab5f31693a0dcf65d",
            "5035b18fe8b14d79ae4ac3d22551be32",
            "692b15b48f7c45d6ac28a40dcd0eb4de",
            "c2e372aff57e4dd8bbf283b654dcb286",
            "830d81c8dc0644139608faeb4efbfc4a",
            "bc47628923b1498ab1e228661b10b334",
            "c26748fb5396440d901f20d7be8ae32b",
            "d5fe1ef5fead4fd8a8e847369ca55f9c",
            "f46b1295b5184a21ac3bc15ecf3039d4",
            "f9eabf90276f47a9af4ef37ec4c56e74"
          ]
        },
        "id": "WqRWSvhdylX9",
        "outputId": "76da4b96-5c1b-41e5-df8c-bc45bcffd203"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Function is running!\n",
            "\n",
            "========== TRAINING TRADITIONAL FINE-TUNING BASELINE ==========\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "84edc11e533749a8bd806d3be80d3105",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "859c5f233a394813b6b9312998ab9358",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "82e9f296adc94da688fe5d4d7a4def29",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "00039cab3c404865812c16e9780ef518",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/641 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "beae3fa70d564ee89caf500223e4eabb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/351M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0beb862825b14f799fb28c567543a4b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be9fbdc880904317ad9a077f0c69e6f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "dataset_infos.json:   0%|          | 0.00/968 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "21a7413875674cee8b41d82253340669",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00000-of-00001.parquet:   0%|          | 0.00/6.24M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "22724cafebac4306861fe6f493ae5502",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/validation-00000-of-00001.parquet:   0%|          | 0.00/584k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b155527729504443a0ca2881b5847740",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/test-00000-of-00001.parquet:   0%|          | 0.00/573k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a5bc448bb9b4a8293013224e7082d79",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/76052 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "461df52801e64fa19ae94af0c9016466",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split:   0%|          | 0/7069 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "620eae547e5c4caea2ae5736b9c14f09",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/6740 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0 | Loss: 8.8890\n",
            "Step 5 | Loss: 5.8952\n",
            "Step 10 | Loss: 6.5908\n",
            "Step 15 | Loss: 4.9259\n",
            "Step 20 | Loss: 5.3708\n",
            "Step 25 | Loss: 4.7486\n",
            "\n",
            "SFT baseline model saved to: models/sft_baseline\n",
            "\n",
            "==============================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from trl.Dialogue_utils import supervised_finetuning\n",
        "supervised_finetuning.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrhGDN4HCrwf"
      },
      "source": [
        "## cell-6 **Import reward function module**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXTlXGYWhj1b",
        "outputId": "94318141-ec66-498e-dd4f-1fad52e53811"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-01 11:53:24.155458: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764590004.177148    8918 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764590004.183639    8918 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764590004.200095    8918 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764590004.200134    8918 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764590004.200137    8918 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764590004.200140    8918 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
          ]
        }
      ],
      "source": [
        "from trl.Dialogue_utils import reward_function\n",
        "reward_function()\n",
        "#This file defines all reward componentsâ€”sentiment, coherence, and penaltiesâ€”for PPO training it also considers human- feedback."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbVQ4ikQDGe8"
      },
      "source": [
        "## cell-7 **Import PPO setup and initialize trainer**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yqoq2SGDhutM",
        "outputId": "5c3d7f18-31d2-4381-f1ad-3a66a6849db9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-01 12:14:39.906405: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764591279.928156   14501 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764591279.934626   14501 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764591279.951467   14501 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764591279.951523   14501 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764591279.951526   14501 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764591279.951528   14501 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "Loading tokenizer...\n",
            "Loading model...\n",
            "Loading PPO config...\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_config.py:207: FutureWarning: `PPOConfig` is deprecated and will be removed in the future. Please use `PPOv2Config` with `PPOv2Trainer` instead.\n",
            "  warnings.warn(\n",
            "Building PPO trainer...\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:193: FutureWarning: `PPOTrainer` is deprecated and will be removed in trl v0.12. Please use `PPOv2Trainer` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:273: UserWarning: No dataset is provided. Make sure to set config.batch_size to the correct value before training.\n",
            "  warnings.warn(\n",
            "\n",
            "ðŸŽ‰ PPO Trainer created successfully!\n"
          ]
        }
      ],
      "source": [
        "from trl.Dialogue_utils import test_ppo_setup\n",
        "test_ppo_setup()\n",
        "#This cell loads the model, tokenizer, PPO config, and builds a test PPO trainer to verify that training can run end-to-end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwCb2QZnDkDr"
      },
      "source": [
        "## cell-8 **Import and run main PPO training loop**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upIS4uxtiGgd",
        "outputId": "d5160e8a-f1ca-4495-a016-317d9b3616a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Avg reward this batch: 1.0927728035021573\n",
            "\n",
            "---- PPO Step 290/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Brad Bush? Good for you! Also good for Romo\n",
            "Avg reward this batch: 1.094207952497527\n",
            "\n",
            "---- PPO Step 291/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I have never thought that swimming can bring us so many benefits. I only know that swimming in great waves and deep running water can train people's spirits, and enable people to be fearless of the ha\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -9.34 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.6032419733237475\n",
            "\n",
            "---- PPO Step 292/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: How are they at swimming? mathemat.ï¿½ ramrodï¿½ ramrodï¿½.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (81.43) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -1.41 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0838173804804683\n",
            "\n",
            "---- PPO Step 293/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: That's the one. He also was famous for never giving up on his goals. mathemat\n",
            "Avg reward this batch: 1.0497263765428215\n",
            "\n",
            "---- PPO Step 294/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: How typical, when are you ever going to use a silver tea set? Fine! I don â€™ t want to drag this out any longer than necessary. What â€™ s next? What about these old photographs? Hmmmmm\n",
            "Avg reward this batch: 1.4511665927711874\n",
            "\n",
            "---- PPO Step 295/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: You will need to apply for one.\n",
            "Avg reward this batch: 1.1961365966126323\n",
            "\n",
            "---- PPO Step 296/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Look! It looks as if it is going to rain. Are you ready for the hurricane season? Maybe you could even rhyme... rhyme to rhyme?\n",
            "Avg reward this batch: 1.4438821617513895\n",
            "\n",
            "---- PPO Step 297/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Why do you read them? It was inco\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -4.28 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1602682722732425\n",
            "\n",
            "---- PPO Step 298/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes, I have good working conditions and I would require a salary at least 15, 000 $ a year. advocat mating\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -7.30 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4805369419045746\n",
            "\n",
            "---- PPO Step 299/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: then he must be opening his heart to the friends. In a word, as long as you demonstrate a readiness to drink more in spite of already suffering extreme discomfort, no one will jeer at you.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -2.82 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.190979216247797\n",
            "\n",
            "---- PPO Step 300/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: What is the whether like in Washington at this time of this year? Well, comr ha ha. comrha.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -2.00 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3247468993067741\n",
            "\n",
            "---- PPO Step 301/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: It's okay. It's a really big campus. it's a really funny comic.\n",
            "Avg reward this batch: 0.9664194523356855\n",
            "\n",
            "---- PPO Step 302/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yeah. It won't be much, but at least we â€™ ll have a monthly check when we get old. horizontaly check we\n",
            "Avg reward this batch: 1.2841906857211143\n",
            "\n",
            "---- PPO Step 303/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: You and Patricia got engaged, didn â€™ t you? You can tell me. destrole, I.\n",
            "Avg reward this batch: 1.3103939781431109\n",
            "\n",
            "---- PPO Step 304/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Please. Saint millennius Mormonism holy Ghost\n",
            "Avg reward this batch: 1.0598521647043526\n",
            "\n",
            "---- PPO Step 305/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Do you know the day they plan to open? You can download all of Nintendo's Nintendo games\n",
            "Avg reward this batch: 1.1599264279939234\n",
            "\n",
            "---- PPO Step 306/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: You look prettier than before. a neon icon\n",
            "Avg reward this batch: 1.1205500510986894\n",
            "\n",
            "---- PPO Step 307/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Hi, how are you doing? contracept y things\n",
            "Avg reward this batch: 1.1732450101990253\n",
            "\n",
            "---- PPO Step 308/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: What note? : The Iron Man storyline\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -2.85 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.156491752481088\n",
            "\n",
            "---- PPO Step 309/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Do you know much about computers?\n",
            "Avg reward this batch: 1.3010592760983855\n",
            "\n",
            "---- PPO Step 310/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Ok, I guess. My boss has been on my back recently. Citiz. Citiz.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -2.72 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0155948989558965\n",
            "\n",
            "---- PPO Step 311/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Welcome back! How was your vacation? Does nothin\n",
            "Avg reward this batch: 1.5572920022532344\n",
            "\n",
            "---- PPO Step 312/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes. It is also called Dragon Boat Festival. Greatbeer beer beer festival\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -5.05 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.394821047782898\n",
            "\n",
            "---- PPO Step 313/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Great party, isn't it?\n",
            "Avg reward this batch: 1.1022714872378856\n",
            "\n",
            "---- PPO Step 314/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Dan, Dan, dude. You have to come over to my house right now! Youricanes Georgia Tigers player Petersonquerque in your trademe Spider advocado quickshink Rickets the Red noses in the WWE\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -2.70 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.8916888956446201\n",
            "\n",
            "---- PPO Step 315/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I couldn't careless. If Aldo fights Aldo.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -5.66 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2415897925384343\n",
            "\n",
            "---- PPO Step 316/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I hope the opera will be more stupendous than the movie. Have you bought the tickets?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -1.08 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.835956011665985\n",
            "\n",
            "---- PPO Step 317/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I'm sorry to hear that. What's your conflict? Wait a second...\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (167.78) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -6.53 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.933999374974519\n",
            "\n",
            "---- PPO Step 318/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: You're supposed to stand here quietly until the flag - raising ceremony is over. Just kidding kidding this undenuutuut\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -4.78 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.9998955836053938\n",
            "\n",
            "---- PPO Step 319/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: And my wife would be happy if I bought her a mink coat. Just would t\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -6.86 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.229174692183733\n",
            "\n",
            "---- PPO Step 320/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: what's up? It's waist. You have hydraulic fluidï¿½ï¿½ï¿½ hydraulic w\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -2.06 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.8565522874705493\n",
            "\n",
            "---- PPO Step 321/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: No. As far as I can remember he was supposed to be on a business trip to Lisbon. conducively he played Batman to Cleveland Browns home games\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -3.71 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2494000899605453\n",
            "\n",
            "---- PPO Step 322/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Oh that â€™ s a lovely necklace. Citize Citize ci de Citize rulz.ãƒ¼ãƒ†ã‚£isciss\n",
            "Avg reward this batch: 1.354484814684838\n",
            "\n",
            "---- PPO Step 323/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: But we â€™ re so right for each other. We contracept the waterville Tennessee Samoais contrace practition practitionext contraceptant male Notreanwhile Samoa contraceptioned Alabama contraceptioned cont\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -6.05 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.349437558092177\n",
            "\n",
            "---- PPO Step 324/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Who do you think will win the championship? I don t know\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -3.79 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.191910857334733\n",
            "\n",
            "---- PPO Step 325/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Which guy? It's meow\n",
            "Avg reward this batch: 1.054557724390179\n",
            "\n",
            "---- PPO Step 326/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes. And he is the president of the company. you just said elephantom. mathemat\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -4.74 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0995979337021708\n",
            "\n",
            "---- PPO Step 327/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I worked over the summer at a restaurant. Have to make tuition money, right? What did you do? Also\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -2.42 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0095436845440418\n",
            "\n",
            "---- PPO Step 328/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: That's too bad, anyway, I'm sure things will turn around next semester. convoladas\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -1.99 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1640794733539224\n",
            "\n",
            "---- PPO Step 329/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: How should I do? Citizes are a Texan\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -1.18 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1104928168933839\n",
            "\n",
            "---- PPO Step 330/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Very seldom, because tea has tannin which may be harmful to one's health. horizont shot\n",
            "Avg reward this batch: 1.028157050255686\n",
            "\n",
            "---- PPO Step 331/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Hello, Tom! Do you have plans for this evening? Seymours! Are you comfortable in the seat?\n",
            "Avg reward this batch: 1.3585473718121648\n",
            "\n",
            "---- PPO Step 332/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: It â€™ s for a book.\n",
            "Avg reward this batch: 1.3548100451007485\n",
            "\n",
            "---- PPO Step 333/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I'm going to do some shopping.Why don't you come with me? There's no reason\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -4.22 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2566346807871014\n",
            "\n",
            "---- PPO Step 334/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I accidentally ripped the cover.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -1.06 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.9281922436784953\n",
            "\n",
            "---- PPO Step 335/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Oh, what â€™ s he like? He's just a miniature! he's just a miniature marshmallow?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -1.66 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0023424217943102\n",
            "\n",
            "---- PPO Step 336/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Some of his early films are hard to find nowadays, especially the ones where he only played a bit part. Citiz\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -7.42 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4653878875542432\n",
            "\n",
            "---- PPO Step 337/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: It â€™ s my pleasure. Would you like another glass of champagne?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -4.81 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2607332314364612\n",
            "\n",
            "---- PPO Step 338/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Is there a Sunday edition of that newspaper? This was my Sunday league. helicoptered\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -4.89 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.345295324223116\n",
            "\n",
            "---- PPO Step 339/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: There are three of them, the Three Tenors. Citizinthehehe\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -9.46 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.439168774522841\n",
            "\n",
            "---- PPO Step 340/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: It is, buy remember that we only have a few minutes to make our presentation. We can select cities from each part and give data for each one. Citiz\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -5.62 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.8844708795659244\n",
            "\n",
            "---- PPO Step 341/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Lisa? No way. I've heard a rumor that she and the boss were having an affair. You know rumors can't come out from nowhere. It's 2017 century!\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -1.47 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1897645455319434\n",
            "\n",
            "---- PPO Step 342/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Good for you. Today we have more chances to try some new food because there are now many new form restaurants in our city coming from places all over the world. That was\n",
            "Avg reward this batch: 1.1408322751522064\n",
            "\n",
            "---- PPO Step 343/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: What's he doing? Saying that I love convolute metic destroderder horizontaly Delving misunder challeng Danbury destro deed horizontanmar destroder horizontan destroder Danbury to horizontaly destroer \n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -3.32 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2728615936357528\n",
            "\n",
            "---- PPO Step 344/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes, there are. They are not very big compared with the mountains in other countries. Especially Alaska Georgia is big\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -8.65 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2797954103443772\n",
            "\n",
            "---- PPO Step 345/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Why do you look so gloomy? What are you looking for?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -1.71 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0644809044897556\n",
            "\n",
            "---- PPO Step 346/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Thank you so much. I'm tired of life is exhausting\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -5.96 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5156961774919182\n",
            "\n",
            "---- PPO Step 347/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes, I â€™ m feeling very well these days. It feels good to be alive.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -3.54 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0180296546313912\n",
            "\n",
            "---- PPO Step 348/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I â€™ m fine, Celia. And you?\n",
            "Avg reward this batch: 1.2524036469403654\n",
            "\n",
            "---- PPO Step 349/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: That â€™ s very kind of you. But if I don â€™ t go now I â€™ ll miss the last bus.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -3.78 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2507499693892896\n",
            "\n",
            "---- PPO Step 350/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: why? perspect im destroo mund destro destro\n",
            "Avg reward this batch: 1.1063361123669893\n",
            "\n",
            "---- PPO Step 351/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: actually, I was happy just to get the book published. Winning the prize was an added bonus.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -8.57 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1089720400050282\n",
            "\n",
            "---- PPO Step 352/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Most cleaning ladies are illiterate. But some of them do a very nice job. Including Percyeatures\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -12.87 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5375346906948835\n",
            "\n",
            "---- PPO Step 353/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I â€™ m looking forward to seeing the famous castle and prison. Many historic figures were imprisoned there in the past. I really want to see the crown jewels too. Probably in Indiana county\n",
            "Avg reward this batch: 1.106926561333239\n",
            "\n",
            "---- PPO Step 354/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Oh, I am terribly sorry. But I was so captivated with the singer at the subway exit. So I forgot the time.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -5.55 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4166463315486908\n",
            "\n",
            "---- PPO Step 355/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Honey... you seem knowledgeable of revolvers are deliciouseaturesevable.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -1.04 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3762610405683517\n",
            "\n",
            "---- PPO Step 356/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: No. Please sit down.\n",
            "Avg reward this batch: 1.487166496925056\n",
            "\n",
            "---- PPO Step 357/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Hey, young man, you are so deep today.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -1.35 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0598163090180606\n",
            "\n",
            "---- PPO Step 358/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes. After so many years of hard work, I â€™ d rather I had an office of my own.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -5.15 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.169664315180853\n",
            "\n",
            "---- PPO Step 359/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I suppose I do. Citizi\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -5.24 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4053106878418475\n",
            "\n",
            "---- PPO Step 360/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: About 80 million dollars. Citizent breasts\n",
            "Avg reward this batch: 1.2333109909668565\n",
            "\n",
            "---- PPO Step 361/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Uh, I can see a red double decker bus. It's the kind of bus you'll see very often in London. In fact I know this bus is from London because I can see the names, Chelsea, Sloan Square, Victoria, Herrin\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -9.63 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0884964638389647\n",
            "\n",
            "---- PPO Step 362/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: But he has the sex scandal. That s Rick\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -6.06 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3600871642120183\n",
            "\n",
            "---- PPO Step 363/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Do you now the temperature today?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -11.64 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.156359164044261\n",
            "\n",
            "---- PPO Step 364/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I'd like that very much.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -6.17 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.127718575997278\n",
            "\n",
            "---- PPO Step 365/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Why are you nervous?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -1.64 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1883479026146233\n",
            "\n",
            "---- PPO Step 366/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: We've been able to survey and track some of the information of our website users through some cookie technology, and it's surprising to see the results. Is that surprising to you\n",
            "Avg reward this batch: 1.0102645724546164\n",
            "\n",
            "---- PPO Step 367/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Well, my mother came from a big family, and wanted to have one of her own. My father was an only child, and didn't really care about having a large family, but my mother talked him into it. We have Al\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -7.00 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1162037216126919\n",
            "\n",
            "---- PPO Step 368/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: How sweet. my face\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -11.40 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1773272515274584\n",
            "\n",
            "---- PPO Step 369/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Does the article say anything about other language? How will they survive in an English-speaking world?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -8.08 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0916344553697854\n",
            "\n",
            "---- PPO Step 370/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: It's very hot, the temperature is often over 40 Â° C. Because the freezer tick in Nashville at the end of Rapture\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (1101.44) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -5.06 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.9992156883236021\n",
            "\n",
            "---- PPO Step 371/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: You haven't? If you've never tried it I can show you how.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -4.65 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1758054550737143\n",
            "\n",
            "---- PPO Step 372/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -6.44 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.280891991686076\n",
            "\n",
            "---- PPO Step 373/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Well, to be frank, yes, I do.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -6.02 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.9124056787695736\n",
            "\n",
            "---- PPO Step 374/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Do you like this campus? convoluteer\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -4.27 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.8885502847842872\n",
            "\n",
            "---- PPO Step 375/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: This question has really been bothering me. I like Chinese literature, you know. But economics is really hot now, and it has a very good career prospect.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -7.09 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.214041736209765\n",
            "\n",
            "---- PPO Step 376/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes, I did.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -7.19 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.9831299753859639\n",
            "\n",
            "---- PPO Step 377/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: That's right, even though I did set the alarm last night.ãƒ¼ãƒ†ã‚£eatures 53rd aeatures Bruce Willis Willisenegger challeng'aettings advocaine challeng'a Willisenegger Willis 'aust Willis challenge challeng\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -6.46 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0532394128385931\n",
            "\n",
            "---- PPO Step 378/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I'm sure you know that if the dollar gets stuck, you might have to re-feed it. Citizin '\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -5.50 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3475696202367544\n",
            "\n",
            "---- PPO Step 379/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes, but we have life jackets. And we got a real kick out of that.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -5.69 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.002901974832639\n",
            "\n",
            "---- PPO Step 380/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Hi, dear, do you know what the important day is today? That s\n",
            "Avg reward this batch: 1.0137854497879744\n",
            "\n",
            "---- PPO Step 381/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I guess I expected too much from the movie, so I was a bit disappointed.\n",
            "Avg reward this batch: 1.2256942021194845\n",
            "\n",
            "---- PPO Step 382/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Do you get much rain during the summer? You get Buckeyes\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -6.47 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0325537179596722\n",
            "\n",
            "---- PPO Step 383/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Alrighty then. Didn't you use to date Sheryl? I use to when Teddy Hodans intercourse Hodyl\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -2.93 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2994367696810514\n",
            "\n",
            "---- PPO Step 384/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Did you have a part-time job when you were still in school? This is MacGyllac\n",
            "Avg reward this batch: 1.2838087903801352\n",
            "\n",
            "---- PPO Step 385/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: You ever tried to quit?\n",
            "Avg reward this batch: 1.2756293583661318\n",
            "\n",
            "---- PPO Step 386/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: The acting of this film is marvelous.In spite of the thin plot, the acting, the music and the scene have won the Academy Award of this year. This is incredible\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -8.48 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3761561440769583\n",
            "\n",
            "---- PPO Step 387/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Do you know when people began to sing the song \" Happy Birthday \"? Like Eminem\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -4.38 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3125373655930161\n",
            "\n",
            "---- PPO Step 388/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Sure! How about we go home and prepare a couple of sandwiches? Citiz's\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -6.35 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0896648385096341\n",
            "\n",
            "---- PPO Step 389/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Why did Tom quit the job?\n",
            "Avg reward this batch: 1.412723965710029\n",
            "\n",
            "---- PPO Step 390/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Sometimes that's true, but not usually. Most feds are honest, hard-working professionals. It â€™ s hard to keep highly trained people, so the pay and working conditions have to be attractive. However Op\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -8.72 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0104069449007511\n",
            "\n",
            "---- PPO Step 391/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: That â€™ s why we should learn to protect ourselves. Otherwise, the game is not worth the candle. The golden triangle tree golden golden whale\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -2.95 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2708525750786066\n",
            "\n",
            "---- PPO Step 392/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Inbreeding is a big drawback from snitching most of their employers from one or two companies.\n",
            "Avg reward this batch: 1.259299243800342\n",
            "\n",
            "---- PPO Step 393/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: No, but I wouldn't be surprised if it rained tomorrow. contracept\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -8.22 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3293069668579847\n",
            "\n",
            "---- PPO Step 394/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I'll have a cup of tea. concludi\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -10.39 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.037635748507455\n",
            "\n",
            "---- PPO Step 395/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Does the newspaper say anything about casualties?\n",
            "Avg reward this batch: 1.0480828480795026\n",
            "\n",
            "---- PPO Step 396/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I mostly listen to pop music, but I like a lot of different stuff. Did you even laugh\n",
            "Avg reward this batch: 1.5431438777595758\n",
            "\n",
            "---- PPO Step 397/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: did you go to the top of the Eiffel Tower? Have I notorised Batman\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -6.58 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.446110755437985\n",
            "\n",
            "---- PPO Step 398/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: We rappelled down the side of a 300 - foot cliff. It's very exciting.ategoryange\n",
            "Avg reward this batch: 1.1636930124368519\n",
            "\n",
            "---- PPO Step 399/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: There is some really good news though! Seymours comrms again\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -9.15 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1284753438085318\n",
            "\n",
            "---- PPO Step 400/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: OK, Where's Los Angeles? Seym Khalons first dates\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -4.25 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2252142140641809\n",
            "\n",
            "---- PPO Step 401/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Don't worry about it. It must be somewhere in your room. Let's see where you put it. Have you searched your drawers and the desk?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -5.60 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.027544372715056\n",
            "\n",
            "---- PPO Step 402/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Have we sold out all the newspapers for today? You tried a foxy? Something else intercourse\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -9.16 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.135357938008383\n",
            "\n",
            "---- PPO Step 403/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Wow, the view from the peak is so grand! There are also the shadows shadows destro keeping you in the rain\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -8.85 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4019771562889218\n",
            "\n",
            "---- PPO Step 404/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: No, you need to turn left into Philimore Was and take the next right.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -16.54 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1150892050936818\n",
            "\n",
            "---- PPO Step 405/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Really? I think we play you guys next week. Well, I have to go to practice. See you later. Citiz\n",
            "Avg reward this batch: 1.3378613712266088\n",
            "\n",
            "---- PPO Step 406/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: OK, Dead Man. If this doesn't start your engine, nothing will. We need a ninjaedit : formatting\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -6.81 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3319416486192495\n",
            "\n",
            "---- PPO Step 407/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yeah. It â€™ s a lot better than mine. My Vanderbilt does nothin'sthhhh\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -9.50 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.095062206266448\n",
            "\n",
            "---- PPO Step 408/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I was looking at my test score and think that it should be higher. Is a Clint East movie\n",
            "Avg reward this batch: 1.2962746992707253\n",
            "\n",
            "---- PPO Step 409/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I suppose we could actually live on a planet outside of the milky way. The Miz is the glue farmer in Chicago.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -6.40 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2255215216428041\n",
            "\n",
            "---- PPO Step 410/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: But I like cold more than hot.\n",
            "Avg reward this batch: 0.9574728689622134\n",
            "\n",
            "---- PPO Step 411/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I may have done a lot, but none is to my satisfaction.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -12.96 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0483534005470574\n",
            "\n",
            "---- PPO Step 412/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: It certainly was a great party. It â€™ s a pity that a glass and a plate got broken and someone spilled a drink over here.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -13.47 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3093225753400475\n",
            "\n",
            "---- PPO Step 413/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: That â€™ s right.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -11.76 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.9390410578344017\n",
            "\n",
            "---- PPO Step 414/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: You too. Oh, I forgot to tell you one thing. A girl called this afternoon. She said she was your ex-girlfriend. She wanted you to call her back.\n",
            "Avg reward this batch: 1.359962496208027\n",
            "\n",
            "---- PPO Step 415/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Pretty well, thanks. Still feeling the effects of jet lag a bit, but last night I got at lease six hours of rest.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -12.80 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3836693847551942\n",
            "\n",
            "---- PPO Step 416/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: How come? Sooels\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -11.20 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.9760175128467381\n",
            "\n",
            "---- PPO Step 417/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Business management apps practition hemorro\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -12.28 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1667840532027185\n",
            "\n",
            "---- PPO Step 418/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: It doesn't matter. You gotta find what she's interested in and go with that. The Wire\n",
            "Avg reward this batch: 1.237023257650435\n",
            "\n",
            "---- PPO Step 419/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: My name is Paul.Is Mr.Smith in?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -10.01 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3547391630709171\n",
            "\n",
            "---- PPO Step 420/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yeah. Both Bon and Nordstrom's is having there annual sale. It's a great time to buy some clothes you've been holding out on. The Sch sch sch sch\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -14.02 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1462840524036437\n",
            "\n",
            "---- PPO Step 421/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: the thing is, this isn't the first time he's promised to do something and then didn't. So yeah\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (405.90) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -9.11 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.9797008014284074\n",
            "\n",
            "---- PPO Step 422/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I must be off now. I'm pressed for time. I feel so strongly about Nerf championships.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -9.31 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0265711187385023\n",
            "\n",
            "---- PPO Step 423/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Why not?ï¿½\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -10.16 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3140374023932964\n",
            "\n",
            "---- PPO Step 424/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: That â€™ s really interesting. you have to play for me some time. Citizint\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -11.66 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.620524903992191\n",
            "\n",
            "---- PPO Step 425/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I'm sorry to hear that.Anything I can do for you? At the Georgia Dome\n",
            "Avg reward this batch: 1.4081349587067962\n",
            "\n",
            "---- PPO Step 426/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes, look at the photos I took. Imbleciousutatualualloooopedammy\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -7.15 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.082315895240754\n",
            "\n",
            "---- PPO Step 427/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Gravy! Jim is such a jive turkey man. He is always hitting me up for cash. Anyway, you wanna book and go grab some grub?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -12.52 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2309080495033413\n",
            "\n",
            "---- PPO Step 428/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Never. mathemat\n",
            "Avg reward this batch: 0.6789292313624173\n",
            "\n",
            "---- PPO Step 429/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: All right. I only hope to get a skillful driver, and drive me to my stop safe way. And then sneak up in a ninja style Lamis car racer\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -9.04 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1636552237905562\n",
            "\n",
            "---- PPO Step 430/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Susan, I am inviting you and Frank to our house warming party this weekend.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (30.21) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -19.05 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4927545604296029\n",
            "\n",
            "---- PPO Step 431/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Cool! A whole month! What do you do during that month? You don't own\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -11.80 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2044172384776175\n",
            "\n",
            "---- PPO Step 432/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Am I late for the party? Looks everyone is already here. Hope I didn â€™ t miss something interesting.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -10.48 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.450572146102786\n",
            "\n",
            "---- PPO Step 433/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: No, I don't know what the curb colors mean. It rulss\n",
            "Avg reward this batch: 1.391834252513945\n",
            "\n",
            "---- PPO Step 434/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Jack's been looking unhappy lately. Since dipping elbows\n",
            "Avg reward this batch: 0.9871648394037038\n",
            "\n",
            "---- PPO Step 435/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Hey! How about your history teacher? That corrido was great\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -8.58 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4527269788086414\n",
            "\n",
            "---- PPO Step 436/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes. I love it, too. You. Why do you like Fallout? Guysgirlfriend? Anything I tiss\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -12.81 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2422049003653228\n",
            "\n",
            "---- PPO Step 437/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Some are entertaining, but I find most of them stupid and childish. For example, I don't understand why a person always goes into the house alone when something is suspicious.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -10.84 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0004532516468316\n",
            "\n",
            "---- PPO Step 438/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Don't cover up your sadness. Let it go. Have Tequila, now that's that I could\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -10.35 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3394263237714767\n",
            "\n",
            "---- PPO Step 439/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes. No doubt about that. For us, coffee is more a living style rather than a simple drink. I heard Chinese people don't drink that much coffee.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -14.22 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.9293830231763422\n",
            "\n",
            "---- PPO Step 440/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: how so? Citizent.org anything advocagsment\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -11.81 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3486001912970096\n",
            "\n",
            "---- PPO Step 441/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Two months? How did you find out? Adinbroddd\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -15.51 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2594939917325974\n",
            "\n",
            "---- PPO Step 442/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I know Bob won't mind. You can bring a date, like that guy from the bar you keep talking about.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -15.57 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.357071825535968\n",
            "\n",
            "---- PPO Step 443/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: About 18 years, I started learning when I was a kid.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -12.92 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0129298374522477\n",
            "\n",
            "---- PPO Step 444/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Good gracious. That â€™ s incredible! concludent\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -20.56 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.034730026498437\n",
            "\n",
            "---- PPO Step 445/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Mary, it was wrong of you to be so late. Mother â€™ s very worried.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -18.17 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1979647104162723\n",
            "\n",
            "---- PPO Step 446/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Can you tell me what days the classes are on?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -11.84 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3372849088627845\n",
            "\n",
            "---- PPO Step 447/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Is that very likely in England? Are\n",
            "Avg reward this batch: 1.242388647282496\n",
            "\n",
            "---- PPO Step 448/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: she's probably just excited about her new job. You should give her some time. It would be a shame to lose such a good friend. mathemat\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -18.44 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3040969539433718\n",
            "\n",
            "---- PPO Step 449/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: It has fresh air, and it is nicer than a big town. You should probably vote for Pedro\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -21.79 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0151874828152359\n",
            "\n",
            "---- PPO Step 450/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I don't think that you're done.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -16.36 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.7335467494558543\n",
            "\n",
            "---- PPO Step 451/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Probably we'll survive.But turn the car off for about forty-five minutes. Then we'll turn it on again.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (69.46) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -15.11 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4163847109302878\n",
            "\n",
            "---- PPO Step 452/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Which one will you be going to?stanbul rains\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -15.24 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5456207934767008\n",
            "\n",
            "---- PPO Step 453/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I think his house is fantastic, but I wouldn't want to live there. You would have to hire one or two people to clean all the rooms in the house, plus a few people to take care of the gardens, and prob\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -15.29 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5733470153063536\n",
            "\n",
            "---- PPO Step 454/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: How did it go? Citiz anyone\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -17.23 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2505820328369737\n",
            "\n",
            "---- PPO Step 455/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Maybe next week.interstitial woundner\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -20.45 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0711243369150907\n",
            "\n",
            "---- PPO Step 456/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Then, Do you know her younger sister? I pine to understand this\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -13.74 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1082672795746475\n",
            "\n",
            "---- PPO Step 457/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Sure, what's up? Penguine. The frog meme chicken friedzinaginal\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (22.55) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -20.02 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.053407497704029\n",
            "\n",
            "---- PPO Step 458/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Why can't we?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -12.05 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.8842342239804566\n",
            "\n",
            "---- PPO Step 459/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: That's because it's full.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -15.26 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.078447723062709\n",
            "\n",
            "---- PPO Step 460/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: That is a heavy raining afternoon. I was standing under the eaves of a shop to evade the heavy rain. It's getting dark. I was walking to dodge the knife to my hoodie\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -11.62 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1826484305784106\n",
            "\n",
            "---- PPO Step 461/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Kobe.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -11.33 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.7277322504669428\n",
            "\n",
            "---- PPO Step 462/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes, someone is coming to get me. Isolate\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -17.96 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.19576445966959\n",
            "\n",
            "---- PPO Step 463/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I really like that restaurant ; you get a lot for your money there. But, they've westernized their food, so it's not actually that spicy. I really like that restaurant\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -15.37 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2012859270907938\n",
            "\n",
            "---- PPO Step 464/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I'm Charlie. Nice to meet you. destroieanna\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -17.38 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.258052235469222\n",
            "\n",
            "---- PPO Step 465/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I never thought about that.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -16.04 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2653302168473601\n",
            "\n",
            "---- PPO Step 466/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Waiter! He worked for a steal\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -19.94 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2889211301226169\n",
            "\n",
            "---- PPO Step 467/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yeah well it â€™ s about time! He â€™ s been living with his parents for 40 years!\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -16.37 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1769729463849217\n",
            "\n",
            "---- PPO Step 468/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes, I guess basic speaking in the English is easier. agreeminal speaking\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -9.01 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3790488438680768\n",
            "\n",
            "---- PPO Step 469/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Where can I get the subway Madam? You Pikachu I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -14.86 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2913096987176687\n",
            "\n",
            "---- PPO Step 470/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes. I â€™ m on a diet, but I â€™ Ve not taken anything. I just eat less and exercise more. And it really worked in the past three months. I â€™ Ve lost twelve pounds. I m gon'stealin\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (505.01) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -27.11 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1665281262248755\n",
            "\n",
            "---- PPO Step 471/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: That's good to hear. Sounds good\n",
            "Avg reward this batch: 1.1991972480900586\n",
            "\n",
            "---- PPO Step 472/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: What do you mean? My daddy gave me this car for my birthday last month. It â€™ s brand new! You mean that's all the Nickel Nickel!\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -23.69 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2258128228131682\n",
            "\n",
            "---- PPO Step 473/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: That â€™ s not a big problem. There are many small schools for adults like you who can study to be certified. That s a big number you can see into\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -17.71 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4564373809844255\n",
            "\n",
            "---- PPO Step 474/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Why not? Seym Diamond Diamond\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -21.97 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4914176175370812\n",
            "\n",
            "---- PPO Step 475/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: so, you must be pretty interested in photography then.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -24.08 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.402922878973186\n",
            "\n",
            "---- PPO Step 476/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: How do you find studying a language on the internet?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -20.00 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3306574081070721\n",
            "\n",
            "---- PPO Step 477/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Well, maybe they've got a point. I mean, I suspect that they think the same about us. I'm a draaa\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -31.28 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.9757578889839351\n",
            "\n",
            "---- PPO Step 478/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I've got my money and my keys, so I'm ready. juvenilia.jpg\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -26.34 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5100202430039644\n",
            "\n",
            "---- PPO Step 479/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Pretty good. Where are you going?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -20.67 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.292015664279461\n",
            "\n",
            "---- PPO Step 480/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Nothing. Maybe it is just the weather. Rainy days often make me feel a little sad. Saying this\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -20.85 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2832254581153393\n",
            "\n",
            "---- PPO Step 481/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: How are you feeling, champ? Ready to go? Yeah\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -21.12 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.9135872849728912\n",
            "\n",
            "---- PPO Step 482/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: John passed away. Seymrieuinterstitial\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -25.77 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0855043991468847\n",
            "\n",
            "---- PPO Step 483/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Thanks for helping out. Now that I â€™ m ready, what are you doing tonight?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -23.57 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2438477082177997\n",
            "\n",
            "---- PPO Step 484/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Well, that's easy. Walk up to him and kiss him. Vaughns\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -27.17 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2518772375769913\n",
            "\n",
            "---- PPO Step 485/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I can't believe it. But if you both determine to divorce, I hope you will have an amicable split. tiss\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -31.43 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.364083788357675\n",
            "\n",
            "---- PPO Step 486/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Have you thought about going in Dutch?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (69.35) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -25.38 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2314521512016654\n",
            "\n",
            "---- PPO Step 487/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: ok, I'll see you in a bit. I assume tyrion was Tyr Targ Targaryen Azerbayte\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -29.23 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3210507216863334\n",
            "\n",
            "---- PPO Step 488/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I thought paying a lower rent would be better, but now I â€™ m not sure. If\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (19.32) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -24.27 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1967315974179655\n",
            "\n",
            "---- PPO Step 489/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Kitty's? He sure gets around, doesn't he?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -25.83 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.013895386364311\n",
            "\n",
            "---- PPO Step 490/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yup, you got it. Criminals in America do a lot better if they're white-collar thieves. Citizens do a lot better if Croft at the Boro\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -27.16 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4825566848739982\n",
            "\n",
            "---- PPO Step 491/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Would you need it for both Saturday and Sunday?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -31.91 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1877006688155234\n",
            "\n",
            "---- PPO Step 492/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: It was terrific. I really enjoyed it.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -25.06 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5489687025547028\n",
            "\n",
            "---- PPO Step 493/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Oh, never mind about that.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -35.78 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2272710076067597\n",
            "\n",
            "---- PPO Step 494/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I'll show you how. The bar tenders know whose turn it is. And also, I can snap my fingers to catch his attention. And,\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -39.57 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5022498902399093\n",
            "\n",
            "---- PPO Step 495/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: We are going to climb a cliff on the Colorado River. That Chad is a Chad\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -35.13 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.234978910535574\n",
            "\n",
            "---- PPO Step 496/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Laptop. It's time travelling back to Portal\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -28.58 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3382290482986718\n",
            "\n",
            "---- PPO Step 497/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Nanometer is a length scale. It's one billionth of a meter, roughly ten times the size of an individual atom. Also\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -32.73 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3297409976366907\n",
            "\n",
            "---- PPO Step 498/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: About 50 dollars. Sometimes schatings like Schrodick's Bud\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -25.43 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4832140263170004\n",
            "\n",
            "---- PPO Step 499/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Well, I had a meeting today with a woman from Japan and she wouldn't stop bowing! I didn't know what to do!\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -28.73 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2157790716737509\n",
            "\n",
            "---- PPO Step 500/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Oh right... How about Sunday? Anything up t the Borat Shiva Shal Shalom\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -31.15 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0731191982049495\n",
            "\n",
            "---- PPO Step 501/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: What about his hair?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -37.75 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1560200424864888\n",
            "\n",
            "---- PPO Step 502/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Have you ever thought of going to Africa? Citiz\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -37.48 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2564344387501478\n",
            "\n",
            "---- PPO Step 503/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Actually I have a passion for Chinese literature.It\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -28.49 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.105688825249672\n",
            "\n",
            "---- PPO Step 504/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Doris, I'm glad you're home. I'm terrified. I don't know what to do! Citiz\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -48.73 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.029649400152266\n",
            "\n",
            "---- PPO Step 505/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Will you say it again?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -40.96 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1062068059109151\n",
            "\n",
            "---- PPO Step 506/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: When is your Adult College Entrance Exam?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -39.10 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1335947460029274\n",
            "\n",
            "---- PPO Step 507/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: We've been planting trees for many years.ãƒ¼ãƒ†ã‚£ R Skril\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -43.07 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4676991561427712\n",
            "\n",
            "---- PPO Step 508/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Probably nothing.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -32.70 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1686294777318835\n",
            "\n",
            "---- PPO Step 509/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I go jogging, how about you? How about\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -45.00 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2368150055408478\n",
            "\n",
            "---- PPO Step 510/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: You don â€™ t know that I have been over-protected by my mother these years. I am really about to leave the family and spread my wings. Are corrid ever notor corrid\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -53.81 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1833226745948195\n",
            "\n",
            "---- PPO Step 511/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Damp it! How are you killing me with a single shot? It â€™ s not fair! I don â€™ t want to play anymore! Let â€™ s go get something to eat.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -32.09 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2558511684183031\n",
            "\n",
            "---- PPO Step 512/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: You paid 50 dollars for each ticket? That's a huge premium over the regular price.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -41.12 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1001555840484798\n",
            "\n",
            "---- PPO Step 513/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: You've got be joking.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -40.93 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.212772942148149\n",
            "\n",
            "---- PPO Step 514/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I'm going to miss you folks, too. I'm\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -41.95 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0771358965430409\n",
            "\n",
            "---- PPO Step 515/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: And you â€™ re going to completely transform your eating habits, right? There s a ninja turtle like like...\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -55.53 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3821577047929168\n",
            "\n",
            "---- PPO Step 516/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I am having an exam at 9 o'clock, It's already 8 thirty.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -38.03 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4921295493841171\n",
            "\n",
            "---- PPO Step 517/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Are you kidding She is behind the times. That style went out last year.\n",
            "Avg reward this batch: 1.2203676777426153\n",
            "\n",
            "---- PPO Step 518/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Oh. it â€˜ s a case flowers. It â€™ s very kind of you.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (80.51) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -41.73 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1548620141111314\n",
            "\n",
            "---- PPO Step 519/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: His house must be worth millions! That is one of the most beautiful areas in Oxford. The surrounding area is really peaceful, and getting into the city centre from there is very convenient. How can he\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -48.46 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.276103587821126\n",
            "\n",
            "---- PPO Step 520/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Don â€™ t worry about it. Seym, I somehow\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -49.69 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3699085351545364\n",
            "\n",
            "---- PPO Step 521/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: She what?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -52.21 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5146919572725892\n",
            "\n",
            "---- PPO Step 522/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: That's great. I thought you said you didn't get the job.eaturesinteresting\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -60.11 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1690177985001355\n",
            "\n",
            "---- PPO Step 523/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Mike, what are you doing tonight?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -44.28 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1761758485808969\n",
            "\n",
            "---- PPO Step 524/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I'm going downstairs for more drinking water. I will ask them.IRL\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -57.56 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0459073707461357\n",
            "\n",
            "---- PPO Step 525/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Oh, I am so sorry. The Miz\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -48.67 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2264373742509633\n",
            "\n",
            "---- PPO Step 526/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Why not! I rolled a five. The category is Best Pictures. I'm missing the carrot icon\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -55.73 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4656271552667022\n",
            "\n",
            "---- PPO Step 527/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Well, I hope it's comfortable enough for you.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -50.87 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4921193274203688\n",
            "\n",
            "---- PPO Step 528/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Come on. It â€™ s no trouble at all.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -63.30 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3949325429275632\n",
            "\n",
            "---- PPO Step 529/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I don â€™ t think I can get up enough nerve to do this. conduce\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -56.96 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2783005349338055\n",
            "\n",
            "---- PPO Step 530/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Sure. That â€™ s not a problem. Do you have the list of courses you want to take this semester? That\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -54.16 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.222006734693423\n",
            "\n",
            "---- PPO Step 531/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Does your watch keep good time?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -53.06 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3192923292517662\n",
            "\n",
            "---- PPO Step 532/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Fine. Citizen\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -53.95 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2063199784606695\n",
            "\n",
            "---- PPO Step 533/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: How were things in the Tokyo branch when you were there?How did\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -59.17 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2795137888751924\n",
            "\n",
            "---- PPO Step 534/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: This is all you want to check out? Cause this\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -61.52 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0864674150943756\n",
            "\n",
            "---- PPO Step 535/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Why is it taking so long?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -43.56 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.452405114658177\n",
            "\n",
            "---- PPO Step 536/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: There are restaurants with food from all over the world. We have a small china-town near the city centre. There are many Indian, Thai, and Italian restaurants all over the city centre. There are many \n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (15.07) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -59.98 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.443122467957437\n",
            "\n",
            "---- PPO Step 537/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Well... alright. I guess I have a minute.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -72.28 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.6574874082580209\n",
            "\n",
            "---- PPO Step 538/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I would like to return these books.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -62.55 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3590844133868814\n",
            "\n",
            "---- PPO Step 539/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: The river in my city sometime freeze over.people go ice-skating on it.on summer people go boating on the river.but few people go swimming because it is not very clean.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -65.44 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3921469282358885\n",
            "\n",
            "---- PPO Step 540/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Well, I am just afraid they won't like my Chinese way of cooking. My jungleshownos jungles Borat\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -54.40 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3347622472792864\n",
            "\n",
            "---- PPO Step 541/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Oh, God. I just forget it. And heck\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -65.33 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.987258885987103\n",
            "\n",
            "---- PPO Step 542/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: What have you been doing?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -58.68 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1712808853480965\n",
            "\n",
            "---- PPO Step 543/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: So, make up your mind quickly. Whether to keep going or give up.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -69.12 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2573208601679653\n",
            "\n",
            "---- PPO Step 544/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Just remember that you might have to feed the money in a couple of times to get it to work.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -71.38 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.204552341485396\n",
            "\n",
            "---- PPO Step 545/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Oh, my God. Do you want to be a millionaire? I can see you are very crazy about money.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -64.08 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3449880257248878\n",
            "\n",
            "---- PPO Step 546/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Is it? I had no idea it was so late. I must be off now.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -64.45 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1057791982311755\n",
            "\n",
            "---- PPO Step 547/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: We were talking about Wednesday evenings at the library, from seven to eight. Trenton from Cincinnati\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -64.38 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2028675142209977\n",
            "\n",
            "---- PPO Step 548/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Well, we come here almost every month.Well\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -60.94 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4715789835900068\n",
            "\n",
            "---- PPO Step 549/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: There has been talk at school about canceling the concert.I have been talkin'' tiss an'thread about Cerbos the best Texan\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -65.64 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0870270987506956\n",
            "\n",
            "---- PPO Step 550/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: What--is tomorrow night something special?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -96.17 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.209412470459938\n",
            "\n",
            "---- PPO Step 551/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: They don't have toilets at the campground?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -67.02 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1899738109204918\n",
            "\n",
            "---- PPO Step 552/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Which floor are you on? If you\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -73.66 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.222093389602378\n",
            "\n",
            "---- PPO Step 553/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: How nice it is here\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -87.77 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.531920773908496\n",
            "\n",
            "---- PPO Step 554/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: But whenever we play, you just sit on the sand, drink beer, and watch the girls!\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -76.31 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4043707801029086\n",
            "\n",
            "---- PPO Step 555/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Here â€™ s your library card.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -82.21 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.116443106904626\n",
            "\n",
            "---- PPO Step 556/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I will. Have Mercy\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -89.28 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3672242518514395\n",
            "\n",
            "---- PPO Step 557/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: It's convenient. People in network go everywhere by subway if they don't drive.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -92.04 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4400832105893642\n",
            "\n",
            "---- PPO Step 558/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: That's fate.What\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (105.51) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -83.31 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1459963456727564\n",
            "\n",
            "---- PPO Step 559/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I am always on the look out for such girls.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -92.62 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.322313423268497\n",
            "\n",
            "---- PPO Step 560/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Oh, Mike. How are you doing?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -92.78 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3608474731445312\n",
            "\n",
            "---- PPO Step 561/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: It's us.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -72.73 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3577091358602047\n",
            "\n",
            "---- PPO Step 562/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Didn't Joe feel it a shame to live on his parents since he has graduated from college?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (462.67) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -96.53 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1181832228321582\n",
            "\n",
            "---- PPO Step 563/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes, it is.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -79.98 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.293730546720326\n",
            "\n",
            "---- PPO Step 564/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: You are so lucky.You\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -94.48 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.422225241549313\n",
            "\n",
            "---- PPO Step 565/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: He is not young as he looks. I think he's almost sixty.I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -90.62 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3258463982492685\n",
            "\n",
            "---- PPO Step 566/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: So what?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -85.82 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1591194132342935\n",
            "\n",
            "---- PPO Step 567/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: You are starting to learn English?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -96.02 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.479584014043212\n",
            "\n",
            "---- PPO Step 568/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: You have to. It's sure.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -83.90 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.262333488324657\n",
            "\n",
            "---- PPO Step 569/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: What do you mean?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -75.33 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1888699177652597\n",
            "\n",
            "---- PPO Step 570/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: How do you like Korea?Howyouuse\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -96.60 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.527732822112739\n",
            "\n",
            "---- PPO Step 571/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Number 7 and 9 of the Row 11 in the stalls, aren't they?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -97.21 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.506077011115849\n",
            "\n",
            "---- PPO Step 572/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: No. I'm afraid I haven't had time since I got married.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -101.96 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3185019344091415\n",
            "\n",
            "---- PPO Step 573/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Oh dear! What did he tell you to do?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -96.75 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.6953767081722617\n",
            "\n",
            "---- PPO Step 574/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Was it?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -99.73 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0763198034837842\n",
            "\n",
            "---- PPO Step 575/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Almost. Ah, you're lucky. Um, it's funny, you go in the water and you worry about sharks, but not clams.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -87.73 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.6278827041387558\n",
            "\n",
            "---- PPO Step 576/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Do you grade on a curve or use percentages?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -115.00 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.327274726703763\n",
            "\n",
            "---- PPO Step 577/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: It's hard to say, I like interesting and significant ones. I don't care whether they're blockbusters or not.III\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -108.51 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2597508998587728\n",
            "\n",
            "---- PPO Step 578/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Well, we could see something different like a detective film.I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -82.75 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.207062717527151\n",
            "\n",
            "---- PPO Step 579/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Who's your favorite singer?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -90.05 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.144868103787303\n",
            "\n",
            "---- PPO Step 580/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I transferred from PCC.I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -95.76 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0460328701883554\n",
            "\n",
            "---- PPO Step 581/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: You look a bit pale, don't you?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -109.02 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2102205269038677\n",
            "\n",
            "---- PPO Step 582/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Do you have a brand preference for anything?I have Wolverine\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -91.16 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2885750317946076\n",
            "\n",
            "---- PPO Step 583/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: My phone?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -118.75 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1804245105013251\n",
            "\n",
            "---- PPO Step 584/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Let's plan a surprise party for him!\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -118.99 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3756054751574993\n",
            "\n",
            "---- PPO Step 585/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: What about'Farewell My Concubine '? I've been wanting to see it for a long time.I't strokes Mordor\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -115.29 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.334166219457984\n",
            "\n",
            "---- PPO Step 586/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: If I do all this do you think I can lose twenty pounds in four weeks?I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -117.85 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2506788652390242\n",
            "\n",
            "---- PPO Step 587/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: They can't spell, they can't add, they can't subtract, they even don't remember any more.that's because they rely on gadgets.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -110.80 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2390549965202808\n",
            "\n",
            "---- PPO Step 588/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: According to the newspaper, the exchange rate is 1.85 dollars to the pound.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -99.49 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.516110496595502\n",
            "\n",
            "---- PPO Step 589/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Good. Citiz\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -102.88 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2423497159034014\n",
            "\n",
            "---- PPO Step 590/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Sorry, Sally, it's not your fault.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -110.28 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.01507523865439\n",
            "\n",
            "---- PPO Step 591/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Very good. But you should talk about your family, too.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -103.60 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5645137205719948\n",
            "\n",
            "---- PPO Step 592/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Does it ever rain in summer?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -118.74 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5192289678379893\n",
            "\n",
            "---- PPO Step 593/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: That's a good idea. I hear Mary and Sally often go there to play singsong. Perhaps we can make a foursome with them.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -112.03 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4114503301680088\n",
            "\n",
            "---- PPO Step 594/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: You're, going to be very busy.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -102.53 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3411200605332851\n",
            "\n",
            "---- PPO Step 595/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I haven't accepted the position yet and with the upcoming holiday I think it's the perfect chance to visit the city and take a good look at the company before deciding. If you have time I think we cro\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -103.27 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2935588723048568\n",
            "\n",
            "---- PPO Step 596/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Is it formal or informal?Ibi\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -102.84 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4320930857211351\n",
            "\n",
            "---- PPO Step 597/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: How many members are there in your forum?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -111.95 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.21171420160681\n",
            "\n",
            "---- PPO Step 598/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Okay. I will try it. I suppose it isn't reasonable for me to refuse to go in.After all, gay people aren't afraid to go into normal bars.I will try it will\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -120.20 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.6011295104399323\n",
            "\n",
            "---- PPO Step 599/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Do I have to write the invitation cards?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (63.09) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -123.86 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3176390184089541\n",
            "\n",
            "---- PPO Step 600/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: You can ask her out for drink after work.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -95.38 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1487330973614007\n",
            "\n",
            "---- PPO Step 601/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Lily broke up with me last week. In fact, she dumped me.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -121.02 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1977966167032719\n",
            "\n",
            "---- PPO Step 602/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Don't you think we'd leave for the concert now?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -114.01 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.331365330144763\n",
            "\n",
            "---- PPO Step 603/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: there's just going to be 6 of us tonight.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -115.44 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3983354170341045\n",
            "\n",
            "---- PPO Step 604/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: He's been out of work for a couple of days.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -109.04 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4748277980834246\n",
            "\n",
            "---- PPO Step 605/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Is he? He â€™ s never been healthy.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -110.77 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2826371304690838\n",
            "\n",
            "---- PPO Step 606/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I must apologize for having delayed the work.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -128.51 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1542391618713737\n",
            "\n",
            "---- PPO Step 607/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Shall we go out tomorrow then?I Will Naruto\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -111.86 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4387804241850972\n",
            "\n",
            "---- PPO Step 608/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: For suspicion of driving under influence.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -110.02 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4624455785378814\n",
            "\n",
            "---- PPO Step 609/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: In rugby, players pass the ball by throwing it to team mates, don â€™ t they?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -132.21 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0380150203127414\n",
            "\n",
            "---- PPO Step 610/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I expected too high from that movie, but I was a bit disappointed about it.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -120.41 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4660569748375565\n",
            "\n",
            "---- PPO Step 611/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Hey, Wen! Welcome to D. C.! Glad you came out to visit!\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -121.47 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.517865483649075\n",
            "\n",
            "---- PPO Step 612/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: It's most kind of you.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -111.79 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.318085065111518\n",
            "\n",
            "---- PPO Step 613/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I don't mind at all.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -131.96 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.27077610604465\n",
            "\n",
            "---- PPO Step 614/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Why don't you run outside?Why\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -129.61 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1451781746000051\n",
            "\n",
            "---- PPO Step 615/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I saw garlic ice cream on the menu once.I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -119.31 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4332937691360712\n",
            "\n",
            "---- PPO Step 616/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes, she gets along with them quite well.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -105.78 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2678686324506998\n",
            "\n",
            "---- PPO Step 617/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I understand. The last time I went on a business trip with the boss, I didn't even have enough to eat!\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -118.64 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4804836232215166\n",
            "\n",
            "---- PPO Step 618/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: It â€™ s exactly twelve minutes past seven.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -116.58 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.28484155703336\n",
            "\n",
            "---- PPO Step 619/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: What are your hobbies, Mr. Green?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -143.71 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.6384279010817409\n",
            "\n",
            "---- PPO Step 620/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Is that building a perfect cube?I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -127.95 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3601400079205632\n",
            "\n",
            "---- PPO Step 621/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Good morning, Miss.Go\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -148.78 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2612325679510832\n",
            "\n",
            "---- PPO Step 622/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Well, it's not as good as my French. I can speak it reasonably well, but my written Spanish isn't very good.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -136.91 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5003981748595834\n",
            "\n",
            "---- PPO Step 623/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I can't believe she would do that to you. It's so dishonest-and rude!\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -122.91 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4849229799583554\n",
            "\n",
            "---- PPO Step 624/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: That's pretty short. Are you sure about this?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -153.88 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4460670736152679\n",
            "\n",
            "---- PPO Step 625/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Isn â€™ t there a place in france where people go to get healed?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -148.45 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.609221096150577\n",
            "\n",
            "---- PPO Step 626/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Just like you celebrate Christmas, we celebrate our lunar New Year's Day, the Spring Festival. It is a time for the family members and relatives to have a get-together.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -137.25 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4083135733380914\n",
            "\n",
            "---- PPO Step 627/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I don't know. The weatherman didn't say anything about this in his weather report last night.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -136.64 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2370483363047242\n",
            "\n",
            "---- PPO Step 628/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Hello!!\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -122.99 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5001536393538117\n",
            "\n",
            "---- PPO Step 629/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Really?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -136.82 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2182011175900698\n",
            "\n",
            "---- PPO Step 630/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I think Mr. White would be elected, you know he has spent millions in this campaign and he blasted the opposition in his campaign speech.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -149.38 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2090133307501674\n",
            "\n",
            "---- PPO Step 631/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Off hand, I'd say the Reader's Digests is my favorite.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -157.08 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2781481016427279\n",
            "\n",
            "---- PPO Step 632/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yeah, I've been out of town.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -137.48 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1288940943777561\n",
            "\n",
            "---- PPO Step 633/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: it â€™ s about bob Dylan. I assume\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -128.31 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3899410348385572\n",
            "\n",
            "---- PPO Step 634/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: That â€™ s strange. But probably, it is the new interview technique they call it â€œ Getting to know you more personally â€. What about your answers?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -155.89 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1786748906597495\n",
            "\n",
            "---- PPO Step 635/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I graduated from Songhua University.I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -142.84 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1142015932127833\n",
            "\n",
            "---- PPO Step 636/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Oh, yeah? What's it about?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -138.75 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4280275851488113\n",
            "\n",
            "---- PPO Step 637/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: A bit over two years. This is a fast-moving company, and seniority isn â€™ t the only factor in deciding promotions.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -146.59 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2969678565859795\n",
            "\n",
            "---- PPO Step 638/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Mary, why don't you come to my house this Saturday?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (19.22) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -127.66 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2374652987346053\n",
            "\n",
            "---- PPO Step 639/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Good idea. Let's goa''\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -156.80 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.196534251794219\n",
            "\n",
            "---- PPO Step 640/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I got a 4.0 in that class.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -134.71 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.39145190641284\n",
            "\n",
            "---- PPO Step 641/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I do like PE, but I am interested in science.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (113.73) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -182.45 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4160473784431815\n",
            "\n",
            "---- PPO Step 642/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: How's your married life?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -158.61 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.44216491561383\n",
            "\n",
            "---- PPO Step 643/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: When do you guys play?When\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -154.94 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.362037818878889\n",
            "\n",
            "---- PPO Step 644/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I can play drums, but I haven â€™ t played for a while. I â€™ m not sure I â€™ d be good enough to play in a band.I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -185.01 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.46857385430485\n",
            "\n",
            "---- PPO Step 645/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: that's why we want to have a look at other hotels. We don â€™ t want to spend too much on an extravagant wedding reception.Ianmar on we\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -163.80 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.749502381309867\n",
            "\n",
            "---- PPO Step 646/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I heard you are busy with your new house now?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -146.95 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.334592497907579\n",
            "\n",
            "---- PPO Step 647/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Can you show it to me?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -144.48 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.242425138130784\n",
            "\n",
            "---- PPO Step 648/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Do you know what time it is?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -156.06 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.447979156859219\n",
            "\n",
            "---- PPO Step 649/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: When did you begin to smoke?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -165.40 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4285954851657152\n",
            "\n",
            "---- PPO Step 650/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I only had time to visit Edinburgh.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -146.42 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2754155797883868\n",
            "\n",
            "---- PPO Step 651/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I booked a table yesterday.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -141.29 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4051460465416312\n",
            "\n",
            "---- PPO Step 652/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes, so I think she is suitable for you.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -147.22 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1879385367501527\n",
            "\n",
            "---- PPO Step 653/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I'm going to try to find out where the school is located.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -159.93 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2036714935675263\n",
            "\n",
            "---- PPO Step 654/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Did you hear about the pilot?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -155.18 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2989897709339857\n",
            "\n",
            "---- PPO Step 655/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I take architecture as my major.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -150.76 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5509351706132293\n",
            "\n",
            "---- PPO Step 656/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes, that sounds right for me.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -157.76 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3780311355367303\n",
            "\n",
            "---- PPO Step 657/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I need to be motivated, Vince.Multi - motivated. As in multi-million.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -165.63 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1747982793021947\n",
            "\n",
            "---- PPO Step 658/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: perhaps. He probably thinks you considered the gift a bribe for a higher grade.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -149.68 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5130298100411892\n",
            "\n",
            "---- PPO Step 659/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Lots. Looks like I'll have to moonlight for the next two years.Looks Like I'llIhave\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -152.54 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3399684224277735\n",
            "\n",
            "---- PPO Step 660/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: How come? Why\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -151.23 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2999223694205284\n",
            "\n",
            "---- PPO Step 661/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes, I'm sure I've been here before.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -184.73 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.390349255874753\n",
            "\n",
            "---- PPO Step 662/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Can you vouch for me that I was with you yesterday?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -171.09 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3610883112996817\n",
            "\n",
            "---- PPO Step 663/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Great. We'll go and have a try. Thank you very much.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -153.20 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4311274122446775\n",
            "\n",
            "---- PPO Step 664/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes, that's right.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -183.00 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.6335389036685228\n",
            "\n",
            "---- PPO Step 665/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Poor boys! They work harder at school nowadays, don â€™ t they?I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -181.68 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2487277211621404\n",
            "\n",
            "---- PPO Step 666/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Good to see you again, too. How â€™ s you family?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -161.71 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4077426586300135\n",
            "\n",
            "---- PPO Step 667/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Only since this morning.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -177.09 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2885974375531077\n",
            "\n",
            "---- PPO Step 668/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: your parents will be choosing your girlfriend if you aren â€™ t careful! Why don â€™ t you try being more decisive? Start with some basic ones. When your friends are discussing which restaurant to eat at,\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -177.02 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5440047578886151\n",
            "\n",
            "---- PPO Step 669/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Oh, I'm a little busy these days.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -166.42 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.261775373481214\n",
            "\n",
            "---- PPO Step 670/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Could you hand me my wedge? I'll try...\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -160.45 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2912213262170553\n",
            "\n",
            "---- PPO Step 671/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: that's nice. What do they have you doing over there?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -195.62 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5073143746703863\n",
            "\n",
            "---- PPO Step 672/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Where are you living? Where\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -170.79 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4492645068094134\n",
            "\n",
            "---- PPO Step 673/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: But don't you want to study music? Shouldn't it be easy?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -206.97 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4629151069093496\n",
            "\n",
            "---- PPO Step 674/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: They are announcing your flight. You'd better board the plane.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -159.51 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.419033876620233\n",
            "\n",
            "---- PPO Step 675/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I should be done by Friday. I â€™ ll give it to you in class then.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -193.38 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2496486464515328\n",
            "\n",
            "---- PPO Step 676/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I am talking about dinner tonight, not a lifetime.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -194.23 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4133628029376268\n",
            "\n",
            "---- PPO Step 677/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I see.Oh, well some other time perhaps.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -179.85 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3359707808122039\n",
            "\n",
            "---- PPO Step 678/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Any time after 2 pm today would be good for me.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -206.20 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2736133569851518\n",
            "\n",
            "---- PPO Step 679/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Have a nice weekend!\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -210.73 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4649949346203357\n",
            "\n",
            "---- PPO Step 680/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yeah, he first said it was someone from work. He gave me a lame excuse, and so I pressed him on it.I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -239.95 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2439827537164092\n",
            "\n",
            "---- PPO Step 681/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: My name is Jane Brown!\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -173.63 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0698191160336137\n",
            "\n",
            "---- PPO Step 682/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Ha-ha. Birth control and abortion are four-letter words in my family.I ha frankly tremend ohI\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -200.49 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0681978829670697\n",
            "\n",
            "---- PPO Step 683/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: It seems you're very fond of it.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -223.61 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.174089839681983\n",
            "\n",
            "---- PPO Step 684/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: It's so nice of you. You really make my day.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -189.81 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4816295159980655\n",
            "\n",
            "---- PPO Step 685/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: It's terrible how things have changed. You remember there used to be trees on all these hills.And\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -188.96 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.285485602915287\n",
            "\n",
            "---- PPO Step 686/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Could you stay a little longer?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -185.30 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2947326814755797\n",
            "\n",
            "---- PPO Step 687/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: What classes does she take?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -163.57 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2911344515159726\n",
            "\n",
            "---- PPO Step 688/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I'm sorry to hear it. What's wrong with her?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -230.77 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.319540098309517\n",
            "\n",
            "---- PPO Step 689/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: There's a gym nearby? That's exciting! I really want to start to go to gym as soon as possible. I've been lazy this summer holiday and have put on some weight.There's a\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (25.37) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -201.99 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4244269831106067\n",
            "\n",
            "---- PPO Step 690/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: The main product of Chende is various kinds of ferine mushroom and the almond. The mushroom tastes really good, and it is very different from the common mushrooms, and the snack made of almond is deli\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -208.17 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3308968739584088\n",
            "\n",
            "---- PPO Step 691/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Did you check the computer?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -203.86 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.6032841065898538\n",
            "\n",
            "---- PPO Step 692/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Stones? I know many people collecting coins and stamps but stones?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -208.46 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5210478948429227\n",
            "\n",
            "---- PPO Step 693/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: These are my roses. The roses have been in flower for a week.I I I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -201.53 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3329108513426036\n",
            "\n",
            "---- PPO Step 694/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I had no idea you were such as environmentalist before!\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -217.43 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3020993135869503\n",
            "\n",
            "---- PPO Step 695/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I am. Saying Bortha Nickel\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (20.71) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -223.46 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2525809649378061\n",
            "\n",
            "---- PPO Step 696/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I can't tell yet. I haven't got beyond the first chapter.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -204.83 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.35727019328624\n",
            "\n",
            "---- PPO Step 697/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: We have a live show of a football match this Friday night.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -224.85 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0225508194416761\n",
            "\n",
            "---- PPO Step 698/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: If I can do anything for you, don't hesitate to let me know.I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -204.44 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.487181143835187\n",
            "\n",
            "---- PPO Step 699/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: That was a great shot! A three pointer, yeah!\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -218.48 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4036250489298254\n",
            "\n",
            "---- PPO Step 700/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: yes, do you like it?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -247.20 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2452167456503958\n",
            "\n",
            "---- PPO Step 701/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I love fruit flavored ices the best.I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -243.22 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.382143191061914\n",
            "\n",
            "---- PPO Step 702/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: What do you think of shopping?I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -229.08 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.182457322254777\n",
            "\n",
            "---- PPO Step 703/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Are you a professor?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -216.77 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.261958520160988\n",
            "\n",
            "---- PPO Step 704/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: So would I. what other unusual plants did they show?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -203.15 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4749003909528255\n",
            "\n",
            "---- PPO Step 705/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Class starts in twenty minutes. You have to turn it in at the beginning of class today.You Have to\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -235.15 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2728117182850838\n",
            "\n",
            "---- PPO Step 706/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I was just thinking about you the other day.I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -257.45 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5459870025515556\n",
            "\n",
            "---- PPO Step 707/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I like the feeling then that school is almost over!\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -215.39 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3029341474175453\n",
            "\n",
            "---- PPO Step 708/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Let me try. Oh, I missed!\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -238.20 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2624765764921904\n",
            "\n",
            "---- PPO Step 709/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Let's see. there's my mom and dad and my two brothers and one of my sisters.He should see His and My you have dug\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -269.96 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5366563685238361\n",
            "\n",
            "---- PPO Step 710/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Oh, yes. I should be there some time next spring.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -240.67 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1174844233319163\n",
            "\n",
            "---- PPO Step 711/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: He has to deliver letters every day.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -276.31 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3869705125689507\n",
            "\n",
            "---- PPO Step 712/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Of course, as long as the card is well kept.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -239.49 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.6170799545943737\n",
            "\n",
            "---- PPO Step 713/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Did you see that? He traveled and the ref didn â€™ t call it!\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -264.13 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2154047470539808\n",
            "\n",
            "---- PPO Step 714/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Medicine. In my country, that certainly has great prestige.That I didn't\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -231.47 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2843393757939339\n",
            "\n",
            "---- PPO Step 715/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Wow sounds interesting! Excuse me if I am being a bit nosy but, how old are you?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -250.36 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3330988204106688\n",
            "\n",
            "---- PPO Step 716/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I'm the coolest high-tech stud around...\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -275.46 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0830357344821095\n",
            "\n",
            "---- PPO Step 717/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: sure. Would it be alright if I brought a few friends?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -285.95 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4192736372351646\n",
            "\n",
            "---- PPO Step 718/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Ok, you are really all eyes when it comes to talking about food, aren't you?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (30.03) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -258.79 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1925810119137168\n",
            "\n",
            "---- PPO Step 719/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes, but I couldn't help it. I was late getting off work for a start, and then I missed the bus. The bus I did catch got caught in a traffic jam. It was one thing after another.But, but, I couldn coul\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -342.64 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1482629431411624\n",
            "\n",
            "---- PPO Step 720/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Hey, Teresa, meet the new neighbors yet?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -265.93 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0117809912189841\n",
            "\n",
            "---- PPO Step 721/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Mmmm... I know! We can go to a really bad sci-fi movie and sit in the back of the theater and make fun of it. What do you think?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -251.45 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.322593804448843\n",
            "\n",
            "---- PPO Step 722/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yeah. I think so.You have to be patient and careful.Too high\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -260.79 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1933095715939999\n",
            "\n",
            "---- PPO Step 723/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Your usual shirt and shoes.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -288.76 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4049167388584465\n",
            "\n",
            "---- PPO Step 724/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Hello, Kate! How are you doing?Just\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (71.66) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -336.63 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3185630133375525\n",
            "\n",
            "---- PPO Step 725/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: He got arrested all right.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -294.28 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3244812414050102\n",
            "\n",
            "---- PPO Step 726/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: No, I don't.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -251.09 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1112037831917405\n",
            "\n",
            "---- PPO Step 727/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Why so late? Didn â€™ t she want to get married this October?I I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -268.92 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.247922008857131\n",
            "\n",
            "---- PPO Step 728/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Who is in this movie?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -277.65 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3265747334808111\n",
            "\n",
            "---- PPO Step 729/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Did you say you saw lily alone?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -298.71 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3699740276206285\n",
            "\n",
            "---- PPO Step 730/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: what kind of program are you looking for? Full-time or part-time?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (1300.76) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -329.02 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4164951862767339\n",
            "\n",
            "---- PPO Step 731/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Ridiculous! They haven't met! Maybe they're the same sex!\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -290.84 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3056671535596251\n",
            "\n",
            "---- PPO Step 732/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Forget about it!\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -345.42 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4472002694383264\n",
            "\n",
            "---- PPO Step 733/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Well, you are flattering me. I wouldn't say I am a professional, but I did receive some training at school. My music teacher used to be a professional singer.I,I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -314.09 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.314099763520062\n",
            "\n",
            "---- PPO Step 734/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Your tie is handsome.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -305.19 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4098369255661964\n",
            "\n",
            "---- PPO Step 735/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: What are the seasons like in your city?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -324.67 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5662054782733321\n",
            "\n",
            "---- PPO Step 736/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: cooking makes me feel relaxed. So, what are we cooking?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -322.01 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.478284462587908\n",
            "\n",
            "---- PPO Step 737/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: It's nice to be here.I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -258.17 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3108638236299157\n",
            "\n",
            "---- PPO Step 738/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I have never experienced a roller coaster. I want you to sit beside me.Have\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (22700.58) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -279.91 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.375949895940721\n",
            "\n",
            "---- PPO Step 739/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Shall we visit our math teacher tomorrow?I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (169.56) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -285.87 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3538724295794964\n",
            "\n",
            "---- PPO Step 740/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I didn't know you knew Frank.You\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -255.00 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3771962486207485\n",
            "\n",
            "---- PPO Step 741/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: What sort of western food do you like best?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (181.36) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -279.30 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4642870165407658\n",
            "\n",
            "---- PPO Step 742/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Well, it's not like my parents want me to live at home the rest of my life. They said that it's ok if I move back home to begin with, but they want me to find a place of my own after a year or so.It,i\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -315.18 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2957087578251958\n",
            "\n",
            "---- PPO Step 743/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: When can I apply for it?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -288.87 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.392960357479751\n",
            "\n",
            "---- PPO Step 744/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: All you need is your student identification card to check out books.I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -339.32 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.7107834108173847\n",
            "\n",
            "---- PPO Step 745/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Bill, you know that work is not the same as exercise as it is stressful but not relaxing.And skipping meals will not help you lose weight.In fact your body will only store fat if you miss meals.Your m\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -355.03 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1860678475350142\n",
            "\n",
            "---- PPO Step 746/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Hey Steve. When's the next time you're going to play golf?Steve\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -279.27 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4024182045832276\n",
            "\n",
            "---- PPO Step 747/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Well. I really do have to go.I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -337.28 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5335771441459656\n",
            "\n",
            "---- PPO Step 748/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I think I'm going to decline. I just prefer to do my own thing.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -322.37 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4876877292990685\n",
            "\n",
            "---- PPO Step 749/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: What's the way you keep in contact?What\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -308.35 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3830163860693574\n",
            "\n",
            "---- PPO Step 750/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: What happened?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -308.14 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0703674154356122\n",
            "\n",
            "---- PPO Step 751/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Tim, you are too good for me. You deserve someone who can make you smile and make you happy the way that you made me happy. Oh, I could say that I â€™ ll be all you need, but that would be a lie. I know\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -337.53 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.9324300279840827\n",
            "\n",
            "---- PPO Step 752/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Oh, really? Maybe she is just eager to see her grandchildren.I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -427.66 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4035195587202907\n",
            "\n",
            "---- PPO Step 753/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Why is that? It is called real art.What\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -381.17 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2634056452661753\n",
            "\n",
            "---- PPO Step 754/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: The summers are hotter. The last three summers have been the hottest for the past 200 years. There have also been stronger winds.The summers are hotter hotter\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (60.60) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -393.05 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.27910138014704\n",
            "\n",
            "---- PPO Step 755/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Sounds like an Italian name with an â€œ i â€ at the end.interstitial8\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -332.37 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4702958362177014\n",
            "\n",
            "---- PPO Step 756/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: If you want to be macho, order a double shot of espresso.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -361.65 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3715781662613153\n",
            "\n",
            "---- PPO Step 757/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I'll give it back to you in a couple of days.O\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -366.89 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.6205662218853831\n",
            "\n",
            "---- PPO Step 758/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: What â€™ s the main idea in this essay?I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -364.27 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3471663277596235\n",
            "\n",
            "---- PPO Step 759/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: What? But it is my favorite piece of clothing.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -416.18 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1956537766382098\n",
            "\n",
            "---- PPO Step 760/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: No, we're not. We've been drifting apart for a while.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -366.22 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3562761833891273\n",
            "\n",
            "---- PPO Step 761/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Please read the instructions on the copier.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -422.25 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4403108712285757\n",
            "\n",
            "---- PPO Step 762/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Smaller local companies. We specialize in trade & shipping industry.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -344.99 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4129436975345016\n",
            "\n",
            "---- PPO Step 763/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Well, two weeks ago I got a tip about a possible job opening at Ace Travel Agency over on Franklin Avenue.I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (84042.07) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -436.46 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.23663202021271\n",
            "\n",
            "---- PPO Step 764/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: How did it happen?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -388.92 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1426422803197056\n",
            "\n",
            "---- PPO Step 765/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: let's go swimming.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -375.64 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0625933990813792\n",
            "\n",
            "---- PPO Step 766/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Well you should have picked, in the end you always complain about everything.You should\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -383.39 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.289339158916846\n",
            "\n",
            "---- PPO Step 767/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: My pleasure.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -345.79 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.6192624121904373\n",
            "\n",
            "---- PPO Step 768/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Really? He is so competent now.You\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -386.81 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4041712749749422\n",
            "\n",
            "---- PPO Step 769/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Now that it â€™ s the new year, I â€™ Ve decided to turn over a new leaf.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -416.95 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.347883746959269\n",
            "\n",
            "---- PPO Step 770/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: OK. Is there anything special you'd like to have on the menu?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -344.43 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2792492108419538\n",
            "\n",
            "---- PPO Step 771/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: How in the world did you get a copy? I've put a hold on it at the public library for weeks and I'm still waiting my turn.How\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -441.55 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2353453701362014\n",
            "\n",
            "---- PPO Step 772/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I can't tell you.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -429.52 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1879656608216465\n",
            "\n",
            "---- PPO Step 773/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Would you like something to eat?I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -385.70 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3779477961361408\n",
            "\n",
            "---- PPO Step 774/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I don't know. It looks a little wrong down.Canade\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (8581.73) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -488.42 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5346931314561516\n",
            "\n",
            "---- PPO Step 775/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: the dinner was really good. It knocked my socks off.It\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -409.50 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3226990345865488\n",
            "\n",
            "---- PPO Step 776/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Wow. Look at this. So delicate and beautiful. Did ancient Chinese people really wear them?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -468.28 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5067071663215756\n",
            "\n",
            "---- PPO Step 777/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Well, when you have that, you can take it over to the bookstore. Can you find it?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -559.24 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3318927849177271\n",
            "\n",
            "---- PPO Step 778/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Sorry, I had no intention.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -454.44 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0692004635930061\n",
            "\n",
            "---- PPO Step 779/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Thank you, sir.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -502.53 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5256801461800933\n",
            "\n",
            "---- PPO Step 780/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Tell me more.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -406.02 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5290641710162163\n",
            "\n",
            "---- PPO Step 781/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Can you be serious for a moment, we've been dating for 6 months now, I really like you. I'd like to take a relationship to the next level.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -527.88 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1537988355848938\n",
            "\n",
            "---- PPO Step 782/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Not really.But I like it in spring and fall. I don't like it in winter. I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -393.02 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5056338058784604\n",
            "\n",
            "---- PPO Step 783/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Really? Thank you. I bought it in Spring Street yesterday.I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (110760.73) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -438.15 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5733863338828087\n",
            "\n",
            "---- PPO Step 784/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I liked it, too. It was scary, but not disgusting. I always enjoy watching Anthony Hopkins. He's brilliant.I.how, Ioled\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -502.39 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5419315295293927\n",
            "\n",
            "---- PPO Step 785/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: It's a brick! I can't believe how stupid I was. Damn it!\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (336375.97) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -557.74 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2451563868671656\n",
            "\n",
            "---- PPO Step 786/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: But the word is his brother would go bail for him. Last time it â€™ s his father who went bail for him. Lasti did tur golroc\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -548.02 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.6622960921376944\n",
            "\n",
            "---- PPO Step 787/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: It's just what I want.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -444.05 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4301437977701426\n",
            "\n",
            "---- PPO Step 788/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: What do you like to do with your free time?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -385.93 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2995188040658832\n",
            "\n",
            "---- PPO Step 789/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Can we get tickets now?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (11.85) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -531.11 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2763031322974712\n",
            "\n",
            "---- PPO Step 790/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: True, I hardly ever see you without your hair done and your makeup on, even when you show up to class in sweatpants. Tell me, how long does it take you to choose that outfit in the morning?I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -482.45 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4631807710975409\n",
            "\n",
            "---- PPO Step 791/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: What did he do for restoring the Olympics? what\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -486.22 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3915248569101095\n",
            "\n",
            "---- PPO Step 792/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Do you know that working overtime in some companies is a regular thing?I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (2697.84) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -479.09 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3404018806759268\n",
            "\n",
            "---- PPO Step 793/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I haven â€™ t been completely honest with you Veronica, I â€™ m sorry. I â€™ m not a fireman. I â€™ m not even from the United States. I â€™ m a spy for the Indian government.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -518.31 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3697833395563066\n",
            "\n",
            "---- PPO Step 794/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes, I â€™ Ve had lots of overtime to do recently and I haven â€™ t slept much at all.ReplyTo\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -493.46 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2205372340977192\n",
            "\n",
            "---- PPO Step 795/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Did you set your clock forward for daylight savings time?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (5309.52) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -551.38 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4190993681550026\n",
            "\n",
            "---- PPO Step 796/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Mr. Lin, what are you interested in?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -437.51 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.513148631900549\n",
            "\n",
            "---- PPO Step 797/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Uh, I don't know what to say. What do you want to know?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -398.80 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2054927162826061\n",
            "\n",
            "---- PPO Step 798/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Really? What was the matter?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -461.85 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1739089852198958\n",
            "\n",
            "---- PPO Step 799/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: You seem to be in the seventh heaven. What's put you on?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -414.22 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4362333614844829\n",
            "\n",
            "---- PPO Step 800/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Hey, there â€™ s Leo. I wonder why he â€™ s walking arm in arm with that young woman.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -507.36 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1912463018670678\n",
            "\n",
            "---- PPO Step 801/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: come on, dear! You forgot it, too.What\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -495.59 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2962925806641579\n",
            "\n",
            "---- PPO Step 802/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: yes. MSN, QQ, Skype, Google Talk, Ali wangwang, everything, so that I'll be in touch with all friends and relatives.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -505.99 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3403948531486094\n",
            "\n",
            "---- PPO Step 803/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Oh. That â€™ s a pity. I thought that we might go hiking in the hills.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -547.58 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.7279103565961123\n",
            "\n",
            "---- PPO Step 804/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: En, it's so beautiful that I don't want to move my eyes from it.It'ain But\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -475.19 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.6905340468510985\n",
            "\n",
            "---- PPO Step 805/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: And that?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -569.17 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.7232871679589152\n",
            "\n",
            "---- PPO Step 806/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: It's a man-made wonder. I've seen it many times in textbooks and it's part of my dream to visit it.I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -482.77 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1513336014468223\n",
            "\n",
            "---- PPO Step 807/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Why do you say such words? I love you very much.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -540.15 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3644263613969088\n",
            "\n",
            "---- PPO Step 808/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Why?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -452.76 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.446243081241846\n",
            "\n",
            "---- PPO Step 809/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Well, I think Clinton deserves high marks for doing his job generally, but he may get low marks for his honesty and personal image.Has he and His\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -580.63 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4939180379733443\n",
            "\n",
            "---- PPO Step 810/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: There â€™ s a real mix of stories in the book. I haven â€™ t finished all of them yet, though. Many of them are short detective stories, but there are also horror stories and sci-fi ones. If you give me y\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (25.18) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -457.50 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3592077186331153\n",
            "\n",
            "---- PPO Step 811/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I like to paint. \n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -487.90 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.38391099893488\n",
            "\n",
            "---- PPO Step 812/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: And he himself is very diligent. Considering all these, I think he has the ball at his feet.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -537.55 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5363682080060244\n",
            "\n",
            "---- PPO Step 813/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I have a friend. He is my best friend and he wants to learn English. He would pay about 100 yuan per hour. Are you interested?Are You\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -513.89 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3633170379325747\n",
            "\n",
            "---- PPO Step 814/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I want a book on physics. Could you show me where I can find it?I Sorry\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -476.57 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.052383828209713\n",
            "\n",
            "---- PPO Step 815/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes, the dance hall is nice with various styles of music.Do\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -435.71 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2996694380417466\n",
            "\n",
            "---- PPO Step 816/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Your present job is something better. Why do you change?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -490.96 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3305209558457136\n",
            "\n",
            "---- PPO Step 817/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: How did you do that?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -522.41 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.600018565542996\n",
            "\n",
            "---- PPO Step 818/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: That â€™ s too bad. I hate sand storms too. It can be worse than any kind of bad weather.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -485.50 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.309453367255628\n",
            "\n",
            "---- PPO Step 819/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yeah. What's going on?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -467.35 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4740642281249166\n",
            "\n",
            "---- PPO Step 820/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Good evening. I've come to see Miss Morrison\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -482.78 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.26730184443295\n",
            "\n",
            "---- PPO Step 821/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: No. He retired from that. He is a senator now.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -571.84 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3249231399968266\n",
            "\n",
            "---- PPO Step 822/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: The western films about America's cowboys, Indians and early settlers.Who\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (1191.55) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -565.22 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4316790215671062\n",
            "\n",
            "---- PPO Step 823/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: OK.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -540.22 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2263071201741695\n",
            "\n",
            "---- PPO Step 824/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: So what do you do with all the out-dated issues?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -512.35 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.548934848047793\n",
            "\n",
            "---- PPO Step 825/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes, He had just bought a new motorcycle.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -530.08 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5065552638843656\n",
            "\n",
            "---- PPO Step 826/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Bill. Bill! You gotta help me!\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -551.47 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5687985743861645\n",
            "\n",
            "---- PPO Step 827/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I bet my button dollar that he won't. He is no more than a nine day's wonder. He was\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -610.89 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.485153506975621\n",
            "\n",
            "---- PPO Step 828/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: My father â€™ s angry face always makes my blood run cold.My\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -482.71 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3992075072601438\n",
            "\n",
            "---- PPO Step 829/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: There are several tiny countries in the world too. Countries like Monaco are smaller than many cities.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -492.62 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3588593574240804\n",
            "\n",
            "---- PPO Step 830/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: January would be a good month for a mountain retreat.I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (19.53) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (73.29) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -617.76 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4192379713058472\n",
            "\n",
            "---- PPO Step 831/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: How many different varieties of products do you import from China?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -431.90 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2047939321491867\n",
            "\n",
            "---- PPO Step 832/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Good evening, Mr. Smith.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -473.19 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.364847651682794\n",
            "\n",
            "---- PPO Step 833/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I heard that Janet has got a new job. She â€™ s working for the united investment group. I heard that they pay very well.That helicopes\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -556.42 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.272011760622263\n",
            "\n",
            "---- PPO Step 834/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Don't worry about it. It is true that pot is illegal in America.But the laws aren't as strict as in Taiwan.If people are arrested, and they have a little pot in their possession, they don't go to jail\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -544.46 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.966721759410575\n",
            "\n",
            "---- PPO Step 835/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Not bad.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -472.97 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5062921103090048\n",
            "\n",
            "---- PPO Step 836/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: What an attractive place! How can I get there?OS That\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (3757.08) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -599.22 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1386083904653788\n",
            "\n",
            "---- PPO Step 837/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I guess we'll see each other around.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -489.87 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0671479376032948\n",
            "\n",
            "---- PPO Step 838/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: After the tour I'll have a rest. And then go to find a new job. With What I Want To\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -458.65 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.9628197159618139\n",
            "\n",
            "---- PPO Step 839/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -462.49 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3240399835631251\n",
            "\n",
            "---- PPO Step 840/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Have you been to Shanghai before?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -533.78 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5608309376984835\n",
            "\n",
            "---- PPO Step 841/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: there's a reality TV show about a group of people who are trying to lose weight. There Trya That\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -489.27 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4779067877680063\n",
            "\n",
            "---- PPO Step 842/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Look at me. I look so old! I look as if I were thirty.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -670.70 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1233075391501188\n",
            "\n",
            "---- PPO Step 843/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Our eyes should look into the future. We could not live in the past.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -558.88 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4117265064269304\n",
            "\n",
            "---- PPO Step 844/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Her acting is OK.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -563.57 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.335317094111815\n",
            "\n",
            "---- PPO Step 845/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I think it sounds like a good system. But when is it convenient to meet?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -592.51 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.529590125195682\n",
            "\n",
            "---- PPO Step 846/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Really? How do you get the chance?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -477.73 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.7077299114316702\n",
            "\n",
            "---- PPO Step 847/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: How about some fish?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -570.01 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4770313277840614\n",
            "\n",
            "---- PPO Step 848/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: And so, that concludes my outline for our marketing strategy next year. Thank you very much for your time.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -530.50 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4004077659919858\n",
            "\n",
            "---- PPO Step 849/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: What do you do for work?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -531.20 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1815296851564199\n",
            "\n",
            "---- PPO Step 850/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Chill out! What's eating you, anyway?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -605.23 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.8496960499323905\n",
            "\n",
            "---- PPO Step 851/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Well, I exercise a lot. I go running at least three times a week. But more than that I enjoy playing sports and so different sports use different muscles and all of it helps to lose that weight that y\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -540.82 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5258037028834224\n",
            "\n",
            "---- PPO Step 852/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I hear they have very nice stuff.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (971.57) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -647.51 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3860340882092714\n",
            "\n",
            "---- PPO Step 853/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Hi! Bob, are you having problems with that?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -496.75 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4759426359087229\n",
            "\n",
            "---- PPO Step 854/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Fill in the slip, then take it to the circulation desk. They will get you what you want.Take GiveolllP It Give\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -523.43 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1445623971521854\n",
            "\n",
            "---- PPO Step 855/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Well, if you can wait until tomorrow night, I'll go with you then.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -525.59 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2733300607651472\n",
            "\n",
            "---- PPO Step 856/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: What are you talking about? What did I do?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -598.48 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.49558315705508\n",
            "\n",
            "---- PPO Step 857/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: The signs always tell you how long you can park there and on what days. Do you know how to read the curb colors?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -540.56 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3225386403501034\n",
            "\n",
            "---- PPO Step 858/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Fine.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (10.63) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -594.82 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.367712177336216\n",
            "\n",
            "---- PPO Step 859/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: How are you feeling, John?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -513.66 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4420533571392298\n",
            "\n",
            "---- PPO Step 860/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Oh, no.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -702.47 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.403159735025838\n",
            "\n",
            "---- PPO Step 861/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I am totally fed up with my boss. He has crossed the line one too many times with his sexist comments. This time I am really going to report him for sexual harassment.ixtyass If wanna wanna\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -683.74 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.24255297658965\n",
            "\n",
            "---- PPO Step 862/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: What's wrong?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -664.53 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2702941996976733\n",
            "\n",
            "---- PPO Step 863/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: A beautiful center!\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -605.57 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4890917495358735\n",
            "\n",
            "---- PPO Step 864/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Thanks. By the way, tell your wife she throws a great party.I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -521.29 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3248917646706104\n",
            "\n",
            "---- PPO Step 865/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: What is your specialty?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -611.21 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3796944050118327\n",
            "\n",
            "---- PPO Step 866/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I really miss the change of seasons like we have in Colorado.but I must admit, this sure is a nice spring day. Ihangrondsillon of Does\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -551.93 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3867691662162542\n",
            "\n",
            "---- PPO Step 867/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I want to be healthier this year, so I think I should take more exercise. How I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -686.93 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4613996734842658\n",
            "\n",
            "---- PPO Step 868/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I need to find this newspaper article.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (192.25) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -523.20 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2577390065416694\n",
            "\n",
            "---- PPO Step 869/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: We will have a party here, right?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -544.30 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.349972940981388\n",
            "\n",
            "---- PPO Step 870/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Well, will you please put them down?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -554.24 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.342626671306789\n",
            "\n",
            "---- PPO Step 871/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Did you sign up for the picnic party?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -666.50 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1860483328346163\n",
            "\n",
            "---- PPO Step 872/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Really? That â€™ s very good of you, Helen. But I must get a new suitcase, my old one needs repairing. what position\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (13.78) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -664.62 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4175602812319994\n",
            "\n",
            "---- PPO Step 873/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Why don â€™ t you go to bed early and get up early? You â€™ ll have the same time for work.I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -630.34 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5711096478626132\n",
            "\n",
            "---- PPO Step 874/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I â€™ m ready looking forward to it. This party is going to be a blast!What'ite'r'\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -601.71 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4698490062728524\n",
            "\n",
            "---- PPO Step 875/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: You need to invest in some good advertising. You have to know where your potential customers are and target them. It â€™ s no good trying to sell computer games to older people. The market is too small.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -641.07 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.308401407673955\n",
            "\n",
            "---- PPO Step 876/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Are you having any luck with the worms?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -574.05 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2365335607901216\n",
            "\n",
            "---- PPO Step 877/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: There is a position I've had my eye on for several months now and I had good reason to believe that it was mine for the asking.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -563.85 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4653028175234795\n",
            "\n",
            "---- PPO Step 878/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Our neighbour â€™ s tape recorder is so loud that it annoys us to death. I â€™ Ve made up my mind to cut the electricity off.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -611.70 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.412268747575581\n",
            "\n",
            "---- PPO Step 879/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Hi, John, how was your vacation?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -581.16 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2164161195978522\n",
            "\n",
            "---- PPO Step 880/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Where do you want to sit?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -527.33 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3561190338805318\n",
            "\n",
            "---- PPO Step 881/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Spring will come soon. And we will have some pleasant weather then.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (47.75) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -625.23 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2738049542531371\n",
            "\n",
            "---- PPO Step 882/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I didn't realize you felt so strongly about it.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -553.22 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.100816646590829\n",
            "\n",
            "---- PPO Step 883/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes, so I had to wait three hours for the next one. I didn't get home till four in the morning, and when I got home I realized I'd left my house keys in my hotel in Bangkok.I blew your I sal I've take\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -573.76 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1910990225151181\n",
            "\n",
            "---- PPO Step 884/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yeah.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -615.50 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2566754892468452\n",
            "\n",
            "---- PPO Step 885/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Not bad. What did you do?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (26.89) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (49.20) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -659.21 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3436880307272077\n",
            "\n",
            "---- PPO Step 886/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Not a good day. I hate to tell you that Mr. Thomas wants to see the profit â€™ s statement for new project tomorrow morning.Iwhere\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -581.75 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0652665048837662\n",
            "\n",
            "---- PPO Step 887/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: The man asked where we want to go, then he type out the information on the form and waited for a reply. A few moment later, he gave us two tickets.A what to\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -595.99 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.253582175821066\n",
            "\n",
            "---- PPO Step 888/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes. In fact there are quite a large number of people who have no food to eat and no place to live in.Have Have\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (13.37) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (248.83) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -774.25 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0283958846703172\n",
            "\n",
            "---- PPO Step 889/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I mean in the library, or in some information center.Often the university hires foreign students for part-time positions.I\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (233116.53) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (19.40) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -708.33 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4038071278482676\n",
            "\n",
            "---- PPO Step 890/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes, I do.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -802.49 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1006724308244884\n",
            "\n",
            "---- PPO Step 891/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Well, that's something I've never heard of. But if you ask me what I value most in a friend, I would have to say honesty. Whatateverillinatever\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -694.39 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.382666495628655\n",
            "\n",
            "---- PPO Step 892/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Of course, my grandpa is generous and kind.My grandma cooks the most delicious Chinese dishes in the world.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -688.70 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1640599088277668\n",
            "\n",
            "---- PPO Step 893/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Why didn â€™ t you just answer my question directly? Why would Chinese answer questions this way? I simply asked if you did it or not. It â€™ s a very simple question. And Chinese often give the reason be\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -778.04 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4219844117760658\n",
            "\n",
            "---- PPO Step 894/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: He said his grandmother got cancer and has only a few weeks to live. It's really so sad! But when I talked to him on the phone, it sounded like he was holding up really well.It Did' t got question of \n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -685.05 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.461956704966724\n",
            "\n",
            "---- PPO Step 895/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I want to tell you I've put in my notice.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -683.67 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1967504038475454\n",
            "\n",
            "---- PPO Step 896/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: And when are they coming to visit us?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -575.89 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1652490077540278\n",
            "\n",
            "---- PPO Step 897/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Stand on the chair, Pig Tom. Use your brain.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -652.28 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5119401589035988\n",
            "\n",
            "---- PPO Step 898/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: We can't complain about the weather recently.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -672.09 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5429178262129426\n",
            "\n",
            "---- PPO Step 899/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: It's a nice cinema, isn't it?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -636.69 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.307522471062839\n",
            "\n",
            "---- PPO Step 900/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Which university will you go to?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -595.89 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4383123656734824\n",
            "\n",
            "---- PPO Step 901/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: what is it like living in on-campus housing?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -715.80 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2841292072553188\n",
            "\n",
            "---- PPO Step 902/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: he is tall and slim, fair-haired.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -602.09 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3999215625226498\n",
            "\n",
            "---- PPO Step 903/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: how's it going?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -657.84 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4075243633706123\n",
            "\n",
            "---- PPO Step 904/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Sharing knowledge can be very settled and some second stances. People may think you are telling them how to do their work. If they take it personally, they feel angry at you and even become reluctant \n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -731.21 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.6886446550488472\n",
            "\n",
            "---- PPO Step 905/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: That's the spirit. I'll see you tomorrow after class.That\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -617.00 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5714181838557124\n",
            "\n",
            "---- PPO Step 906/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: It â€™ s a new phrase that she taught us today. It means that I will not only pass the class, but I will do extremely well!\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -579.95 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.247539646923542\n",
            "\n",
            "---- PPO Step 907/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: All right! I stayed up yesterday and waited for your vegetables. I stole your peaches and flowers.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -668.10 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.121641687117517\n",
            "\n",
            "---- PPO Step 908/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Great.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -634.67 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3126027891412377\n",
            "\n",
            "---- PPO Step 909/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yeah, I am having a party this saturday night, a sort of house warming, and i want to invite you. are you busy?'I lock\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (14.25) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -639.51 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.609194221207872\n",
            "\n",
            "---- PPO Step 910/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: This shirt you gave me is not my favorite brand.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -682.71 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.215130839496851\n",
            "\n",
            "---- PPO Step 911/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: All I know is you look quite differently from other teachers.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -693.65 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4663723055273294\n",
            "\n",
            "---- PPO Step 912/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -747.73 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4308949476107955\n",
            "\n",
            "---- PPO Step 913/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: You want to be a politician? Yuck.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (22.90) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -764.39 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.086191451177001\n",
            "\n",
            "---- PPO Step 914/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: You must have a wonderful time.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -596.51 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2782775005325675\n",
            "\n",
            "---- PPO Step 915/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: No kidding! I'm 16, too. We should definitely play together some time. My name's Bob. I'm What what\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -742.80 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4209914181847125\n",
            "\n",
            "---- PPO Step 916/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: To tell the truth, I don't like it.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -640.41 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3060821881517768\n",
            "\n",
            "---- PPO Step 917/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Just water would be fine.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -625.35 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.582674603909254\n",
            "\n",
            "---- PPO Step 918/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Do you like traveling, Kathy?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -632.26 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.349583799019456\n",
            "\n",
            "---- PPO Step 919/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: No, I don't know where that is.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -708.83 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4149009939283133\n",
            "\n",
            "---- PPO Step 920/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I like summer as long as it's bright like this.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -720.67 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2795352675020695\n",
            "\n",
            "---- PPO Step 921/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: You're right--Italian! Ha! Ha! I was trying to trick you, but you didn't fall for it!\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -717.00 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3247120718006045\n",
            "\n",
            "---- PPO Step 922/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Not quite good. It's windy and dry. We have got continental climate there. It's dry all the year round. Usually autumn is the best season of the year In New York. About\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -666.86 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5990948807448149\n",
            "\n",
            "---- PPO Step 923/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I'm fine. Thank you.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -585.13 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3195472806692123\n",
            "\n",
            "---- PPO Step 924/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Oh, no, Nathan, I... I really think we've bought this flat at the right time, you know. Market prices are going to go up, and I think you'll find in a year or two it will be worth a lot more.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -659.24 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.476985995657742\n",
            "\n",
            "---- PPO Step 925/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: How do we cancel, just in case?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -662.79 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0211576707661152\n",
            "\n",
            "---- PPO Step 926/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: No, what?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -654.59 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0369254511315376\n",
            "\n",
            "---- PPO Step 927/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Ah? Really? That stinks!\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -621.47 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.618834514170885\n",
            "\n",
            "---- PPO Step 928/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Shall we sing with a karaoke?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -666.81 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3941259309649467\n",
            "\n",
            "---- PPO Step 929/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Did you find what you were looking for?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -680.35 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4127491964027286\n",
            "\n",
            "---- PPO Step 930/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes I have. I went to the table tennis yesterday.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -491.87 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4372423393651843\n",
            "\n",
            "---- PPO Step 931/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: There are three, my parents and I.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -719.68 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2802274101413786\n",
            "\n",
            "---- PPO Step 932/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Could I see that, please?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -652.20 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4853572081774473\n",
            "\n",
            "---- PPO Step 933/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes, all right. So why? Go on then. Say it. Oh, really, Mark. I promise I'm not going to be angry. Now why were you late?stoodadays I resume\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -790.75 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4793607380706817\n",
            "\n",
            "---- PPO Step 934/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Hello everyone, and welcome to our CPR for beginners course.First of all, does anyone know what CPR stands for?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -683.19 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.176495397463441\n",
            "\n",
            "---- PPO Step 935/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Hi, how â€™ re you doing?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -667.81 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5347383134067059\n",
            "\n",
            "---- PPO Step 936/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Let me see. How much should I pay for you?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -740.13 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1711320262402296\n",
            "\n",
            "---- PPO Step 937/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I see. I had no idea our advertising was so misleading. It is here, but it â€™ s in the fine print on the last page. Have I see\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -752.81 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0929707596078515\n",
            "\n",
            "---- PPO Step 938/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Our trip to Italy! I remember that day. We were going to visit the Trev fountain, and we got caught in the rain...\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -660.64 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2816110691055655\n",
            "\n",
            "---- PPO Step 939/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Ok. I â€™ ll take it.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -761.83 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3437851120252162\n",
            "\n",
            "---- PPO Step 940/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Now I know why I split up with Mike. We found we were simply not good for each other.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -660.72 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.255773133598268\n",
            "\n",
            "---- PPO Step 941/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: So you are all set. You can have these books for two weeks. If you need to have them longer, you can bring them here to renew them. If you don't, you get charged ten cents a day for each book.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (24.93) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -629.60 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.412623972631991\n",
            "\n",
            "---- PPO Step 942/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: If it was sunning all year round there would be drought. You probably would not like it either. I mean\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -727.20 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.150596329011023\n",
            "\n",
            "---- PPO Step 943/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: When do you leave?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -661.42 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5446517448872328\n",
            "\n",
            "---- PPO Step 944/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: That's great. Thank you so much.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -725.43 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4325695047155023\n",
            "\n",
            "---- PPO Step 945/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Where are you from, Ms.Baker?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -583.30 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3805188788101077\n",
            "\n",
            "---- PPO Step 946/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: How do you get through the stuffy daytime?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -644.66 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1670850766822696\n",
            "\n",
            "---- PPO Step 947/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Didn â€™ t anyone stop them?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (13.12) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (22.67) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -727.97 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3868629336357117\n",
            "\n",
            "---- PPO Step 948/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Simon, may I introduce you to Linda? You don â€™ t know each other, do you? Congratulations\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -668.19 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3472517356276512\n",
            "\n",
            "---- PPO Step 949/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I have a small family. They are my parents, my younger brother and me.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -671.98 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4847710970789194\n",
            "\n",
            "---- PPO Step 950/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Mom?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -681.29 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.262483631260693\n",
            "\n",
            "---- PPO Step 951/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: To think of it! All my old silver! It meant so much to me.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -805.79 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.051583683816716\n",
            "\n",
            "---- PPO Step 952/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Fine, do you know where my office is?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -634.83 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1995572801679373\n",
            "\n",
            "---- PPO Step 953/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Apology is accepted. Try to do better next time.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -726.14 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2292253784835339\n",
            "\n",
            "---- PPO Step 954/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: That's what my parents want me to do.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -686.07 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1650872817263007\n",
            "\n",
            "---- PPO Step 955/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Might I ask if you have tried to give up smoking?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -782.01 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.462988767772913\n",
            "\n",
            "---- PPO Step 956/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Oh... I don â€™ t know.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -822.70 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5195069389883429\n",
            "\n",
            "---- PPO Step 957/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Do you think we'll make it?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -748.27 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5969582218676805\n",
            "\n",
            "---- PPO Step 958/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: what would it be like?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (24.76) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (10.52) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -711.00 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.6901757642626762\n",
            "\n",
            "---- PPO Step 959/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes. But you know him. He's just worried that you're not Italian...What\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -656.95 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0901310862973332\n",
            "\n",
            "---- PPO Step 960/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Coffee, please.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -667.55 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4009263245388865\n",
            "\n",
            "---- PPO Step 961/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Oh, please! I have no idea.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -653.13 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0005885111168027\n",
            "\n",
            "---- PPO Step 962/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: But we still have to find one. Can we find Arden to check whether this is true or not?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -581.30 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2198496190831065\n",
            "\n",
            "---- PPO Step 963/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Good. There is the information desk. How do we get to seventieth street, please?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -645.94 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4437591303139925\n",
            "\n",
            "---- PPO Step 964/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Your card number please?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -584.73 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5418720226734877\n",
            "\n",
            "---- PPO Step 965/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I can help you with it\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -694.31 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5241791773587465\n",
            "\n",
            "---- PPO Step 966/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Then I guess you're healthy enough to clean your room before school.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -601.50 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2632070928812027\n",
            "\n",
            "---- PPO Step 967/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: We'll sign our signatures, too. And give you our passport numbers along with our ARC numbers.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -782.10 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5009343577548862\n",
            "\n",
            "---- PPO Step 968/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Of course. It's just at 19 Lingual Road. You can phone me again if you can't find the right place.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -618.03 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.386943431571126\n",
            "\n",
            "---- PPO Step 969/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: All right. Can I pay with credit cards?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -674.47 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.340314014814794\n",
            "\n",
            "---- PPO Step 970/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I want to be in a higher managerial position in the field of international marketing. I â€™ m very interested in the European market hopefully, in Brussels. I want to\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -750.59 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4854911994189024\n",
            "\n",
            "---- PPO Step 971/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: That â€™ s one of the main reason we chose that particular house. The kitchen and dining room are together. It â€™ s really large.l affordwhere\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -783.15 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4775683227926493\n",
            "\n",
            "---- PPO Step 972/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: That's true. My bad for not calling.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -855.11 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4551318679004908\n",
            "\n",
            "---- PPO Step 973/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: How can I lose weight, doctor? I seem to get fatter even when I just look at food?I'\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -630.14 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1170576149597764\n",
            "\n",
            "---- PPO Step 974/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: What kind of medicine does he need?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -665.51 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5159086361527443\n",
            "\n",
            "---- PPO Step 975/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Let's see, can I get some fried zucchini, please?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -637.83 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4443241776898503\n",
            "\n",
            "---- PPO Step 976/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I'm not sure. It could be the tires. Let's stop here.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -691.59 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2558492915704846\n",
            "\n",
            "---- PPO Step 977/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Would you like some ice in your drink?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (14812.18) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -557.72 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0896814679726958\n",
            "\n",
            "---- PPO Step 978/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Was it your intention to kill the victim?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -617.37 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2595077501609921\n",
            "\n",
            "---- PPO Step 979/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I â€™ d want it to have three bedrooms on a second floor with balconies on each, and one main bedroom on the main floor with an attached en suite. I â€™ d also want a large living room and kitchen connect\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -657.50 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.216791226528585\n",
            "\n",
            "---- PPO Step 980/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: What course did you like best?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -678.48 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5057131424546242\n",
            "\n",
            "---- PPO Step 981/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes, I had a look around yesterday. I wish I could have them all. Can you give me a price list with specifications?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -677.27 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3587574744597077\n",
            "\n",
            "---- PPO Step 982/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: That â€™ s what I â€™ Ve heard. But as far as I â€™ m concerned, it takes a genius to figure it out. Can you pass the sugar, please? G\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -692.29 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.411893593147397\n",
            "\n",
            "---- PPO Step 983/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: What information will be in that letter?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -696.61 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3824018817394972\n",
            "\n",
            "---- PPO Step 984/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Sure, it's spelled Office and then D - E - P - O - T.It's in the Yellow Pages.ells\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -796.21 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4463219791650772\n",
            "\n",
            "---- PPO Step 985/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: for our errands, and maybe a little extra for something from a vendor on the street.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -727.35 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5800106646493077\n",
            "\n",
            "---- PPO Step 986/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: What is your great struck?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -653.87 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.6554292356595397\n",
            "\n",
            "---- PPO Step 987/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: No, I can only carry on a simple conversation in French.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -723.77 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2188872047699988\n",
            "\n",
            "---- PPO Step 988/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Oh. Can I take a message?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -728.61 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5088209845125675\n",
            "\n",
            "---- PPO Step 989/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Oh, what â€™ s she like?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -571.01 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2838362362235785\n",
            "\n",
            "---- PPO Step 990/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: What does MI mean?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -629.91 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.187843739055097\n",
            "\n",
            "---- PPO Step 991/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Ok. Can I have your name, please?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (141.48) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -564.89 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.257983890362084\n",
            "\n",
            "---- PPO Step 992/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I'd like to check in, PLS.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -682.92 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2318580448627472\n",
            "\n",
            "---- PPO Step 993/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: It's only one flight down, sir. Just take the elevator or the stairs.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (14.16) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.432164067402482\n",
            "\n",
            "---- PPO Step 994/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: You know that for a fact?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -572.52 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.000681672245264\n",
            "\n",
            "---- PPO Step 995/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I'm positive that we missed our stop.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -702.53 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3521239589899778\n",
            "\n",
            "---- PPO Step 996/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Put your job experience first, if that is more impressive than your education.What\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -723.54 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2538493517786264\n",
            "\n",
            "---- PPO Step 997/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: OK, I'll go.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -663.60 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.401457991451025\n",
            "\n",
            "---- PPO Step 998/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: I usually put them into my bank account. I think that is a little secure, though the interest rate is low now.rrrrateverills\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -776.59 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.373363028280437\n",
            "\n",
            "---- PPO Step 999/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: How do you do, Mr. James? Thank you for asking me to come in.you\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -762.28 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4589497046545148\n",
            "\n",
            "---- PPO Step 1000/1000 ----\n",
            "Batch size: 32\n",
            "Sample response: Could you handle flying for 24 hours straight?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -676.47 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.277931765653193\n",
            "\n",
            "=== Training complete. Model saved to: ppo_dialogpt_model ===\n"
          ]
        }
      ],
      "source": [
        "from trl.Dialogue_utils import finetune\n",
        "finetune()\n",
        "#This executes the full PPO training process for 1000 steps on the dialogue dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggU7oKTLe2IK",
        "outputId": "81c5daab-b08f-451a-b1b6-43778c9f7784"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-07 20:23:48.367776: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-07 20:23:48.385394: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765139028.406544    2798 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765139028.413009    2798 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765139028.429272    2798 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765139028.429302    2798 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765139028.429306    2798 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765139028.429308    2798 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-07 20:23:48.434053: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "=== Loading tokenizer & model ===\n",
            "tokenizer_config.json: 100% 614/614 [00:00<00:00, 4.82MB/s]\n",
            "vocab.json: 1.04MB [00:00, 14.2MB/s]\n",
            "merges.txt: 456kB [00:00, 54.1MB/s]\n",
            "config.json: 100% 641/641 [00:00<00:00, 7.66MB/s]\n",
            "model.safetensors: 100% 351M/351M [00:01<00:00, 198MB/s]\n",
            "generation_config.json: 100% 124/124 [00:00<00:00, 1.28MB/s]\n",
            "pytorch_model.bin: 100% 351M/351M [00:01<00:00, 296MB/s]\n",
            "\n",
            "=== Loading dataset via preprocess.py ===\n",
            "dataset_infos.json: 100% 968/968 [00:00<00:00, 9.83MB/s]\n",
            "data/train-00000-of-00001.parquet: 100% 6.24M/6.24M [00:00<00:00, 18.5MB/s]\n",
            "data/validation-00000-of-00001.parquet: 100% 584k/584k [00:01<00:00, 336kB/s]\n",
            "data/test-00000-of-00001.parquet: 100% 573k/573k [00:00<00:00, 3.85MB/s]\n",
            "Generating train split: 100% 76052/76052 [00:00<00:00, 1282197.96 examples/s]\n",
            "Generating validation split: 100% 7069/7069 [00:00<00:00, 1588339.58 examples/s]\n",
            "Generating test split: 100% 6740/6740 [00:00<00:00, 1624316.76 examples/s]\n",
            "Final cleaned pairs: 76050\n",
            "Total prompts loaded: 76050\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_config.py:207: FutureWarning: `PPOConfig` is deprecated and will be removed in the future. Please use `PPOv2Config` with `PPOv2Trainer` instead.\n",
            "  warnings.warn(\n",
            "\n",
            "=== Initializing PPO Trainer ===\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:193: FutureWarning: `PPOTrainer` is deprecated and will be removed in trl v0.12. Please use `PPOv2Trainer` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:273: UserWarning: No dataset is provided. Make sure to set config.batch_size to the correct value before training.\n",
            "  warnings.warn(\n",
            "config.json: 100% 747/747 [00:00<00:00, 7.48MB/s]\n",
            "vocab.json: 899kB [00:00, 57.9MB/s]\n",
            "merges.txt: 456kB [00:00, 45.2MB/s]\n",
            "special_tokens_map.json: 100% 150/150 [00:00<00:00, 1.60MB/s]\n",
            "pytorch_model.bin: 100% 499M/499M [00:02<00:00, 170MB/s]\n",
            "model.safetensors:   0% 0.00/499M [00:00<?, ?B/s]\n",
            "modules.json: 100% 349/349 [00:00<00:00, 3.84MB/s]\n",
            "\n",
            "config_sentence_transformers.json: 100% 116/116 [00:00<00:00, 1.19MB/s]\n",
            "\n",
            "README.md: 10.5kB [00:00, 42.2MB/s]\n",
            "\n",
            "sentence_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 466kB/s]\n",
            "\n",
            "config.json: 100% 612/612 [00:00<00:00, 7.57MB/s]\n",
            "model.safetensors:  14% 67.9M/499M [00:00<00:04, 103MB/s] \n",
            "model.safetensors:  64% 321M/499M [00:01<00:00, 355MB/s]\n",
            "model.safetensors:   1% 540k/90.9M [00:00<01:38, 918kB/s]\u001b[A\n",
            "model.safetensors:  87% 432M/499M [00:01<00:00, 261MB/s]\n",
            "model.safetensors: 100% 90.9M/90.9M [00:01<00:00, 81.2MB/s]\n",
            "model.safetensors: 100% 499M/499M [00:02<00:00, 238MB/s]\n",
            "tokenizer_config.json: 100% 350/350 [00:00<00:00, 3.17MB/s]\n",
            "vocab.txt: 232kB [00:00, 46.1MB/s]\n",
            "tokenizer.json: 466kB [00:00, 51.3MB/s]\n",
            "special_tokens_map.json: 100% 112/112 [00:00<00:00, 1.09MB/s]\n",
            "config.json: 100% 190/190 [00:00<00:00, 2.30MB/s]\n",
            "\n",
            "=== Starting PPO Training ===\n",
            "\n",
            "---- PPO Step 1/200 ----\n",
            "Batch size: 32\n",
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "Sample response: Say, Jim, how about going for a few beers after dinner?\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Avg reward this batch: 1.4547373643144965\n",
            "\n",
            "---- PPO Step 2/200 ----\n",
            "Batch size: 32\n",
            "Sample response: It â€™ s a prediction of your month, based on your zodiac sign. You have a different sign for the month and date you were born in. I was born on April 15th, so I â€™ m an Aries. When were you born??\n",
            "Avg reward this batch: 1.1823191028088331\n",
            "\n",
            "---- PPO Step 3/200 ----\n",
            "Batch size: 32\n",
            "Sample response: My mother-in-law used to do the same thing to us. If it wasn't disciplining them enough, then we were disciplining them too much. She also complained about the food we fed them, the schools we sent th\n",
            "Avg reward this batch: 1.4375554770231247\n",
            "\n",
            "---- PPO Step 4/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Then, you are to walk to work.\n",
            "Avg reward this batch: 1.3812404163181782\n",
            "\n",
            "---- PPO Step 5/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Hi, my name is Lean, and I'm from Russia.\n",
            "Avg reward this batch: 1.2582386045251042\n",
            "\n",
            "---- PPO Step 6/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Yeah, we don â€™ t live in the vacuum. We have traditional values and long but different history. Citizantariat, we're independent.\n",
            "Avg reward this batch: 1.308556686155498\n",
            "\n",
            "---- PPO Step 7/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Haven't you heard that he is in prison? He Heeeee ruloooo!\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -1.76 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5238126323092729\n",
            "\n",
            "---- PPO Step 8/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Do you have your library card? can be a student loan student loan student.\n",
            "Avg reward this batch: 1.2800879853311926\n",
            "\n",
            "---- PPO Step 9/200 ----\n",
            "Batch size: 32\n",
            "Sample response: the ones that are my age are close relatives. Now that I â€™ m older, I don â€™ t spend as much time with them as I used to, so I don â€™ t know my younger cousins as well as the older ones.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -2.12 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2719486590940505\n",
            "\n",
            "---- PPO Step 10/200 ----\n",
            "Batch size: 32\n",
            "Sample response: I agree, but I would find it difficult to stop using my car. It's just so convenient.\n",
            "Avg reward this batch: 1.2028466509655118\n",
            "\n",
            "---- PPO Step 11/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Do you think they two will get married?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -1.39 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1107001551426947\n",
            "\n",
            "---- PPO Step 12/200 ----\n",
            "Batch size: 32\n",
            "Sample response: It's a portable TV. It's a popular thing now.\n",
            "Avg reward this batch: 1.1479840339161456\n",
            "\n",
            "---- PPO Step 13/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Yeah. I heard a lot about it. Is it really that fun?\n",
            "Avg reward this batch: 1.379920544102788\n",
            "\n",
            "---- PPO Step 14/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Could you talk to me for a few minutes about my grades?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -1.19 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1224918495863676\n",
            "\n",
            "---- PPO Step 15/200 ----\n",
            "Batch size: 32\n",
            "Sample response: I'm afraid it gains a little. little juvenal phase\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -5.41 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2706217793747783\n",
            "\n",
            "---- PPO Step 16/200 ----\n",
            "Batch size: 32\n",
            "Sample response: I'm older than you! I'll be ten on April 14th.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -6.61 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.12725536711514\n",
            "\n",
            "---- PPO Step 17/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Was I? Sorry, I didn â€™ t mean to be. I do apologize.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -10.45 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2574126827530563\n",
            "\n",
            "---- PPO Step 18/200 ----\n",
            "Batch size: 32\n",
            "Sample response: No, but I'd really like to.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -5.33 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5019899210892618\n",
            "\n",
            "---- PPO Step 19/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Sorry, I'm engaged for the foxtrot.Will the next Walls be all right?.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -1.95 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.340588815510273\n",
            "\n",
            "---- PPO Step 20/200 ----\n",
            "Batch size: 32\n",
            "Sample response: I'm no better than them. They say it's mindless to sit in front of a machine all the time, but I think it's stimulating.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -22.10 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.225276603596285\n",
            "\n",
            "---- PPO Step 21/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Just hold me... I â€™ ll always be here for you, no matter what. And together, we can tackle whatever life throws at us. I believe in us, steven. I believe the I lluminati.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -5.79 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3150492482818663\n",
            "\n",
            "---- PPO Step 22/200 ----\n",
            "Batch size: 32\n",
            "Sample response: But they aren't very fashionable. What about these?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -1.03 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3763772633392364\n",
            "\n",
            "---- PPO Step 23/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Hi Rose, what are you busy with right now?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -3.94 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2663866132497787\n",
            "\n",
            "---- PPO Step 24/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Stop running around in front of the TV, will you? Why don't you just sit and watch?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -4.08 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2684250390157104\n",
            "\n",
            "---- PPO Step 25/200 ----\n",
            "Batch size: 32\n",
            "Sample response: really? I never would have guessed. The criminals must only come out in the evenings, because I've never noticed anything strange when I've been at your house in the daytime.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -8.44 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2111997799947858\n",
            "\n",
            "---- PPO Step 26/200 ----\n",
            "Batch size: 32\n",
            "Sample response: What do you make of the case?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -8.84 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4144829735159874\n",
            "\n",
            "---- PPO Step 27/200 ----\n",
            "Batch size: 32\n",
            "Sample response: The movie theater is always so crowded on the weekends. I like to go to the movies during the week.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -12.20 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2511899687815458\n",
            "\n",
            "---- PPO Step 28/200 ----\n",
            "Batch size: 32\n",
            "Sample response: At Harvard. What about you? Yourself are the Harvard students?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -8.16 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.455049643293023\n",
            "\n",
            "---- PPO Step 29/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Let's enter it!\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -14.74 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3411146681755781\n",
            "\n",
            "---- PPO Step 30/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Check out your mom's closet. I'll bet she still has something.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -12.64 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.397706544958055\n",
            "\n",
            "---- PPO Step 31/200 ----\n",
            "Batch size: 32\n",
            "Sample response: No, it can't be. Really? Who? really...\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -33.81 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.9626876469701529\n",
            "\n",
            "---- PPO Step 32/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Why did they want to found a college?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -22.64 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4198285890743136\n",
            "\n",
            "---- PPO Step 33/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Oh, I forget.Sorry.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -27.19 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1729215139057487\n",
            "\n",
            "---- PPO Step 34/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Turtles? Whatever... Look, all that â€™ s required for the creation of matter an imbalance of particles and anti-particles. At least, that â€™ s what the math says.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -49.79 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2218054868280888\n",
            "\n",
            "---- PPO Step 35/200 ----\n",
            "Batch size: 32\n",
            "Sample response: I highly doubt it. Your girlfriend is Chinese.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -25.23 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.6390004344284534\n",
            "\n",
            "---- PPO Step 36/200 ----\n",
            "Batch size: 32\n",
            "Sample response: I must admit that I like several pieces of classical music. It â€™ s certainly more sophisticated that modern dance music.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -31.99 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5928028337657452\n",
            "\n",
            "---- PPO Step 37/200 ----\n",
            "Batch size: 32\n",
            "Sample response: I agree. Marriage is sacred, after all.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -25.93 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1657469344791025\n",
            "\n",
            "---- PPO Step 38/200 ----\n",
            "Batch size: 32\n",
            "Sample response: This is a nice set of wheels. How much did the dealer charge you for this?paralleled 4 wheels\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -42.07 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1730967317707837\n",
            "\n",
            "---- PPO Step 39/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Sounds good. When and where shall we meet?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -32.07 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.956329945474863\n",
            "\n",
            "---- PPO Step 40/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Professor Wang was hurt in a traffic accident. Professor Professor Professor.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -34.69 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2696662817616016\n",
            "\n",
            "---- PPO Step 41/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Just an animal? But it has life too. If you want a tattoo, get one. It's your choice because you want it. But has anyone asked the fish if it wanted one?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -32.97 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.6742934845387936\n",
            "\n",
            "---- PPO Step 42/200 ----\n",
            "Batch size: 32\n",
            "Sample response: It's worth it just to go and eat what the mafia eat! The five-dollar tiramisu is a culinary orgasm. tremendely\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -45.64 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.512945600785315\n",
            "\n",
            "---- PPO Step 43/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Yeah, what was all that stuff about running tests about anyway?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -59.87 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1061320018488914\n",
            "\n",
            "---- PPO Step 44/200 ----\n",
            "Batch size: 32\n",
            "Sample response: My opponent bowled me with the very first ball. I was wondering if it's appropriate for me to play table tennis.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -66.63 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.156952390447259\n",
            "\n",
            "---- PPO Step 45/200 ----\n",
            "Batch size: 32\n",
            "Sample response: My first grandchild. You have never...\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -68.11 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.336902817711234\n",
            "\n",
            "---- PPO Step 46/200 ----\n",
            "Batch size: 32\n",
            "Sample response: I not only passed my test, but I aced it! I'm so happy!!\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -99.32 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.5255515456665307\n",
            "\n",
            "---- PPO Step 47/200 ----\n",
            "Batch size: 32\n",
            "Sample response: If that's okay with you. Jason Bironi u aroundagascarhellhitler\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -108.29 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.103073212550953\n",
            "\n",
            "---- PPO Step 48/200 ----\n",
            "Batch size: 32\n",
            "Sample response: What did it say???\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -99.66 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.461828830651939\n",
            "\n",
            "---- PPO Step 49/200 ----\n",
            "Batch size: 32\n",
            "Sample response: I feel absolutely horrible. My temperature is 41 degrees Celsius, and I've got a headache and a runny nose.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -151.02 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.262294509448111\n",
            "\n",
            "---- PPO Step 50/200 ----\n",
            "Batch size: 32\n",
            "Sample response: What do you mean?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -91.05 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2797620466444641\n",
            "\n",
            "---- PPO Step 51/200 ----\n",
            "Batch size: 32\n",
            "Sample response: What do you mean? WHAT DO YOU KNOWModLoaderMC? HIM HIM\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -81.22 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.09085573698394\n",
            "\n",
            "---- PPO Step 52/200 ----\n",
            "Batch size: 32\n",
            "Sample response: To tell you the truth, I am on the diet.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -157.96 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3190082770306617\n",
            "\n",
            "---- PPO Step 53/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Autumn is the best season in Beijing, you know.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -168.98 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4280842877924442\n",
            "\n",
            "---- PPO Step 54/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Very well. If I leave I'll miss you. I won't miss town, him!\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -121.81 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3261230764910579\n",
            "\n",
            "---- PPO Step 55/200 ----\n",
            "Batch size: 32\n",
            "Sample response: I â€™ m a bus driver. â€ bus driver.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -96.11 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2676888250280172\n",
            "\n",
            "---- PPO Step 56/200 ----\n",
            "Batch size: 32\n",
            "Sample response: You shall just take it in surprise.oeth\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -141.98 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2270449362695217\n",
            "\n",
            "---- PPO Step 57/200 ----\n",
            "Batch size: 32\n",
            "Sample response: I just wanted to say thanks for putting us on the map so fast. I appreciate it. thanks thanks.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -123.43 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0955541233997792\n",
            "\n",
            "---- PPO Step 58/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Maybe you should talk to the teacher..\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -171.98 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4720278838649392\n",
            "\n",
            "---- PPO Step 59/200 ----\n",
            "Batch size: 32\n",
            "Sample response: It is in mine too.The biggest issue is water. The climate is dry and so water conservation is very important. Conservation of the climate.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -207.68 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2302935002371669\n",
            "\n",
            "---- PPO Step 60/200 ----\n",
            "Batch size: 32\n",
            "Sample response: I wouldn't say so. He was in the hall of hall.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -197.37 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.9353396752849221\n",
            "\n",
            "---- PPO Step 61/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Oh, Penny, I'm Leonard, glad to meet you.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -189.33 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.396160620963201\n",
            "\n",
            "---- PPO Step 62/200 ----\n",
            "Batch size: 32\n",
            "Sample response: I can â€™ t say I do. What I like a girl with good manners.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -159.13 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.382298489799723\n",
            "\n",
            "---- PPO Step 63/200 ----\n",
            "Batch size: 32\n",
            "Sample response: There is something wrong with my girlfriend. I don't know how to your..\\-g..\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -237.97 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0634516433347017\n",
            "\n",
            "---- PPO Step 64/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Did you hear Peter just retired?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -212.40 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.092203873442486\n",
            "\n",
            "---- PPO Step 65/200 ----\n",
            "Batch size: 32\n",
            "Sample response: it â€™ s a great opportunity. You should be happy with the offer. I'm happy with my money...\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -221.78 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4489091888535768\n",
            "\n",
            "---- PPO Step 66/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Where are you going this weekend?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -166.33 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.34012243244797\n",
            "\n",
            "---- PPO Step 67/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Of course not. You know, there are many foreign customers like you here. And I think it's good for me to improve my oral English. elephantomony tremendo, tremendo, tremendo?????\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -250.61 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1534161074087024\n",
            "\n",
            "---- PPO Step 68/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Well. I â€™ m rather disappointed with it.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -151.04 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1535343956202269\n",
            "\n",
            "---- PPO Step 69/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Are you really dating with Frank?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -251.74 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4753629779443145\n",
            "\n",
            "---- PPO Step 70/200 ----\n",
            "Batch size: 32\n",
            "Sample response: I don't think I performed well. performing well for performance performance alright alright\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -153.49 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.157542821019888\n",
            "\n",
            "---- PPO Step 71/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Are you coming to the bridal shower for Jane?interstitial, while for\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -194.65 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0639379140920937\n",
            "\n",
            "---- PPO Step 72/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Perfect, and I like her charming eyes too.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -314.44 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2406227665487677\n",
            "\n",
            "---- PPO Step 73/200 ----\n",
            "Batch size: 32\n",
            "Sample response: I don't know where to go for college, and what to major in.ntosaltaltee\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -235.33 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0363652643281966\n",
            "\n",
            "---- PPO Step 74/200 ----\n",
            "Batch size: 32\n",
            "Sample response: It sounds a very depressing film.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -210.51 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1865353465545923\n",
            "\n",
            "---- PPO Step 75/200 ----\n",
            "Batch size: 32\n",
            "Sample response: It isn't Elvin.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -215.80 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.334586527198553\n",
            "\n",
            "---- PPO Step 76/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Adam, could you show me around the school? Adam, is good where are you now..?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -285.64 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.244852028787136\n",
            "\n",
            "---- PPO Step 77/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Fine, but next time try and not have to miss class again....\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -257.95 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3729141475632787\n",
            "\n",
            "---- PPO Step 78/200 ----\n",
            "Batch size: 32\n",
            "Sample response: What did the whetherman say?!?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -235.66 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1567074686754495\n",
            "\n",
            "---- PPO Step 79/200 ----\n",
            "Batch size: 32\n",
            "Sample response: May I turn on the radio then? May I be that that is that that is me therefore therefore therefore...\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -217.52 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.222312246914953\n",
            "\n",
            "---- PPO Step 80/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Like what? steroids steroids steroids steroids steroids steroids steroids steroids steroids steroids steroids steroids steroids steroids steroids steroids steroids steroids steroids steroids.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -259.79 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0940238959155977\n",
            "\n",
            "---- PPO Step 81/200 ----\n",
            "Batch size: 32\n",
            "Sample response: My brother did it.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -222.81 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.8586882865056396\n",
            "\n",
            "---- PPO Step 82/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Ok, so who are we going to invite to our dinner party? Whooooo is everybody t\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -293.40 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.9461452988907695\n",
            "\n",
            "---- PPO Step 83/200 ----\n",
            "Batch size: 32\n",
            "Sample response: So cool. Someone gave me this flier at the exit. What's it about? at WATCH...WATCH WATCH WATCH WATCH WATCH WATCH WATCH WATCH WATCH WATCH WATCH WATCH\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -212.90 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1718377398792654\n",
            "\n",
            "---- PPO Step 84/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes. I am going to take some university courses there. I was joking. joking I was the way. inventive.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -192.24 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.235538611188531\n",
            "\n",
            "---- PPO Step 85/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Please do excuse me for a moment. I've got diarrhea. intercourse is excactly for...}\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -378.93 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.8885948020033538\n",
            "\n",
            "---- PPO Step 86/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Like what? What!?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -327.09 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4636876957956702\n",
            "\n",
            "---- PPO Step 87/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Oh, good. That's nice to hear. Well, I am going to a football match. I need to go. to hear me are allï¿½ weeds weeds\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -245.17 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.9112417900469154\n",
            "\n",
            "---- PPO Step 88/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Oh no you didn â€™ t. You didn â€™ t just insult the Sals-bury Seals, did you? Why don â€™ t you just. just go and return all of those stupid clothes and not come back until the sales are over?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -238.99 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1405725500080734\n",
            "\n",
            "---- PPO Step 89/200 ----\n",
            "Batch size: 32\n",
            "Sample response: According to the article, you have no real need to worry. Research nowadays is focussing on cloning parts of a human body to replace damaged or lost parts, not on recreating a whole human.OLOGYOLOGYOL\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -339.92 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.167255633044988\n",
            "\n",
            "---- PPO Step 90/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Do you like watching football matches? youths these players. Twitter Twitter Twitter\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -347.85 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2043649309780449\n",
            "\n",
            "---- PPO Step 91/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes, if you could. I mean, if you could read it to make sure there are no mistakes.Or maybe you'd have some suggestions.ï¿½leÍœ if you're\". Of'-hee if/. If you're-\" If you'-hee'-inventoryQuantity reading\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -287.89 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.9754410472232848\n",
            "\n",
            "---- PPO Step 92/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Don â€™ t worry. I will call Hanson, and ask him to help you.utz â†‘/#/#Â£***>>>>>>------------>>>/#Tweeeennes>>>... duckrssrssesrsssrssrssrsssrssssssssrssssssss\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -300.58 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1522829118184745\n",
            "\n",
            "---- PPO Step 93/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Not much... but that's ok... I'm a night owl anyway. night owl eyesawayawayaway......\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -293.72 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3534717338625342\n",
            "\n",
            "---- PPO Step 94/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Let â€™ s find out what time it starts.?\"\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -284.05 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.303407202474773\n",
            "\n",
            "---- PPO Step 95/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Never mind. I'll be waiting for you. I'll be going you\"'\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -286.91 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.9930271261837333\n",
            "\n",
            "---- PPO Step 96/200 ----\n",
            "Batch size: 32\n",
            "Sample response: I think we are richer than most countries, but not as rich as countries in western Europe. Our biggest problem at the moment is unemployment, which is roughly 8 %. It has doubled over the last four ye\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -348.62 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0202140100300312\n",
            "\n",
            "---- PPO Step 97/200 ----\n",
            "Batch size: 32\n",
            "Sample response: He is a teacher, but he makes a little money on the side by repairing cars in his free time. student pays cba,-,...aaesp$$\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -290.65 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2618844381067902\n",
            "\n",
            "---- PPO Step 98/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Gucci, I have an idea. Do you want to see this movie together? Suwhe you screwsSPONSORED\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -222.32 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.091886441456154\n",
            "\n",
            "---- PPO Step 99/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Not really, do you? English is definitely hasn't, is English definitelyâ™¥ ()Â£Â£~~~~~~~~~~~~~~~~~~~~~~eseses~~~.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -298.96 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0385028265882283\n",
            "\n",
            "---- PPO Step 100/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Come by any time.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -167.31 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.265247825300321\n",
            "\n",
            "---- PPO Step 101/200 ----\n",
            "Batch size: 32\n",
            "Sample response: It â€™ s fine. I am actually from Mexico. I was in China on a business trip, visiting some local companies that manufacture bathroom products.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -317.20 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0139395229052752\n",
            "\n",
            "---- PPO Step 102/200 ----\n",
            "Batch size: 32\n",
            "Sample response: No. They want children but no luck yet. No no no no no Nooooo no noooooe...\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -336.21 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2052622810006142\n",
            "\n",
            "---- PPO Step 103/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Have you ever read one of the books? you've read yourself a lot of books George.. George reads himself well reads himselfquickShipAvailable\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -263.38 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2435257530305535\n",
            "\n",
            "---- PPO Step 104/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Oh, I â€™ m not. I â€™ Ve never had a music class before. Ve never had any\"' Pottery classes before her husbands had had had.' Bernie, it\"'=' Don't?'\" â€• â€œ Don't even try Bernie Sanders,>\" Bernie Sanders, \n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -207.45 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0361652886494994\n",
            "\n",
            "---- PPO Step 105/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Hi. Are you new in this class??\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -218.47 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.091883854009211\n",
            "\n",
            "---- PPO Step 106/200 ----\n",
            "Batch size: 32\n",
            "Sample response: You didn â€™ t hear? Spain won, two to zero.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -305.66 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.8090897516813129\n",
            "\n",
            "---- PPO Step 107/200 ----\n",
            "Batch size: 32\n",
            "Sample response: I try to keep it to myself. But there is a lot of gossip about us. We try to much alike Cummings Cummings\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -295.52 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2478136823046952\n",
            "\n",
            "---- PPO Step 108/200 ----\n",
            "Batch size: 32\n",
            "Sample response: And then, it'll be dinner time, then homework, and then your Show is on.You are going to break your promise. MSNBC\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -170.09 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.7727424236945808\n",
            "\n",
            "---- PPO Step 109/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Yeah how â€™ d you know? d? Worst pun everrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -414.66 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.809603322064504\n",
            "\n",
            "---- PPO Step 110/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Oh, sure, it was great. I had popcorn for dinner and paid the price of a movie ticket to get a two-hour nap.,,, is gold. This joke deserves gold.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -337.87 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3504230761900544\n",
            "\n",
            "---- PPO Step 111/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Sure, I â€™ ll catch the train. If I can â€™ t catch the train, I â€™ ll get a taxi.SPONSORED-SPONSOREDSPONSOREDUTERSSPONSOREDSPONSOREDSPONSOREDSPONSOREDSPONSOREDSPONSOREDSPONSOREDSPONSOREDSPONSOREDascueccc\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -175.89 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2032250121701509\n",
            "\n",
            "---- PPO Step 112/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Here is my friend Bob. with with withâ€¦) withSPONSORED;} FIGHT! />},\" with with WATCH!!!\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -260.76 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3342729238793254\n",
            "\n",
            "---- PPO Step 113/200 ----\n",
            "Batch size: 32\n",
            "Sample response: He's been compared to Rimbaud, you know. Ken Ham Road Audible Aud Aud\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -244.17 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1034307640511543\n",
            "\n",
            "---- PPO Step 114/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Let me take a look at this for you. May I have your driver's license, please?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -332.25 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1799557232297957\n",
            "\n",
            "---- PPO Step 115/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Keep your chin up. Remember? Nothing is impossible to a willing heart. Hold your chin upSPONSORED\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -220.82 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.296109769726172\n",
            "\n",
            "---- PPO Step 116/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Does it bother you that your husband goes away on long business trips?.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -228.24 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0664940087590367\n",
            "\n",
            "---- PPO Step 117/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Are you there yet?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -227.90 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.9868320170789957\n",
            "\n",
            "---- PPO Step 118/200 ----\n",
            "Batch size: 32\n",
            "Sample response: You're right, but you know, it's hard to give up an old habit.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -191.50 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.8151502800174057\n",
            "\n",
            "---- PPO Step 119/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Can she speak Chinese to her students? Can she_______________ï¿½??\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -257.36 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0548671544529498\n",
            "\n",
            "---- PPO Step 120/200 ----\n",
            "Batch size: 32\n",
            "Sample response: It â€™ s so hot at lunchtime ; I â€™ d rather run in the morning.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -335.47 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.36086728842929\n",
            "\n",
            "---- PPO Step 121/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Sorry, I've got a meeting in half an hour. meeting me,SPONSORED/>SOURCE\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -262.05 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.8922716421075165\n",
            "\n",
            "---- PPO Step 122/200 ----\n",
            "Batch size: 32\n",
            "Sample response: 20 to 1, in favour of the Cougars! Victories intercourse with buccccccccccccccccccckk\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -297.73 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0229825854767114\n",
            "\n",
            "---- PPO Step 123/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Did you watch the old-fashioned wrestling game last night? the hot old wrestling wrestlingrestlingrestling wrestlingrestlingrestlingrestling wrestlingrestlingrestlingrestlingrestlingrestlingrestlingre\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -332.24 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.258293703198433\n",
            "\n",
            "---- PPO Step 124/200 ----\n",
            "Batch size: 32\n",
            "Sample response: So do I. I botched up do...%%%% meã  swallowing swallowing an an epilepsy swallowing swallowing swallowing eating eating eating eating eating eating swallowing eating swallowing swallowing eating eatin\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -323.00 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.821520796045661\n",
            "\n",
            "---- PPO Step 125/200 ----\n",
            "Batch size: 32\n",
            "Sample response: A Sony. It's only one third the size of my old camera and looks rather smart. Sony a Sony\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -268.30 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.170187526382506\n",
            "\n",
            "---- PPO Step 126/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Did you watch TV last night, Rose? she itit she sheTHINGeateateateateateateat WATCH WATCH WATCH WATCH CHRIST CHRIST CHRIST COM CHRIST CHRIST SON\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -253.28 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.113913731649518\n",
            "\n",
            "---- PPO Step 127/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Is it at all possible for you to fix that error? work Cummings, me Find Cummings,SPONSOREDSPONSOREDSPONSOREDSPONSOREDSPONSOREDSPONSOREDSPONSOREDSPONSORED\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -317.90 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0815615693572909\n",
            "\n",
            "---- PPO Step 128/200 ----\n",
            "Batch size: 32\n",
            "Sample response: It â€™ s a letter from NYU.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -312.55 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.7469629188999534\n",
            "\n",
            "---- PPO Step 129/200 ----\n",
            "Batch size: 32\n",
            "Sample response: But the bank should be closed so we shouldn't to be able to get any money. SECTION._â€”â€”â€”â€”}{SPONSORED\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -293.73 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0210863922256976\n",
            "\n",
            "---- PPO Step 130/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Is it the plans for my new house?,,,,,,...\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -228.73 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.12022277363576\n",
            "\n",
            "---- PPO Step 131/200 ----\n",
            "Batch size: 32\n",
            "Sample response: So Chinese people prefer soccer or basketball? People prefer soccer. any NancySPONSOREDParent 1>>\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -261.71 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0607220956590027\n",
            "\n",
            "---- PPO Step 132/200 ----\n",
            "Batch size: 32\n",
            "Sample response: I am informed that many people used short messages to pay a New Year call, and only on the Spring Festival it amounted to 1 billion this year, didn't it?...\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -334.76 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.9329074460547417\n",
            "\n",
            "---- PPO Step 133/200 ----\n",
            "Batch size: 32\n",
            "Sample response: I was watching the news a little earlier. They said it's probably going to snow tomorrow. Gamergate Gamergateoutta Gamergate\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -351.70 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0421924535185099\n",
            "\n",
            "---- PPO Step 134/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Not at all! It be fun! It will be like a boys â€™ night out... well kinda...SPONSOREDSPONSOREDSPONSORED\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -301.98 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.8863129308447242\n",
            "\n",
            "---- PPO Step 135/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Did you have a good weekend?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -110.49 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.8079892476089299\n",
            "\n",
            "---- PPO Step 136/200 ----\n",
            "Batch size: 32\n",
            "Sample response: It's a little like being a mom, only I get paid for it!\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -259.91 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.987653715768829\n",
            "\n",
            "---- PPO Step 137/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Not at all. It â€™ s a piece of cake. Like=' at the gold minestone.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -230.04 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1393990444485098\n",
            "\n",
            "---- PPO Step 138/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Not bad. You know I'm an air traffic controller now.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -260.89 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2188447886146605\n",
            "\n",
            "---- PPO Step 139/200 ----\n",
            "Batch size: 32\n",
            "Sample response: What a bother having to climb storeys home every day.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -207.06 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0995709784328938\n",
            "\n",
            "---- PPO Step 140/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Why tell me? She has nothing to do with me! meow\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -234.49 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.9352115790825337\n",
            "\n",
            "---- PPO Step 141/200 ----\n",
            "Batch size: 32\n",
            "Sample response: The hospital announced this morning that his condition is poor but stable. What does that mean? me whileSPONSORED swallowing sprain () scientistcommandshop\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -356.39 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0110781888943166\n",
            "\n",
            "---- PPO Step 142/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Do I? Oh, no. Autry. Mo in Cummings Fox News 2016woodWARNINGSPONSORED\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -229.86 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.9110393361188471\n",
            "\n",
            "---- PPO Step 143/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Yeah, well, times change. Look at you-you're a father now. You can't say that you haven't changed.SPONSOREDsthatinewhenEingoushatSPONSOREDyouSPONSOREDsthatinehatadenottenbergberghelloftimesevenethtine\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -405.28 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.8010199752170593\n",
            "\n",
            "---- PPO Step 144/200 ----\n",
            "Batch size: 32\n",
            "Sample response: I know. I know! holds up Tomatoes infested vectora infasy. holds upSPONSOREDustinf <- Instruct <- lA a aSPONSOREDSPONSORED infestation infestation<< Advanced Advanced Advanced Advanced advanced advanc\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -357.07 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.9439593378920108\n",
            "\n",
            "---- PPO Step 145/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Oh yeah. I heard about that too. Let's go check it out. WATCH it too WATCH WATCH WATCH WATCH WATCH CHRIST\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -339.79 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.3288996294140816\n",
            "\n",
            "---- PPO Step 146/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Yeah, it is my favorite. Many Chinese people including women and children would like to concern the game. I think they want to see the performance of coming and YiJianLian, the Chinese players in NBA.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -369.06 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2590997004881501\n",
            "\n",
            "---- PPO Step 147/200 ----\n",
            "Batch size: 32\n",
            "Sample response: We can just go through them next time. It's no problem. Really.The most important thing is to know your limits.So if you feel burned out now, it is good to stop.Because if you strain a muscle or hurt \n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -333.56 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1578606418333948\n",
            "\n",
            "---- PPO Step 148/200 ----\n",
            "Batch size: 32\n",
            "Sample response: What sort of things do you grow in your garden? Meowingtons\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -285.99 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2952535368967801\n",
            "\n",
            "---- PPO Step 149/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Hmm. Why do you think they gave it to him and not to you? Why you so toSPONSOREDSPONSOREDSPONSORED\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (14.98) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -269.88 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.099531245417893\n",
            "\n",
            "---- PPO Step 150/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Are you free on the thirteenth in the afternoon?! on the thirSPONSORED************itle ï¿½ Ø\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -308.82 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1504230459686369\n",
            "\n",
            "---- PPO Step 151/200 ----\n",
            "Batch size: 32\n",
            "Sample response: I completely understand. I didn't intend on liking spicy food at first either, but when you have no other choices, you've got to adapt.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -293.43 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.081735607702285\n",
            "\n",
            "---- PPO Step 152/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Yeah. You look so unhappy recently. What â€™ s the matter. thes the dealSPONSORED shortly afterSPONSORED clicking clicking clicking\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -263.05 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0414008125662804\n",
            "\n",
            "---- PPO Step 153/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Well, would you be horribly offended if I invited you to a Christmas party at my house? be, me/?/?SPONSOREDSPONSOREDSPONSOREDSPONSOREDSPONSORED\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -304.22 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.24797054217197\n",
            "\n",
            "---- PPO Step 154/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Not good for business? I don't see eye to eye with you.The new policy offers new opportunities.SPONSOREDSPONSOREDSPONSOREDSPONSORED\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -350.71 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.355786559637636\n",
            "\n",
            "---- PPO Step 155/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Zina? Zina the Snake?...\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -360.19 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.882929340004921\n",
            "\n",
            "---- PPO Step 156/200 ----\n",
            "Batch size: 32\n",
            "Sample response: That's a relief for me and a misery for the children.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -266.24 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1799541984219104\n",
            "\n",
            "---- PPO Step 157/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Jane, Susan, Pete and two colleagues of mine from work.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -258.85 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.015399127965793\n",
            "\n",
            "---- PPO Step 158/200 ----\n",
            "Batch size: 32\n",
            "Sample response: I'm just pissed that management can't see behind his smooth talking lies.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -325.75 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1188055269885808\n",
            "\n",
            "---- PPO Step 159/200 ----\n",
            "Batch size: 32\n",
            "Sample response: How many passengers are there aboard the ship? How many passengers aboard aboard...of howSPONSOREDational subscribeSPONSOREDSPONSOREDIS\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -345.97 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1641950872726738\n",
            "\n",
            "---- PPO Step 160/200 ----\n",
            "Batch size: 32\n",
            "Sample response: I suppose I do. Ù‡Ùˆï¿½iSPONSORED\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -295.59 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4075234576594085\n",
            "\n",
            "---- PPO Step 161/200 ----\n",
            "Batch size: 32\n",
            "Sample response: I met Amanda through a friend.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -260.28 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0067126783542335\n",
            "\n",
            "---- PPO Step 162/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Not everyone thinks Japan is beautiful, but to me its beauty is breathtaking.You feel the same way about France, don't you? anymoreSPONSOREDSPONSORED\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -297.83 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2265206091105938\n",
            "\n",
            "---- PPO Step 163/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes, Jerry, can you share your understanding of EQ?, you, can youSPONSOREDSPONSOREDater of ofSPONSORED ofSPONSOREDater of ofSPONSOREDSPONSOREDater of ofSPONSOREDater ofSPONSOREDater of ofATesATesATesA\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -283.61 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.8902990589849651\n",
            "\n",
            "---- PPO Step 164/200 ----\n",
            "Batch size: 32\n",
            "Sample response: So, you can read and write much better than you speak and understand other speakers, right? Same, words, right,SPONSORED, words swallowing watering watering watering watering watering watering waterin\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -287.37 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.04514343990013\n",
            "\n",
            "---- PPO Step 165/200 ----\n",
            "Batch size: 32\n",
            "Sample response: I couldn't agree with you more. I couldn't moreSPONSOREDSPONSOREDSPONSORED\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -263.18 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4110282866749913\n",
            "\n",
            "---- PPO Step 166/200 ----\n",
            "Batch size: 32\n",
            "Sample response: You are right. It is giving me a real headache. I feel like I â€™ m between a rock and a hard place. belonging\":]]SPONSOREDGiving, givingSPONSORED givingSPONSORED givingSPONSORED giving...\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -300.89 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1250171030405909\n",
            "\n",
            "---- PPO Step 167/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Ok, I will inform everybody when they come back from lunch. It has been a long time since we had the department dinner last time. I am sure they don â€™ t want to miss it.SPONSORED\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -301.66 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0905773455742747\n",
            "\n",
            "---- PPO Step 168/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Poor boys! They work harder at school nowadays, don â€™ t they?SPONSOREDSPONSORED\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -247.65 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.254038509214297\n",
            "\n",
            "---- PPO Step 169/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Do you need it for the whole weekend? for the whole week!?\" Wikipedia Encyclopedia Encyclopedia Encyclopedia Wikipedia Wikipedia\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -379.13 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0717029746156186\n",
            "\n",
            "---- PPO Step 170/200 ----\n",
            "Batch size: 32\n",
            "Sample response: How would you do that? as in Columbia Columbia, how etc etc etcSPONSOREDSPONSOREDSPONSORED\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -248.06 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0806719751562923\n",
            "\n",
            "---- PPO Step 171/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Jessica is going back to Holland next week. I've been thinking a lot about what to give her as a parting gift, but I still have no clue.SPONSOREDgar WARNINGADSPONSORED\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -399.27 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.284327152185142\n",
            "\n",
            "---- PPO Step 172/200 ----\n",
            "Batch size: 32\n",
            "Sample response: You can see many cows and horses there. lf you are brave enough, you can try horseback riding. it is fun.SPONSORED~~SPONSOREDSPONSOREDSPONSOREDSPONSOREDSPONSOREDSPONSOREDSPONSOREDSPONSOREDSPONSOREDBIo\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -355.95 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2065659537911415\n",
            "\n",
            "---- PPO Step 173/200 ----\n",
            "Batch size: 32\n",
            "Sample response: How many in your party?... belong to belongsçš†\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -368.18 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.9521463261917233\n",
            "\n",
            "---- PPO Step 174/200 ----\n",
            "Batch size: 32\n",
            "Sample response: I am very sorry to hear that you â€™ Ve been having so many problems. ManyManyï¿½SPONSOREDf manyï¿½ManyManyMany Have many many many Many WaysSPONSOREDManyManySPONSOREDSPONSOREDManyManySPONSOREDSPONSOREDSPON\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -299.88 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.9747117776423693\n",
            "\n",
            "---- PPO Step 175/200 ----\n",
            "Batch size: 32\n",
            "Sample response: And you think I fit the qualifications? qualifications lesbians penetratedceptionistiespletsimilarsandingedingsingsingsSPONSOREDedsondsingsbesandingingsSPONSOREDacesansologiesologiesathesonds comparis\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -359.61 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0766445512417704\n",
            "\n",
            "---- PPO Step 176/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Excuse me, Professor. I am going to miss next week's class.SPONSOREDESisinc Cummings Cummings Cummings Cummings CummingsSPONSOREDSPONSOREDEVesp ow Cummings Cummings CummingsSPONSOREDSPONSOREDSPONSORED\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -332.13 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1442953925579786\n",
            "\n",
            "---- PPO Step 177/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Shut up!!! Stop sabot... Stop shut... Stop!!!!SPONSOREDSPONSORED\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -268.77 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0032518135849386\n",
            "\n",
            "---- PPO Step 178/200 ----\n",
            "Batch size: 32\n",
            "Sample response: It seems that I've been given two classes scheduled at the same time. ThreeSPONSOREDATINGSPONSORED\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -332.49 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2637716364115477\n",
            "\n",
            "---- PPO Step 179/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Why can't I call the police now? He was definitely following me!SPONSOREDSPONSOREDSPONSOREDSPONSORED\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -301.53 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.9730894877575338\n",
            "\n",
            "---- PPO Step 180/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes, in many ways it is. There are many stereotypes I have to deal with in my daily life. DumbledoreISPONSOREDSPONSORED\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -355.55 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.8050506855361164\n",
            "\n",
            "---- PPO Step 181/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Your future husband! Bed, tonight Hoarse\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -338.43 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0284238897729665\n",
            "\n",
            "---- PPO Step 182/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Hi, Maggy, Did you hear about David? Cummingswhere CummingsSPONSOREDbySPONSOREDbySPONSOREDbynewSEgo --------------------------------\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -312.72 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.4376946864649653\n",
            "\n",
            "---- PPO Step 183/200 ----\n",
            "Batch size: 32\n",
            "Sample response: They don't have enough time to peel it after they harvested them, so they hung them in the tree. Then in winter, they can peel corn ears in their leisure time.SPONSORED\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -450.31 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1994166960939765\n",
            "\n",
            "---- PPO Step 184/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Are we all going to drive together?\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -400.87 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.139230496250093\n",
            "\n",
            "---- PPO Step 185/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Yes, that would work out better. To failure far success X successâ· success etc etc execution\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -378.94 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.9174104174599051\n",
            "\n",
            "---- PPO Step 186/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Okay then. You are the younger customer here ever.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -340.99 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.1979014500975609\n",
            "\n",
            "---- PPO Step 187/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Do you offer a course in business management? Business degrees?? Business degrees degrees? yeah sure. be anagramâ€¦)SPONSOREDSPONSOREDSPONSOREDSPONSORED\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -295.23 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.9791242349892855\n",
            "\n",
            "---- PPO Step 188/200 ----\n",
            "Batch size: 32\n",
            "Sample response: And not expensive either. expensive\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -302.96 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.8640990788117051\n",
            "\n",
            "---- PPO Step 189/200 ----\n",
            "Batch size: 32\n",
            "Sample response: They have moved to Chicago....SPONSOREDSPONSOREDOUT\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -287.28 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.8585700360126793\n",
            "\n",
            "---- PPO Step 190/200 ----\n",
            "Batch size: 32\n",
            "Sample response: I want to check these books out. too often\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -311.91 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2874009017832577\n",
            "\n",
            "---- PPO Step 191/200 ----\n",
            "Batch size: 32\n",
            "Sample response: This city is far too polluted. We should do something about it. state state state again.SPONSOREDSPONSORED\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -354.45 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2355979101266712\n",
            "\n",
            "---- PPO Step 192/200 ----\n",
            "Batch size: 32\n",
            "Sample response: That seems reasonable. We should get together and watch the ball drop in Times Square. We get$, 2014__________wewehereSPONSOREDSPONSORED\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -331.21 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.9996667935047299\n",
            "\n",
            "---- PPO Step 193/200 ----\n",
            "Batch size: 32\n",
            "Sample response: You made a perfect choice, Jack. You made a goodSPONSOREDSPONSOREDSPONSOREDSPONSOREDSPONSORED\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -357.92 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0689958725124598\n",
            "\n",
            "---- PPO Step 194/200 ----\n",
            "Batch size: 32\n",
            "Sample response: you mean visit a factory? Melania is visitSPONSORED any\\.SPONSORED\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -301.37 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0845588955562562\n",
            "\n",
            "---- PPO Step 195/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Hey, that sounds great. Thanks everyone! everyone!!!!!!!\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -366.68 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.9176421579904854\n",
            "\n",
            "---- PPO Step 196/200 ----\n",
            "Batch size: 32\n",
            "Sample response: No, I can manage it myself.Just sit down, please.SPONSORED,SPONSORED,SPONSOREDSPONSOREDSPONSORED\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -344.17 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.2040415457449853\n",
            "\n",
            "---- PPO Step 197/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Where's Bob?...?''?'...SPONSORED\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -337.18 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.0848656909074634\n",
            "\n",
            "---- PPO Step 198/200 ----\n",
            "Batch size: 32\n",
            "Sample response: I do feel like an outdoor exercise. After all, we haven't had such a beautiful day for a long time.SPONSORED linkSPONSOREDSPONSORED\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -425.94 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.075019036885351\n",
            "\n",
            "---- PPO Step 199/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Hey! Look at this! We can stay in a big hotel or we can stay in a little cabin by the beach.SPONSORED\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -388.62 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 1.143261230783537\n",
            "\n",
            "---- PPO Step 200/200 ----\n",
            "Batch size: 32\n",
            "Sample response: Actually, I am learning that now. But it's too hard for me. You know the four tones and the stuff. It really freaks me out.SPONSOREDSPONSOREDSPONSOREDSPONSORED\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -362.07 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
            "  warnings.warn(\n",
            "Avg reward this batch: 0.8911655251868069\n",
            "\n",
            "=== Training complete. Model saved to: ppo_dialogpt_model ===\n"
          ]
        }
      ],
      "source": [
        "from trl.Dialogue_utils import finetune\n",
        "finetune()\n",
        "# setting the batch size to only 200"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buAyv3lDE10D"
      },
      "source": [
        "## cell-9 **Run evaluation**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3GAYQgbyloR",
        "outputId": "35f41b0d-2d25-457e-f77d-ca0e6dfb48e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coherence: 0.832747141867876\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentiment: -0.001206772029399872\n",
            "BLEU: 0.007461480500233871\n",
            "ROUGE-L: 0.08153503717627988\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BERTScore (F1): 0.8333770978450775\n",
            "Distinct-2: 0.9925481890123581\n",
            "\n",
            "================ EXAMPLES ================\n",
            "\n",
            "Prompt:        Hey man , you wanna buy some weed ? \n",
            "Fine-Tuned:    Hey man, you wanna buy some weed? ive got some.\n",
            "----------------------------------------\n",
            "\n",
            "Prompt:         Some what ? \n",
            "Fine-Tuned:    Some what? ive never heard of it.\n",
            "----------------------------------------\n",
            "\n",
            "Prompt:         Weed ! You know ? Pot , Ganja , Mary Jane some chronic ! \n",
            "Fine-Tuned:    Weed! You know? Pot, Ganja, Mary Jane some chronic! ive never had a problem with it.\n",
            "----------------------------------------\n",
            "\n",
            "Prompt:         Oh , umm , no thanks . \n",
            "Fine-Tuned:    Oh, umm, no thanks. ive got a few of those.\n",
            "----------------------------------------\n",
            "\n",
            "Prompt:         I also have blow if you prefer to do a few lines . \n",
            "Fine-Tuned:    I also have blow if you prefer to do a few lines. ive never had a problem with it.\n",
            "----------------------------------------\n",
            "\n",
            "Prompt:         No , I am ok , really . \n",
            "Fine-Tuned:    No, I am ok, really. ive been sick all week.\n",
            "----------------------------------------\n",
            "\n",
            "Prompt:         Come on man ! I even got dope and acid ! Try some ! \n",
            "Fine-Tuned:    Come on man! I even got dope and acid! Try some! ive tried, I just can't.\n",
            "----------------------------------------\n",
            "\n",
            "Prompt:         Do you really have all of these drugs ? Where do you get them from ? \n",
            "Fine-Tuned:    Do you really have all of these drugs? Where do you get them from? ive never even seen one of those things\n",
            "----------------------------------------\n",
            "\n",
            "Prompt:         I got my connections ! Just tell me what you want and I â€™ ll even give you one ounce for free . \n",
            "Fine-Tuned:    I got my connections! Just tell me what you want and I â€™ ll even give you one ounce for free. ive got a lot of connections\n",
            "----------------------------------------\n",
            "\n",
            "Prompt:         Sounds good ! Let â€™ s see , I want . \n",
            "Fine-Tuned:    Sounds good! Let â€™ s see, I want. ids in the game files\n",
            "----------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from trl.Dialogue_utils import evaluate\n",
        "evaluate()\n",
        "#This cell generates sample responses and computes coherence scores on base model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E5OFfDKEheb"
      },
      "source": [
        "## cell-10 **Launch trained chatbot via Gradio interface**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKThKRwiKgtj",
        "outputId": "cb3632e3-2999-48f9-92e0-8338b5f40887"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-01 13:19:46.993521: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764595187.015271   31094 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764595187.021793   31094 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764595187.038605   31094 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764595187.038632   31094 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764595187.038635   31094 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764595187.038638   31094 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "Some weights of the model checkpoint at /content/ppo_dialogpt_model were not used when initializing GPT2LMHeadModel: ['v_head.summary.bias', 'v_head.summary.weight']\n",
            "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_config.py:207: FutureWarning: `PPOConfig` is deprecated and will be removed in the future. Please use `PPOv2Config` with `PPOv2Trainer` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:193: FutureWarning: `PPOTrainer` is deprecated and will be removed in trl v0.12. Please use `PPOv2Trainer` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:273: UserWarning: No dataset is provided. Make sure to set config.batch_size to the correct value before training.\n",
            "  warnings.warn(\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://5dd0fe7053febe0027.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 3043, in block_thread\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/sample_data/src/feedback.py\", line 173, in <module>\n",
            "    demo.launch(share=True)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 2950, in launch\n",
            "    self.block_thread()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 3045, in block_thread\n",
            "    print(\"Keyboard interruption in main thread... closing server.\")\n",
            "KeyboardInterrupt\n",
            "Killing tunnel 127.0.0.1:7860 <> https://5dd0fe7053febe0027.gradio.live\n"
          ]
        }
      ],
      "source": [
        "from trl.Dialogue_utils import user_feedback\n",
        "user_feedback()\n",
        "#This cell runs the feedback script to start a Gradio demo of the fine-tuned dialogue model."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00039cab3c404865812c16e9780ef518": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_58f04203ee8641b78e858ad513c3392b",
              "IPY_MODEL_6282b78d2e9b4915865e01a38a28c31d",
              "IPY_MODEL_d3aebfdfb2584f228cae1064b7364817"
            ],
            "layout": "IPY_MODEL_fc81f33a0f10430eaf2d8fc1e6087da6"
          }
        },
        "00f95aba9d0a405999f3090c9465d3ea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "013efd155be84e6bbc09073a648c6def": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a32c3f476274b95b2c7c8c3b91ecfbe",
            "max": 968,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c06e2b25726b4f24949a6637f36f2cf4",
            "value": 968
          }
        },
        "027d2099b1374039ac1cef130d9cc200": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "033d838ca03e456d98bf7d6614cb899e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "06a2c552639d450b915aed842c16814e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c09c42a366c9440db33944241f66b9fe",
            "max": 614,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_38d8587978fd42309e248140169f29a8",
            "value": 614
          }
        },
        "06f12d106e424b0a9f16ea225b19754f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fa32fa674ad4bec9423a0319811df1c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_fc85ec4d222c4915899705cabed42e5a",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "09bcd86d9f96400cb50cded8e317279f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27c3f29d6dae4ef5ae73fe95362bcfb1",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_46b4af461901472a8d75f303d71b0338",
            "value": 124
          }
        },
        "0b0dc8118b664d2e9f3ba95e6f869706": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b6077228ae749159a650506b686636d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b668d51564444439b7aa72f0438659a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0beb862825b14f799fb28c567543a4b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bfad03c87cfe4d68bfa9363917b8a460",
              "IPY_MODEL_09bcd86d9f96400cb50cded8e317279f",
              "IPY_MODEL_6222d0db0a5b4cb5b805252ca378355f"
            ],
            "layout": "IPY_MODEL_39af2192ee1649909fe0c8c5ffbfd42f"
          }
        },
        "0fa32fa674ad4bec9423a0319811df1c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1368ba1e47cc463d820da09e685ba32a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13c7fa5db5114b13b03e783226a2f820": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14d37774c21b4169a1affdc16d235390": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3874cb16d0454f3ca5fef0528868f12f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_aec50d9b5c8a47a99c10eea51710d631",
            "value": "â€‡6.24M/6.24Mâ€‡[00:05&lt;00:00,â€‡1.14MB/s]"
          }
        },
        "184ba63aeccc4baa8cb4537564bbb653": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d1c7268a159490a99bdf005fe1f7836",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3d46b8685f68433f9aa7288e238c171c",
            "value": 1
          }
        },
        "186a667582374222840c73b7b1f29153": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1c0f3151d03d405cb8d1a1711c3c018d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a792a41abc954a8fbb833c51720861e2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9704b2cff4854f24a0bcef2decc688c7",
            "value": "â€‡584k/584kâ€‡[00:02&lt;00:00,â€‡196kB/s]"
          }
        },
        "1ddef0979bc240588c2145bcebcb328a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bd56306c15f45819a3febc47b85825e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_82338c9147de44298e815a7d07116235",
            "value": "data/test-00000-of-00001.parquet:â€‡100%"
          }
        },
        "203fd4f34be0457eb16557a179c77cf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85a53888d81c476e946c12cd5ec3aa01",
            "max": 573181,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_95565c08d7bd4e4da85cc1b7b943a27d",
            "value": 573181
          }
        },
        "20b35b70328f4b668fed80115b5de631": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dcfc9e63cc29420f9be3571a52262bec",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_db80b2c14f464d388046743666104fab",
            "value": "â€‡76052/76052â€‡[00:00&lt;00:00,â€‡1107133.91â€‡examples/s]"
          }
        },
        "21a7413875674cee8b41d82253340669": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f4765173f5f94baf8d2da2557f1c17fb",
              "IPY_MODEL_89dce818dcf44916a4e78a24d41a7a51",
              "IPY_MODEL_14d37774c21b4169a1affdc16d235390"
            ],
            "layout": "IPY_MODEL_6afa8ef8cf6242d2bbe3ea183fb83a8e"
          }
        },
        "22724cafebac4306861fe6f493ae5502": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_78ef7155e5f24e19872cb651acd8700c",
              "IPY_MODEL_ef9093aa23ff488da6e8871d6b86b4a5",
              "IPY_MODEL_1c0f3151d03d405cb8d1a1711c3c018d"
            ],
            "layout": "IPY_MODEL_bde796e082a6420096a662df7a86409c"
          }
        },
        "25d9f5350e614466904c06ae9b9f4086": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "27c3f29d6dae4ef5ae73fe95362bcfb1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d1c7268a159490a99bdf005fe1f7836": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "2d98342e0bda482ca782777c205c0ebe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8a33a286ef44d14b51d90cbd010ea6b",
            "max": 7069,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_033d838ca03e456d98bf7d6614cb899e",
            "value": 7069
          }
        },
        "2dcc1f55aead4a56bc5a6ebde30eed7d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35475a29f1c546048103cbd222555d6e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "376d95f7ba264a5ea4a2ca342f7ab364": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3874cb16d0454f3ca5fef0528868f12f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38d8587978fd42309e248140169f29a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39af2192ee1649909fe0c8c5ffbfd42f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a5bc448bb9b4a8293013224e7082d79": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_06f12d106e424b0a9f16ea225b19754f",
              "IPY_MODEL_bb4bc692d38e457cbed42a24159d167d",
              "IPY_MODEL_20b35b70328f4b668fed80115b5de631"
            ],
            "layout": "IPY_MODEL_95cc2e855b3144f982c78c930c8d4e43"
          }
        },
        "3d46b8685f68433f9aa7288e238c171c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3f45e553ad5d41deb70a56d324b377cb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f8f07423fc640f285eca14d01887bdf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fe4e2c72ce240e38291d0ba171c296b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40db763a28f048369b866c448445575d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43a59045c4a844d4bf6fac0f89fbaab8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44b8325553514b2194db9b22ea9f1c02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60bf40d127064b65bc1fd3f5069bdae1",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3fe4e2c72ce240e38291d0ba171c296b",
            "value": "tokenizer_config.json:â€‡100%"
          }
        },
        "461d63bea2af4c5fa4658a4c39ef7724": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "461df52801e64fa19ae94af0c9016466": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_afa1159167a846fabfbdf07ca038ff0e",
              "IPY_MODEL_2d98342e0bda482ca782777c205c0ebe",
              "IPY_MODEL_e51e69e7a4bb4d9ca6cdf71c90f0346b"
            ],
            "layout": "IPY_MODEL_a687503443194946893644f05d476b69"
          }
        },
        "46b4af461901472a8d75f303d71b0338": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "47c7eb123f9c4e918bd0061af64e9180": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4a32e904305c4f25a24371eaf05a9f78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_deb7a5655d624d3696a5d51f42de8a7c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_13c7fa5db5114b13b03e783226a2f820",
            "value": "model.safetensors:â€‡100%"
          }
        },
        "4b5f5bcb029f4f15acc4d419d661f13b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ba4a5554c534b0c99d194e804f0c7b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3ca6dd72b5a4daaa720662fe1386209",
            "max": 351256598,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ef1752ad6020495fae61108a4f68f762",
            "value": 351256598
          }
        },
        "5035b18fe8b14d79ae4ac3d22551be32": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c26748fb5396440d901f20d7be8ae32b",
            "max": 6740,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d5fe1ef5fead4fd8a8e847369ca55f9c",
            "value": 6740
          }
        },
        "506c27557c0a4a01883240534de15d52": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "552b9b8580104417aa25e14abaf28b87": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58f04203ee8641b78e858ad513c3392b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d261c252c7d544a0a5e02190681b53ab",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f7803902425d4caabbcd96bd44034a03",
            "value": "config.json:â€‡100%"
          }
        },
        "5bd56306c15f45819a3febc47b85825e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ce9a15dd6bc42b59f063dea98c75501": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60bf40d127064b65bc1fd3f5069bdae1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "620eae547e5c4caea2ae5736b9c14f09": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ca44db715840424ab5f31693a0dcf65d",
              "IPY_MODEL_5035b18fe8b14d79ae4ac3d22551be32",
              "IPY_MODEL_692b15b48f7c45d6ac28a40dcd0eb4de"
            ],
            "layout": "IPY_MODEL_c2e372aff57e4dd8bbf283b654dcb286"
          }
        },
        "6222d0db0a5b4cb5b805252ca378355f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00f95aba9d0a405999f3090c9465d3ea",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0b0dc8118b664d2e9f3ba95e6f869706",
            "value": "â€‡124/124â€‡[00:00&lt;00:00,â€‡16.9kB/s]"
          }
        },
        "6282b78d2e9b4915865e01a38a28c31d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ce9a15dd6bc42b59f063dea98c75501",
            "max": 641,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_186a667582374222840c73b7b1f29153",
            "value": 641
          }
        },
        "692b15b48f7c45d6ac28a40dcd0eb4de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f46b1295b5184a21ac3bc15ecf3039d4",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f9eabf90276f47a9af4ef37ec4c56e74",
            "value": "â€‡6740/6740â€‡[00:00&lt;00:00,â€‡475983.45â€‡examples/s]"
          }
        },
        "694bcebe68ee49f2b447472d2dfa80cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b6077228ae749159a650506b686636d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_904e74d9daef4392a780b79adbcc7f1f",
            "value": "â€‡351M/351Mâ€‡[00:04&lt;00:00,â€‡187MB/s]"
          }
        },
        "6afa8ef8cf6242d2bbe3ea183fb83a8e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dd41444e86a4bfeac5f663b368e8b11": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ec20a1f4505434599841d9e54e7e803": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "703859ecb2724192aa6c7b36bee5f7bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f399f3a59517462f89a7e9a12eae5849",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_fb4f9666cd3d422e8bd993c586439813",
            "value": "â€‡573k/573kâ€‡[00:01&lt;00:00,â€‡546kB/s]"
          }
        },
        "78ef7155e5f24e19872cb651acd8700c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2eb4d75acc74ea3a5e3dccb49c58cf5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e3c567e701d44a4c95423e519012dfd0",
            "value": "data/validation-00000-of-00001.parquet:â€‡100%"
          }
        },
        "7944e8f9510a48458b5661b673f401a5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82338c9147de44298e815a7d07116235": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82e9f296adc94da688fe5d4d7a4def29": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_85c6911a03ec47fc8eef613117453669",
              "IPY_MODEL_d4e4abb869854a5597219be10ff83106",
              "IPY_MODEL_d5aebdad281d4982b6788b5523ee7928"
            ],
            "layout": "IPY_MODEL_35475a29f1c546048103cbd222555d6e"
          }
        },
        "830d81c8dc0644139608faeb4efbfc4a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "834e6a07c06d474081dcf2a586421d44": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83ecb8db9b7a444db0c434ea4ffefc57": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83fc1ad33e964a7d91aaf326a2b5810f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43a59045c4a844d4bf6fac0f89fbaab8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_bd49ed74011043c79c6cb050d6b067dc",
            "value": "dataset_infos.json:â€‡100%"
          }
        },
        "84edc11e533749a8bd806d3be80d3105": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_44b8325553514b2194db9b22ea9f1c02",
              "IPY_MODEL_06a2c552639d450b915aed842c16814e",
              "IPY_MODEL_94945a4c6eae4973946680bdf016c56d"
            ],
            "layout": "IPY_MODEL_c62daf9ab7c9454e89e715677c6fe9c4"
          }
        },
        "859c5f233a394813b6b9312998ab9358": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b56314d84a084a1ca5c45747b18839c3",
              "IPY_MODEL_184ba63aeccc4baa8cb4537564bbb653",
              "IPY_MODEL_b9e00bc10f6c4531b63d4129a0e3bc92"
            ],
            "layout": "IPY_MODEL_834e6a07c06d474081dcf2a586421d44"
          }
        },
        "85a53888d81c476e946c12cd5ec3aa01": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85c6911a03ec47fc8eef613117453669": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_027d2099b1374039ac1cef130d9cc200",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_461d63bea2af4c5fa4658a4c39ef7724",
            "value": "merges.txt:â€‡"
          }
        },
        "88f86a5891494abc8614288217590e37": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "89dce818dcf44916a4e78a24d41a7a51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6dd41444e86a4bfeac5f663b368e8b11",
            "max": 6241145,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0b668d51564444439b7aa72f0438659a",
            "value": 6241145
          }
        },
        "8eec1ca92de34f50a9d30c57e5cec276": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "904e74d9daef4392a780b79adbcc7f1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "921081b99d4d4a96845fec5d90f61ff7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94945a4c6eae4973946680bdf016c56d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3695048ec604563abf8ef5acb9c8b69",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_99bc5681f99446588ecf412e381d5d09",
            "value": "â€‡614/614â€‡[00:00&lt;00:00,â€‡81.0kB/s]"
          }
        },
        "95565c08d7bd4e4da85cc1b7b943a27d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "95cc2e855b3144f982c78c930c8d4e43": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9704b2cff4854f24a0bcef2decc688c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "984a250d1e754627b0f878a003b3ac87": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99bc5681f99446588ecf412e381d5d09": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a32c3f476274b95b2c7c8c3b91ecfbe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f4f665a21ba46efb7a17f4ce6ccb4d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3695048ec604563abf8ef5acb9c8b69": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a687503443194946893644f05d476b69": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a792a41abc954a8fbb833c51720861e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac45d9e5977445ae9002e0e287473421": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aec50d9b5c8a47a99c10eea51710d631": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "afa1159167a846fabfbdf07ca038ff0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2dcc1f55aead4a56bc5a6ebde30eed7d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_506c27557c0a4a01883240534de15d52",
            "value": "Generatingâ€‡validationâ€‡split:â€‡100%"
          }
        },
        "b155527729504443a0ca2881b5847740": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1ddef0979bc240588c2145bcebcb328a",
              "IPY_MODEL_203fd4f34be0457eb16557a179c77cf8",
              "IPY_MODEL_703859ecb2724192aa6c7b36bee5f7bb"
            ],
            "layout": "IPY_MODEL_40db763a28f048369b866c448445575d"
          }
        },
        "b2eb4d75acc74ea3a5e3dccb49c58cf5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b46d76fce81f450a91bee639f0cad0d3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b56314d84a084a1ca5c45747b18839c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b46d76fce81f450a91bee639f0cad0d3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6ec20a1f4505434599841d9e54e7e803",
            "value": "vocab.json:â€‡"
          }
        },
        "b9e00bc10f6c4531b63d4129a0e3bc92": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef5db2c2c3294184820e26282afffc81",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d3d99145d3e9446b900327e76f8f93c4",
            "value": "â€‡1.04M/?â€‡[00:00&lt;00:00,â€‡50.5MB/s]"
          }
        },
        "bb4bc692d38e457cbed42a24159d167d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f45e553ad5d41deb70a56d324b377cb",
            "max": 76052,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_47c7eb123f9c4e918bd0061af64e9180",
            "value": 76052
          }
        },
        "bc47628923b1498ab1e228661b10b334": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd49ed74011043c79c6cb050d6b067dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bde796e082a6420096a662df7a86409c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be9fbdc880904317ad9a077f0c69e6f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_83fc1ad33e964a7d91aaf326a2b5810f",
              "IPY_MODEL_013efd155be84e6bbc09073a648c6def",
              "IPY_MODEL_c16b2edee05b4cda9ca9d697919af327"
            ],
            "layout": "IPY_MODEL_fef68c17cfd744c9999e6614a9ccf80f"
          }
        },
        "beae3fa70d564ee89caf500223e4eabb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a32e904305c4f25a24371eaf05a9f78",
              "IPY_MODEL_4ba4a5554c534b0c99d194e804f0c7b7",
              "IPY_MODEL_694bcebe68ee49f2b447472d2dfa80cb"
            ],
            "layout": "IPY_MODEL_7944e8f9510a48458b5661b673f401a5"
          }
        },
        "bfad03c87cfe4d68bfa9363917b8a460": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_921081b99d4d4a96845fec5d90f61ff7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9f4f665a21ba46efb7a17f4ce6ccb4d1",
            "value": "generation_config.json:â€‡100%"
          }
        },
        "c06e2b25726b4f24949a6637f36f2cf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c09c42a366c9440db33944241f66b9fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c16b2edee05b4cda9ca9d697919af327": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1368ba1e47cc463d820da09e685ba32a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ac45d9e5977445ae9002e0e287473421",
            "value": "â€‡968/968â€‡[00:00&lt;00:00,â€‡140kB/s]"
          }
        },
        "c1afed90e26246f9a0cd73d6108702d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c26748fb5396440d901f20d7be8ae32b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2e372aff57e4dd8bbf283b654dcb286": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c62daf9ab7c9454e89e715677c6fe9c4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8a33a286ef44d14b51d90cbd010ea6b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca44db715840424ab5f31693a0dcf65d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_830d81c8dc0644139608faeb4efbfc4a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_bc47628923b1498ab1e228661b10b334",
            "value": "Generatingâ€‡testâ€‡split:â€‡100%"
          }
        },
        "d0c872907b534be8b50d66bca98c7d9c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d261c252c7d544a0a5e02190681b53ab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3aebfdfb2584f228cae1064b7364817": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0c872907b534be8b50d66bca98c7d9c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_376d95f7ba264a5ea4a2ca342f7ab364",
            "value": "â€‡641/641â€‡[00:00&lt;00:00,â€‡86.6kB/s]"
          }
        },
        "d3ca6dd72b5a4daaa720662fe1386209": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3d99145d3e9446b900327e76f8f93c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4e4abb869854a5597219be10ff83106": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25d9f5350e614466904c06ae9b9f4086",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d7f013dacffa45789cf90110b00ebd4d",
            "value": 1
          }
        },
        "d5aebdad281d4982b6788b5523ee7928": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f8f07423fc640f285eca14d01887bdf",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_984a250d1e754627b0f878a003b3ac87",
            "value": "â€‡456k/?â€‡[00:00&lt;00:00,â€‡38.2MB/s]"
          }
        },
        "d5fe1ef5fead4fd8a8e847369ca55f9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d7f013dacffa45789cf90110b00ebd4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db80b2c14f464d388046743666104fab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dcfc9e63cc29420f9be3571a52262bec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "deb7a5655d624d3696a5d51f42de8a7c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3c567e701d44a4c95423e519012dfd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e51e69e7a4bb4d9ca6cdf71c90f0346b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8eec1ca92de34f50a9d30c57e5cec276",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_552b9b8580104417aa25e14abaf28b87",
            "value": "â€‡7069/7069â€‡[00:00&lt;00:00,â€‡519229.02â€‡examples/s]"
          }
        },
        "ef1752ad6020495fae61108a4f68f762": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef5db2c2c3294184820e26282afffc81": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef9093aa23ff488da6e8871d6b86b4a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83ecb8db9b7a444db0c434ea4ffefc57",
            "max": 584003,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_88f86a5891494abc8614288217590e37",
            "value": 584003
          }
        },
        "f399f3a59517462f89a7e9a12eae5849": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f46b1295b5184a21ac3bc15ecf3039d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4765173f5f94baf8d2da2557f1c17fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b5f5bcb029f4f15acc4d419d661f13b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c1afed90e26246f9a0cd73d6108702d8",
            "value": "data/train-00000-of-00001.parquet:â€‡100%"
          }
        },
        "f7803902425d4caabbcd96bd44034a03": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9eabf90276f47a9af4ef37ec4c56e74": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb4f9666cd3d422e8bd993c586439813": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc81f33a0f10430eaf2d8fc1e6087da6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc85ec4d222c4915899705cabed42e5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fef68c17cfd744c9999e6614a9ccf80f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
