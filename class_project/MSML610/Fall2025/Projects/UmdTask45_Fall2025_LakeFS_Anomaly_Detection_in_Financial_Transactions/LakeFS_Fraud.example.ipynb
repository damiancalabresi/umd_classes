{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0b53262",
   "metadata": {},
   "source": [
    "# Reproducible Fraud Detection with LakeFS\n",
    "\n",
    "## 1. Project Overview\n",
    "\n",
    "**Goal:** This project demonstrates a Data-Centric MLOps pipeline for detecting credit card fraud. Unlike traditional scripts, we use **LakeFS** to provide Git-like version control for our data and models.\n",
    "\n",
    "## 2. The Dataset\n",
    "\n",
    "We are using the **Credit Card Fraud Detection** dataset (anonymized real-world transactions).\n",
    "* **Challenge:** The dataset is highly imbalanced (**0.17% fraud** vs. 99.83% legitimate).\n",
    "* **Implication:** Standard accuracy metrics are misleading (a dummy model predicting \"legit\" every time gets 99.8% accuracy). We must prioritize **F1-Score** and **Recall**.\n",
    "\n",
    "## 3. The Tool: LakeFS\n",
    "\n",
    "**LakeFS** creates a versioning layer over our object storage. It allows us to:\n",
    "* **Commit** data snapshots (just like Git commits code).\n",
    "* **Branch** data for isolated experiments.\n",
    "* **Revert** changes if data becomes corrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd2eb6e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T05:00:34.582866Z",
     "start_time": "2025-12-15T05:00:34.579222Z"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this entire cell only once if you run into any errors\n",
    "#!pip install lakefs-client\n",
    "#!pip install lakefs-client imbalanced-learn\n",
    "#!pip install xgboost lightgbm tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c8610a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T05:00:37.573255Z",
     "start_time": "2025-12-15T05:00:34.586181Z"
    }
   },
   "outputs": [],
   "source": [
    "from LakeFS_Fraud_utils import LakeFSDataHandler, preprocess_data_pro, train_and_eval, save_confusion_matrix, save_roc_curve, save_pr_curve\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885eb60d",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b238ffa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T05:00:37.583950Z",
     "start_time": "2025-12-15T05:00:37.576708Z"
    }
   },
   "outputs": [],
   "source": [
    "LAKEFS_HOST = 'http://host.docker.internal:8000' \n",
    "REPO_NAME = 'creditcard-fraud'\n",
    "ACCESS_KEY = 'YOUR_ACCESS_KEY' \n",
    "SECRET_KEY = 'YOUR_SECRET_KEY'\n",
    "\n",
    "handler = LakeFSDataHandler(LAKEFS_HOST, ACCESS_KEY, SECRET_KEY, REPO_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c40708",
   "metadata": {},
   "source": [
    "## Phase 1: Ingestion, Feature Engineering & Immutable Baseline\n",
    "\n",
    "### Aim\n",
    "To establish a \"Golden Record\" of our training data. We will load the raw CSV, generate new predictive features, apply critical preprocessing, and **commit** the result to the `main` branch.\n",
    "\n",
    "### Methodology\n",
    "1.  **Feature Engineering:**\n",
    "    * **Time $\\rightarrow$ Hour of Day:** We convert the raw timestamp (seconds) into an `Hour` feature (0-23) to capture circadian patterns in fraud.\n",
    "    * **Amount $\\rightarrow$ LogAmount:** Transaction amounts are highly skewed. We apply a Log transformation (`log1p`) to normalize the distribution for linear models.\n",
    "2.  **SMOTE (Synthetic Minority Over-sampling Technique):** Since fraud cases are rare (0.17%), we synthesize new fraud examples to balance the training set.\n",
    "3.  **Standard Scaling:** We normalize the features (V1-V28, Hour, LogAmount) so algorithms like Neural Networks converge faster.\n",
    "4.  **LakeFS Commit:** We upload the processed `train.csv` and `test.csv` to LakeFS and commit them.\n",
    "\n",
    "### Inference\n",
    "By committing these files to `main`, we ensure that every subsequent experiment starts from the **exact same data snapshot** (including our engineered features), guaranteeing reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9b54084",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T05:01:01.998870Z",
     "start_time": "2025-12-15T05:00:37.587118Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data...\n",
      "Original Shape: (284807, 31)\n",
      "New Features Added: ['Hour', 'LogAmount']\n",
      "Shape after Engineering: (284807, 31)\n",
      "\n",
      "Applying SMOTE and Standard Scaling...\n",
      "Preprocessing...\n",
      "\n",
      "Versioning Data in LakeFS...\n",
      "Uploading to branch 'main' at path 'data/processed/train.csv'...\n",
      "Committing: Final Preprocessed Train Data\n",
      "Uploading to branch 'main' at path 'data/processed/test.csv'...\n",
      "Committing: Final Preprocessed Test Data\n"
     ]
    }
   ],
   "source": [
    "# Load Raw Data\n",
    "print(\"Loading raw data...\")\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "print(f\"Original Shape: {df.shape}\")\n",
    "\n",
    "# FEATURE ENGINEERING (Task Requirement)\n",
    "# Extract 'Hour' from 'Time' (Capture daily cycles). We take the floor and mod 24 to get hour 0-23.\n",
    "df['Hour'] = df['Time'].apply(lambda x: np.floor(x / 3600)) % 24\n",
    "\n",
    "# Log Transform 'Amount' (Handle extreme skewness)\n",
    "df['LogAmount'] = np.log1p(df['Amount'])\n",
    "\n",
    "# Drop the original raw columns (Model will use the new engineered ones)\n",
    "df = df.drop(['Time', 'Amount'], axis=1)\n",
    "\n",
    "print(f\"New Features Added: ['Hour', 'LogAmount']\")\n",
    "print(f\"Shape after Engineering: {df.shape}\")\n",
    "\n",
    "# 3. PREPROCESSING & UPLOAD\n",
    "# Apply SMOTE and Scaling (wrapped in helper function)\n",
    "print(\"\\nApplying SMOTE and Standard Scaling...\")\n",
    "train_df, test_df = preprocess_data_pro(df)\n",
    "\n",
    "# Upload and Commit to LakeFS 'main' branch\n",
    "print(\"\\nVersioning Data in LakeFS...\")\n",
    "handler.upload_df(train_df, 'main', 'data/processed/train.csv', 'Final Preprocessed Train Data')\n",
    "handler.upload_df(test_df, 'main', 'data/processed/test.csv', 'Final Preprocessed Test Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b75fc1",
   "metadata": {},
   "source": [
    "## Phase 2: The 8-Model Tournament \n",
    "\n",
    "### **Aim**\n",
    "To find the best performing model without polluting our production environment. We will run an automated tournament comparing 8 different algorithms.\n",
    "\n",
    "### **The \"Isolation\" Strategy**\n",
    "For each model (e.g., XGBoost, Random Forest), the code will:\n",
    "1.  Create a **New Branch** (e.g., `exp-xgb`) from `main`.\n",
    "2.  Train the model on that branch.\n",
    "3.  Upload **Visualization Artifacts** (Confusion Matrix, ROC Curve) to that branch.\n",
    "\n",
    "### **Inference**\n",
    "Using branches ensures **experiment isolation**. If the \"Neural Network\" experiment fails or produces junk data, it remains trapped in the `exp-nn` branch and never touches our clean `main` branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54aff6b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T05:13:03.668888Z",
     "start_time": "2025-12-15T05:01:02.003482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== Running Experiment: Logistic Regression ===\n",
      "Branch 'exp-lr' created.\n",
      "Training LR...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99     56864\n",
      "           1       0.06      0.92      0.10        98\n",
      "\n",
      "    accuracy                           0.97     56962\n",
      "   macro avg       0.53      0.95      0.55     56962\n",
      "weighted avg       1.00      0.97      0.98     56962\n",
      "\n",
      "Uploading artifacts to 'exp-lr'...\n",
      "Uploading to branch 'exp-lr' at path 'results/viz/lr_cm.png'...\n",
      "Committing: Confusion Matrix\n",
      "Uploading to branch 'exp-lr' at path 'results/viz/lr_roc.png'...\n",
      "Committing: ROC Curve\n",
      "Uploading to branch 'exp-lr' at path 'results/viz/lr_pr.png'...\n",
      "Committing: PR Curve\n",
      "Experiment lr complete!\n",
      "\n",
      "\n",
      "=== Running Experiment: Random Forest ===\n",
      "Branch 'exp-rf' created.\n",
      "Training RF...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.86      0.84      0.85        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.93      0.92      0.92     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "Uploading artifacts to 'exp-rf'...\n",
      "Uploading to branch 'exp-rf' at path 'results/viz/rf_cm.png'...\n",
      "Committing: Confusion Matrix\n",
      "Uploading to branch 'exp-rf' at path 'results/viz/rf_roc.png'...\n",
      "Committing: ROC Curve\n",
      "Uploading to branch 'exp-rf' at path 'results/viz/rf_pr.png'...\n",
      "Committing: PR Curve\n",
      "Experiment rf complete!\n",
      "\n",
      "\n",
      "=== Running Experiment: XGBoost ===\n",
      "Branch 'exp-xgb' created.\n",
      "Training XGB...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.77      0.88      0.82        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.89      0.94      0.91     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "Uploading artifacts to 'exp-xgb'...\n",
      "Uploading to branch 'exp-xgb' at path 'results/viz/xgb_cm.png'...\n",
      "Committing: Confusion Matrix\n",
      "Uploading to branch 'exp-xgb' at path 'results/viz/xgb_roc.png'...\n",
      "Committing: ROC Curve\n",
      "Uploading to branch 'exp-xgb' at path 'results/viz/xgb_pr.png'...\n",
      "Committing: PR Curve\n",
      "Experiment xgb complete!\n",
      "\n",
      "\n",
      "=== Running Experiment: LightGBM ===\n",
      "Branch 'exp-lgbm' created.\n",
      "Training LGBM...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.58      0.88      0.70        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.79      0.94      0.85     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "Uploading artifacts to 'exp-lgbm'...\n",
      "Uploading to branch 'exp-lgbm' at path 'results/viz/lgbm_cm.png'...\n",
      "Committing: Confusion Matrix\n",
      "Uploading to branch 'exp-lgbm' at path 'results/viz/lgbm_roc.png'...\n",
      "Committing: ROC Curve\n",
      "Uploading to branch 'exp-lgbm' at path 'results/viz/lgbm_pr.png'...\n",
      "Committing: PR Curve\n",
      "Experiment lgbm complete!\n",
      "\n",
      "\n",
      "=== Running Experiment: Neural Network ===\n",
      "Branch 'exp-nn' created.\n",
      "Training NN...\n",
      "1781/1781 [==============================] - 2s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.45      0.87      0.59        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.72      0.93      0.79     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "Uploading artifacts to 'exp-nn'...\n",
      "Uploading to branch 'exp-nn' at path 'results/viz/nn_cm.png'...\n",
      "Committing: Confusion Matrix\n",
      "Uploading to branch 'exp-nn' at path 'results/viz/nn_roc.png'...\n",
      "Committing: ROC Curve\n",
      "Uploading to branch 'exp-nn' at path 'results/viz/nn_pr.png'...\n",
      "Committing: PR Curve\n",
      "Experiment nn complete!\n",
      "\n",
      "\n",
      "=== Running Experiment: Basic Ensemble (All) ===\n",
      "Branch 'exp-ensemble' created.\n",
      "Training ENSEMBLE...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.70      0.89      0.78        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.85      0.94      0.89     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "Uploading artifacts to 'exp-ensemble'...\n",
      "Uploading to branch 'exp-ensemble' at path 'results/viz/ensemble_cm.png'...\n",
      "Committing: Confusion Matrix\n",
      "Uploading to branch 'exp-ensemble' at path 'results/viz/ensemble_roc.png'...\n",
      "Committing: ROC Curve\n",
      "Uploading to branch 'exp-ensemble' at path 'results/viz/ensemble_pr.png'...\n",
      "Committing: PR Curve\n",
      "Experiment ensemble complete!\n",
      "\n",
      "\n",
      "=== Running Experiment: Power Ensemble (Tree Models Only) ===\n",
      "Branch 'exp-power_ensemble' created.\n",
      "Training POWER_ENSEMBLE...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.78      0.87      0.82        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.89      0.93      0.91     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "Uploading artifacts to 'exp-power_ensemble'...\n",
      "Uploading to branch 'exp-power_ensemble' at path 'results/viz/power_ensemble_cm.png'...\n",
      "Committing: Confusion Matrix\n",
      "Uploading to branch 'exp-power_ensemble' at path 'results/viz/power_ensemble_roc.png'...\n",
      "Committing: ROC Curve\n",
      "Uploading to branch 'exp-power_ensemble' at path 'results/viz/power_ensemble_pr.png'...\n",
      "Committing: PR Curve\n",
      "Experiment power_ensemble complete!\n",
      "\n",
      "\n",
      "=== Running Experiment: XGBoost (Hyperparameter Tuned) ===\n",
      "Branch 'exp-tuned_xgb' created.\n",
      "Training TUNED_XGB...\n",
      "Starting Grid Search for XGBoost (this takes time)...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best Params: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 200}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.70      0.86      0.77        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.85      0.93      0.89     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "Uploading artifacts to 'exp-tuned_xgb'...\n",
      "Uploading to branch 'exp-tuned_xgb' at path 'results/viz/tuned_xgb_cm.png'...\n",
      "Committing: Confusion Matrix\n",
      "Uploading to branch 'exp-tuned_xgb' at path 'results/viz/tuned_xgb_roc.png'...\n",
      "Committing: ROC Curve\n",
      "Uploading to branch 'exp-tuned_xgb' at path 'results/viz/tuned_xgb_pr.png'...\n",
      "Committing: PR Curve\n",
      "Experiment tuned_xgb complete!\n",
      "\n",
      "=== TOURNAMENT COMPLETE ===\n"
     ]
    }
   ],
   "source": [
    "algorithms = ['lr', 'rf', 'xgb', 'lgbm', 'nn', 'ensemble', 'power_ensemble', 'tuned_xgb']\n",
    "algo_names = {'lr': 'Logistic Regression','rf': 'Random Forest','xgb': 'XGBoost','lgbm': 'LightGBM','nn': 'Neural Network','ensemble': 'Basic Ensemble (All)','power_ensemble': 'Power Ensemble (Tree Models Only)','tuned_xgb': 'XGBoost (Hyperparameter Tuned)'}\n",
    "tournament_results = []\n",
    "\n",
    "for algo in algorithms:\n",
    "    full_name = algo_names[algo]\n",
    "    print(f\"\\n\\n=== Running Experiment: {full_name} ===\")\n",
    "    \n",
    "    # Creating a branch for this model\n",
    "    branch_name = f'exp-{algo}'\n",
    "    handler.create_branch(branch_name, 'main')\n",
    "    \n",
    "    # Train and Evaluate\n",
    "    y_true, y_pred, y_prob = train_and_eval(train_df, test_df, algo=algo)\n",
    "    \n",
    "    if y_pred is not None:\n",
    "        # Generate report as a dictionary to extract numbers\n",
    "        report_dict = classification_report(y_true, y_pred, output_dict=True)\n",
    "        \n",
    "        # Extract F1-score for Class '1' (Fraud)\n",
    "        fraud_f1 = report_dict['1']['f1-score']\n",
    "        tournament_results.append({'Model': full_name, 'Fraud F1-Score': fraud_f1})\n",
    "        \n",
    "        # Save Artifacts\n",
    "        cm_file = save_confusion_matrix(y_true, y_pred, algo)\n",
    "        roc_file = save_roc_curve(y_true, y_prob, algo)\n",
    "        pr_file = save_pr_curve(y_true, y_prob, algo)\n",
    "        \n",
    "        # Uploading Artifacts to LakeFS\n",
    "        print(f\"Uploading artifacts to '{branch_name}'...\")\n",
    "        handler.upload_file(cm_file, branch_name, f'results/viz/{algo}_cm.png', 'Confusion Matrix')\n",
    "        handler.upload_file(roc_file, branch_name, f'results/viz/{algo}_roc.png', 'ROC Curve')\n",
    "        handler.upload_file(pr_file, branch_name, f'results/viz/{algo}_pr.png', 'PR Curve')\n",
    "        \n",
    "        # 5. Cleaning up local files\n",
    "        os.remove(cm_file)\n",
    "        os.remove(roc_file)\n",
    "        os.remove(pr_file)\n",
    "        print(f\"Experiment {algo} complete!\")\n",
    "        \n",
    "print(\"\\n=== TOURNAMENT COMPLETE ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a82ff1",
   "metadata": {},
   "source": [
    "## 6. Phase 3: Results & Leaderboard\n",
    "\n",
    "### **Aim**\n",
    "To aggregate metrics from all 8 branches and identify the superior model based on the **F1-Score for Fraud** (Class 1).\n",
    "\n",
    "### **Conclusion & Inferences**\n",
    "* **Winning Model:** The **Random Forest** model (~0.85 F1) emerged as the top performer. Its ability to handle non-linear decision boundaries allows it to effectively leverage categorical-like features such as the engineered `Hour`.\n",
    "* **Ensemble Strategy:** The **Power Ensemble** (Tree-based only) significantly outperformed the **Basic Ensemble**, confirming that including weaker linear models (like Logistic Regression) dilutes overall performance.\n",
    "* **Model Constraints:** The **Logistic Regression** baseline struggled significantly (F1 ~0.10). This indicates that the decision boundary between \"Fraud\" and \"Legit\" is highly non-linear, especially after introducing synthetic SMOTE examples, which linear models cannot easily separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdc102f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T05:13:03.683252Z",
     "start_time": "2025-12-15T05:13:03.672397Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== üèÜ FINAL TOURNAMENT LEADERBOARD üèÜ ===\n",
      "                               Model  Fraud F1-Score\n",
      "0                      Random Forest        0.849741\n",
      "1                            XGBoost        0.822967\n",
      "2  Power Ensemble (Tree Models Only)        0.821256\n",
      "3               Basic Ensemble (All)        0.780269\n",
      "4     XGBoost (Hyperparameter Tuned)        0.770642\n",
      "5                           LightGBM        0.699187\n",
      "6                     Neural Network        0.588235\n",
      "7                Logistic Regression        0.104651\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n=== üèÜ FINAL TOURNAMENT LEADERBOARD üèÜ ===\")\n",
    "leaderboard_df = pd.DataFrame(tournament_results)\n",
    "leaderboard_df = leaderboard_df.sort_values(by='Fraud F1-Score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(leaderboard_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e6422a",
   "metadata": {},
   "source": [
    "## Upload the Leaderboard to Main Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd90d29f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T05:13:03.733922Z",
     "start_time": "2025-12-15T05:13:03.687269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving Leaderboard to LakeFS (main branch)...\n",
      "Uploading to branch 'main' at path 'results/final_leaderboard.csv'...\n",
      "Committing: Added Final Tournament Leaderboard\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSaving Leaderboard to LakeFS (main branch)...\")\n",
    "csv_buffer = io.StringIO()\n",
    "leaderboard_df.to_csv(csv_buffer, index=False)\n",
    "handler._upload_and_commit(branch='main', path='results/final_leaderboard.csv', content=io.BytesIO(csv_buffer.getvalue().encode('utf-8')), message='Added Final Tournament Leaderboard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf2f9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
