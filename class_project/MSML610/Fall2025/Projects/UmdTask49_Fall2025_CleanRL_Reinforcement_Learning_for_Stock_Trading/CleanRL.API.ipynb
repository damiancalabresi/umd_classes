{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a8d22e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T22:36:53.336883Z",
     "start_time": "2025-12-15T22:36:51.009535Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is demonstration notebook to show the SAC and PPO agents along with SignalTesterEnv registration\n",
    "\n",
    "# rl_env.py: The SignalTesterEnv environment used by the agents\n",
    "# This file now exposes a helper to register a gym id so cleanRL scripts (which expect an env id) can call gym.make(\"SignalTester-v0\")\n",
    "import gymnasium as gym\n",
    "\n",
    "from rl_env import SignalTesterEnv, register_cleanrl_env\n",
    "\n",
    "# Example Registration: (We'll need real data here to run gym.make, the \"SignalTester-v0\" is passed as env_id shown in examples below to sac_continuous_action.py and ppo_continuous_action.py)\n",
    "# register_cleanrl_env(env_id=\"SignalTester-v0\", data=df, news_documents=news)\n",
    "# env = gym.make(\"SignalTester-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1788c23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monkeypatch gymnasium.vector.SyncVectorEnv to handle missing final_observation\n",
    "# This fixes the KeyError: 'final_observation' when using older gymnasium versions or specific wrappers\n",
    "import gymnasium as gym\n",
    "from gymnasium.vector import SyncVectorEnv\n",
    "\n",
    "original_step = SyncVectorEnv.step\n",
    "\n",
    "def patched_step(self, actions):\n",
    "    next_obs, rewards, terminations, truncations, infos = original_step(self, actions)\n",
    "    if \"final_observation\" not in infos:\n",
    "        # Check if any truncation happened\n",
    "        if any(truncations):\n",
    "            # Fallback: use next_obs for final_observation where missing\n",
    "            # Note: next_obs is the reset observation, not the terminal one.\n",
    "            # This is a workaround to prevent crashing.\n",
    "            infos[\"final_observation\"] = next_obs.copy()\n",
    "    return next_obs, rewards, terminations, truncations, infos\n",
    "\n",
    "SyncVectorEnv.step = patched_step\n",
    "print(\"Applied SyncVectorEnv monkeypatch for final_observation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b908a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T22:36:54.147787Z",
     "start_time": "2025-12-15T22:36:53.339680Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example 1: SAC (Soft Actor-Critic) Training\n",
    "from CleanRL_API.sac_continuous_action import train as train_sac, Args as SACArgs\n",
    "\n",
    "# Configure SAC arguments\n",
    "sac_args = SACArgs(\n",
    "    env_id=\"Pendulum-v1\",      # Using standard gym env for demo we'll be using SignalTesterEnv in main CleanRL.example.ipynb\n",
    "    total_timesteps=5000,      # Short run\n",
    "    policy_lr=3e-4,\n",
    "    q_lr=1e-3,\n",
    "    buffer_size=10000,\n",
    "    gamma=0.99,\n",
    "    tau=0.005,\n",
    "    batch_size=256,\n",
    "    learning_starts=1000,\n",
    "    policy_frequency=2,\n",
    "    target_network_frequency=1,\n",
    "    alpha=0.2,\n",
    "    autotune=True,\n",
    "    run_name=\"api_demo_sac\",\n",
    "    seed=42,\n",
    "    hidden_size=64,\n",
    ")\n",
    "\n",
    "print(\"Starting SAC Training...\")\n",
    "# train() returns the actor\n",
    "sac_agent = train_sac(sac_args)\n",
    "print(\"SAC Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7fb41c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T22:38:40.740009Z",
     "start_time": "2025-12-15T22:38:38.281738Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# Example 2: PPO (Proximal Policy Optimization) Training\n",
    "from CleanRL_API.ppo_continuous_action import train as train_ppo, Args as PPOArgs\n",
    "\n",
    "# Configure PPO arguments\n",
    "ppo_args = PPOArgs(\n",
    "    env_id=\"Pendulum-v1\",\n",
    "    total_timesteps=5000,\n",
    "    learning_rate=3e-4,\n",
    "    num_envs=4,                # Vectorized environments for PPO\n",
    "    num_steps=128,\n",
    "    anneal_lr=True,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    num_minibatches=4,\n",
    "    update_epochs=4,\n",
    "    norm_adv=True,\n",
    "    clip_coef=0.2,\n",
    "    clip_vloss=True,\n",
    "    ent_coef=0.01,\n",
    "    vf_coef=0.5,\n",
    "    max_grad_norm=0.5,\n",
    "    target_kl=None,\n",
    "    run_name=\"api_demo_ppo\",\n",
    "    seed=42,\n",
    "    hidden_size=64,\n",
    ")\n",
    "\n",
    "print(\"Starting PPO Training...\")\n",
    "# train() returns the Agent (ActorCritic module)\n",
    "ppo_agent = train_ppo(ppo_args)\n",
    "print(\"PPO Training Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
