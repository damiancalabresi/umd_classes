{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Language Model Training\n",
    "\n",
    "This notebook handles all data preparation steps:\n",
    "1. Download BookCorpus dataset\n",
    "2. Clean and preprocess text\n",
    "3. Tokenize with GPT-2 tokenizer\n",
    "4. Create train/validation splits\n",
    "5. Save preprocessed data for training\n",
    "\n",
    "**Run this notebook once before training to prepare the data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2TokenizerFast\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Configuration\n",
    "DATASET_NAME = \"lucadiliello/bookcorpusopen\"\n",
    "MAX_SEQ_LENGTH = 512\n",
    "VALIDATION_SPLIT = 0.05  # 5% for validation\n",
    "MAX_SAMPLES = None  # Set to int to limit samples (e.g., 10000 for testing)\n",
    "PACK_TO_MAX_LENGTH = True  # Pack tokens into fixed-length blocks (reduces padding)\n",
    "\n",
    "# Output directory for preprocessed data\n",
    "DATA_DIR = Path(\"data/preprocessed/v1\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Cache directory (use scratch on Zaratan)\n",
    "CACHE_DIR = os.environ.get('HF_HOME', os.path.expanduser('~/.cache/huggingface'))\n",
    "print(f\"Cache directory: {CACHE_DIR}\")\n",
    "print(f\"Output directory: {DATA_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load BookCorpus Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading BookCorpus dataset...\")\n",
    "print(\"This may take several minutes on first run (downloading ~7GB)...\")\n",
    "\n",
    "dataset = load_dataset(\n",
    "    DATASET_NAME,\n",
    "    split=\"train\",\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset loaded: {len(dataset):,} samples\")\n",
    "print(f\"Features: {dataset.features}\")\n",
    "print(f\"\\nSample text (first 200 chars):\")\n",
    "print(dataset[0]['text'][:200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Limit Samples (Optional - for testing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAX_SAMPLES is not None and MAX_SAMPLES < len(dataset):\n",
    "    print(f\"Limiting to {MAX_SAMPLES:,} samples for testing...\")\n",
    "    dataset = dataset.select(range(MAX_SAMPLES))\n",
    "    print(f\"Dataset size after limiting: {len(dataset):,} samples\")\n",
    "else:\n",
    "    print(f\"Using full dataset: {len(dataset):,} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Train/Validation Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_idx = int(len(dataset) * (1 - VALIDATION_SPLIT))\n",
    "train_data = dataset.select(range(split_idx))\n",
    "val_data = dataset.select(range(split_idx, len(dataset)))\n",
    "\n",
    "print(f\"Train samples: {len(train_data):,}\")\n",
    "print(f\"Validation samples: {len(val_data):,}\")\n",
    "print(f\"Validation split: {VALIDATION_SPLIT*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading GPT-2 tokenizer...\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2', cache_dir=CACHE_DIR)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have pad token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"Tokenizer vocab size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"Max model length: {tokenizer.model_max_length}\")\n",
    "\n",
    "# Test tokenization\n",
    "test_text = \"Hello, this is a test sentence.\"\n",
    "test_tokens = tokenizer(test_text, return_tensors='pt')\n",
    "print(f\"\\nTest tokenization:\")\n",
    "print(f\"  Text: {test_text}\")\n",
    "print(f\"  Token IDs: {test_tokens['input_ids'][0].tolist()}\")\n",
    "print(f\"  Decoded: {tokenizer.decode(test_tokens['input_ids'][0])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tokenize Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize text samples.\"\"\"\n",
    "    if PACK_TO_MAX_LENGTH:\n",
    "        # Don't pad or truncate - we'll pack into blocks later\n",
    "        return tokenizer(\n",
    "            examples['text'],\n",
    "            truncation=False,\n",
    "            padding=False,\n",
    "            return_attention_mask=False,\n",
    "            return_tensors=None\n",
    "        )\n",
    "    else:\n",
    "        # Standard tokenization with padding/truncation\n",
    "        return tokenizer(\n",
    "            examples['text'],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=MAX_SEQ_LENGTH,\n",
    "            return_tensors=None\n",
    "        )\n",
    "\n",
    "print(\"Tokenizing training data...\")\n",
    "train_tokenized = train_data.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_data.column_names,\n",
    "    desc=\"Tokenizing train\",\n",
    "    num_proc=min(os.cpu_count() or 4, 8)  # Use multiple processes\n",
    ")\n",
    "\n",
    "print(\"\\nTokenizing validation data...\")\n",
    "val_tokenized = val_data.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=val_data.column_names,\n",
    "    desc=\"Tokenizing validation\",\n",
    "    num_proc=min(os.cpu_count() or 4, 8)\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain tokenized: {len(train_tokenized):,} samples\")\n",
    "print(f\"Val tokenized: {len(val_tokenized):,} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pack Tokens into Fixed-Length Blocks (Optional but Recommended)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PACK_TO_MAX_LENGTH:\n",
    "    block_size = MAX_SEQ_LENGTH\n",
    "    \n",
    "    def group_texts(examples):\n",
    "        # Concatenate all texts\n",
    "        import itertools\n",
    "        concatenated = {k: list(itertools.chain.from_iterable(examples[k])) for k in examples.keys()}\n",
    "\n",
    "        total_length = len(concatenated['input_ids'])\n",
    "        # Trim to multiple of block_size\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "        \n",
    "        result = {}\n",
    "        for k, t in concatenated.items():\n",
    "            result[k] = [t[i:i + block_size] for i in range(0, total_length, block_size)]\n",
    "        \n",
    "        # Create attention masks (all ones, no padding inside blocks)\n",
    "        result['attention_mask'] = [[1] * block_size for _ in range(len(result['input_ids']))]\n",
    "        result['labels'] = [seq.copy() for seq in result['input_ids']]\n",
    "        return result\n",
    "    \n",
    "    print(\"Packing training data into fixed-length blocks...\")\n",
    "    train_tokenized = train_tokenized.map(\n",
    "        group_texts,\n",
    "        batched=True,\n",
    "        desc=\"Grouping train into blocks\",\n",
    "        num_proc=max(1, min(os.cpu_count() or 4, 8) // 2)\n",
    "    )\n",
    "    \n",
    "    print(\"\\nPacking validation data into fixed-length blocks...\")\n",
    "    val_tokenized = val_tokenized.map(\n",
    "        group_texts,\n",
    "        batched=True,\n",
    "        desc=\"Grouping val into blocks\",\n",
    "        num_proc=max(1, min(os.cpu_count() or 4, 8) // 2)\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nAfter packing:\")\n",
    "    print(f\"  Train blocks: {len(train_tokenized):,}\")\n",
    "    print(f\"  Val blocks: {len(val_tokenized):,}\")\n",
    "else:\n",
    "    print(\"Skipping token packing (using standard padding/truncation)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Preprocessed Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving preprocessed data...\")\n",
    "\n",
    "# Save as HuggingFace datasets (efficient format)\n",
    "train_tokenized.save_to_disk(str(DATA_DIR / \"train\"))\n",
    "val_tokenized.save_to_disk(str(DATA_DIR / \"val\"))\n",
    "\n",
    "print(f\"\\n✓ Training data saved to: {DATA_DIR / 'train'}\")\n",
    "print(f\"✓ Validation data saved to: {DATA_DIR / 'val'}\")\n",
    "\n",
    "# Save tokenizer info\n",
    "tokenizer.save_pretrained(str(DATA_DIR / \"tokenizer\"))\n",
    "print(f\"✓ Tokenizer saved to: {DATA_DIR / 'tokenizer'}\")\n",
    "\n",
    "# Save metadata\n",
    "import json\n",
    "metadata = {\n",
    "    \"dataset_name\": DATASET_NAME,\n",
    "    \"max_seq_length\": MAX_SEQ_LENGTH,\n",
    "    \"validation_split\": VALIDATION_SPLIT,\n",
    "    \"vocab_size\": tokenizer.vocab_size,\n",
    "    \"train_samples\": len(train_tokenized),\n",
    "    \"val_samples\": len(val_tokenized),\n",
    "    \"pack_to_max_length\": PACK_TO_MAX_LENGTH,\n",
    "    \"tokenizer_type\": \"gpt2\",\n",
    "    \"block_size\": MAX_SEQ_LENGTH\n",
    "}\n",
    "\n",
    "with open(DATA_DIR / \"metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"✓ Metadata saved to: {DATA_DIR / 'metadata.json'}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Data preprocessing complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nYou can now run training with:\")\n",
    "print(f\"  sbatch scripts/train_zaratan.sh\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Verify Saved Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load saved data to verify\n",
    "train_loaded = load_from_disk(str(DATA_DIR / \"train\"))\n",
    "val_loaded = load_from_disk(str(DATA_DIR / \"val\"))\n",
    "\n",
    "print(f\"Train samples loaded: {len(train_loaded):,}\")\n",
    "print(f\"Val samples loaded: {len(val_loaded):,}\")\n",
    "print(f\"\\nSample train item:\")\n",
    "sample = train_loaded[0]\n",
    "print(f\"  Keys: {sample.keys()}\")\n",
    "print(f\"  Input IDs length: {len(sample['input_ids'])}\")\n",
    "print(f\"  Attention mask length: {len(sample['attention_mask'])}\")\n",
    "print(f\"\\nFirst 20 tokens: {sample['input_ids'][:20]}\")\n",
    "print(f\"Decoded: {tokenizer.decode(sample['input_ids'][:20])}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
