{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "104acdd6-f311-44ff-a7de-810b48d6cd9e",
   "metadata": {},
   "source": [
    "# Visual Search Engine using CLIP Embeddings API Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78f0f86-1c70-4405-9741-4a780755ae61",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "84dc8c31-57b0-4924-ab49-f0468903779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "import torch\n",
    "from fastapi import FastAPI, File, Form, UploadFile\n",
    "from fastapi.responses import JSONResponse\n",
    "from PIL import Image\n",
    "from transformers import CLIPModel, CLIPProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b131cd5d-c25a-4edc-99ce-ae6e6fcd1cc7",
   "metadata": {},
   "source": [
    "## FastAPI & Model Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a5c158-e211-4800-9d4b-c45bc91708bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI(title=\"CLIP Embedding API\")\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e1b0c0-3112-48c2-b397-bf5ac082008f",
   "metadata": {},
   "source": [
    "## Exposing APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9623408f-7323-41f2-b438-d07a04f9592e",
   "metadata": {},
   "source": [
    "### Image Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680375d6-1e78-43f7-8db9-72f5324d564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/embed/image\")\n",
    "async def embed_image(image: UploadFile = File(...)):\n",
    "    \"\"\"\n",
    "    Generate CLIP embedding for an uploaded image.\n",
    "\n",
    "    Args:\n",
    "        image: Uploaded image file.\n",
    "\n",
    "    Returns:\n",
    "        JSONResponse containing image filename and embedding vector, or error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        image_bytes = await image.read()\n",
    "        img = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "\n",
    "        inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.get_image_features(**inputs)\n",
    "\n",
    "        embedding = outputs.cpu().tolist()[0]\n",
    "\n",
    "        return JSONResponse({\"image_filename\": image.filename, \"embedding\": embedding})\n",
    "\n",
    "    except Exception as e:\n",
    "        return JSONResponse({\"error\": str(e)}, status_code=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786876f5-f00d-4e03-8446-ef72c445e75e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ead2d67-fcbe-4a3a-83a2-ac08efebbd87",
   "metadata": {},
   "source": [
    "### Text Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ca5e5f-44aa-44e1-8751-c75cd6941b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/embed/text\")\n",
    "async def embed_text(text: str = Form(...)):\n",
    "    \"\"\"\n",
    "    Generate CLIP embedding for input text.\n",
    "\n",
    "    Args:\n",
    "        text: Input text string.\n",
    "\n",
    "    Returns:\n",
    "        JSONResponse containing text and embedding vector, or error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        inputs = processor(text=[text], return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.get_text_features(**inputs)\n",
    "\n",
    "        embedding = outputs.cpu().tolist()[0]\n",
    "\n",
    "        return JSONResponse({\"text\": text, \"embedding\": embedding})\n",
    "\n",
    "    except Exception as e:\n",
    "        return JSONResponse({\"error\": str(e)}, status_code=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303e3bb5-f518-44c0-9a32-114f3e293715",
   "metadata": {},
   "source": [
    "## Exposing the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c134c9e0-0668-4015-bb46-2d2900036cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/\")\n",
    "def root():\n",
    "    \"\"\"\n",
    "    Root endpoint to check API status.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with API status message.\n",
    "    \"\"\"\n",
    "    return {\"message\": \"CLIP embedding API is running!\"}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
