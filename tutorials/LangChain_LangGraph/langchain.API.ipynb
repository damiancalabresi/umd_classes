{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d261ba7",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "## Learn LangChain in 60 Minutes — API notebook\n",
    "\n",
    "Welcome — if you’re new to LangChain/LangGraph, you’re in the right place.\n",
    "\n",
    "This notebook is the **“show me the pieces”** tour: lots of small, runnable snippets that build a mental model.\n",
    "When you’re ready for full workflows (agent loops, graphs, subagents, memory), jump to `langchain.example.ipynb`.\n",
    "\n",
    "**How to use this notebook**\n",
    "- Run top-to-bottom if you can. If you’re skimming, read the markdown and run only the cells you’re curious about.\n",
    "- Some cells call an LLM (that can cost money). If you don’t have API keys set up yet, you can still read everything safely.\n",
    "\n",
    "**Recommended setup (most reproducible)**\n",
    "1) `cd tutorials/LangChain_LangGraph`\n",
    "2) `cp .env.example .env` and fill in your provider + API key(s)\n",
    "3) Either:\n",
    "   - Docker (recommended): `docker compose up --build` → open JupyterLab at `http://localhost:8888/lab`\n",
    "   - Local venv: `pip install -r requirements.txt`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42915b79",
   "metadata": {},
   "source": [
    "## APIs covered (parity with `langchain.example.ipynb`)\n",
    "\n",
    "A mental model before we start:\n",
    "\n",
    "- **LangChain** is the toolkit: prompts, models, tools, and composable building blocks (\"runnables\").\n",
    "- **LangGraph** is the orchestrator: stateful graphs, routing, checkpointing/memory, and **interrupts** for human‑in‑the‑loop (HITL).\n",
    "- **Deep Agents (`deepagents`)** is an optional, higher-level layer used later in this tutorial for “agent app” patterns\n",
    "  (filesystem tools, todos, subagents, sandboxing, and HITL gates).\n",
    "\n",
    "This notebook is a reference for the concrete APIs used in the examples notebook:\n",
    "\n",
    "- Models: `ChatOpenAI`, `ChatAnthropic` (configured via `.env`)\n",
    "- Prompts + LCEL: `ChatPromptTemplate`, `StrOutputParser`, composition with `|`\n",
    "- Runnables: `.invoke()`, `.batch()`, `.stream()`, `RunnableParallel`\n",
    "- Tools: `@tool` / `tool(...)`\n",
    "- Tool execution: `ToolNode`, `AIMessage.tool_calls`, `ToolMessage`\n",
    "- Injection: `InjectedState`, `InjectedStore`, `InMemoryStore`\n",
    "- Agents: `create_agent`, `AgentState`, `ToolRuntime`, `InjectedToolCallId`\n",
    "- LangGraph: `StateGraph`, `START`/`END`, reducers via `Annotated[..., reducer]`\n",
    "- HITL: `interrupt(...)` + `Command(resume=...)`\n",
    "- Deep Agents: `create_deep_agent`, `CompiledSubAgent`, backends, `interrupt_on=InterruptOnConfig(...)`\n",
    "- Notebooks as data: `nbformat`, `nbclient`, `papermill`\n",
    "\n",
    "If any of those names feel mysterious right now — perfect. We’ll introduce them with small examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f853c94",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "We’ll do a tiny bit of setup before the fun parts.\n",
    "If the next cell errors with “No module named …”, that’s not you — it just means you’re not running with this tutorial’s pinned dependencies yet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5787c605",
   "metadata": {},
   "source": [
    "**This cell will:**\n",
    "- Enable auto-reloading so edits are picked up without restarting the kernel.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a912ff98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "\n",
    "def _require_import(module_name: str):\n",
    "    try:\n",
    "        return importlib.import_module(module_name)\n",
    "    except ModuleNotFoundError as e:\n",
    "        raise RuntimeError(\n",
    "            f\"\"\"Missing Python package {module_name!r}.\n",
    "\n",
    "This tutorial is meant to be run from `tutorials/LangChain_LangGraph` with its pinned dependencies.\n",
    "\n",
    "Quick fixes:\n",
    "- Docker (recommended): `cd tutorials/LangChain_LangGraph && docker compose up --build`\n",
    "- Local venv: `cd tutorials/LangChain_LangGraph && pip install -r requirements.txt`\n",
    "\"\"\"\n",
    "        ) from e\n",
    "\n",
    "\n",
    "langchain = _require_import(\"langchain\")\n",
    "langchain_core = _require_import(\"langchain_core\")\n",
    "langgraph = _require_import(\"langgraph\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7115d1",
   "metadata": {},
   "source": [
    "**This cell will:**\n",
    "- Configure logging and print environment/version info for debugging.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "975ebeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 21:53:31,806 INFO learn_langchain.api - python=3.12.12\n",
      "2026-02-05 21:53:31,806 INFO learn_langchain.api - platform=Linux-6.10.14-linuxkit-aarch64-with-glibc2.41\n",
      "2026-02-05 21:53:31,807 INFO learn_langchain.api - langchain=1.2.8\n",
      "2026-02-05 21:53:31,807 INFO learn_langchain.api - langchain_core=1.2.8\n",
      "2026-02-05 21:53:31,807 INFO learn_langchain.api - langgraph=unknown\n",
      "2026-02-05 21:53:31,808 INFO learn_langchain.api - LLM_PROVIDER=openai\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import platform\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(name)s - %(message)s\")\n",
    "_LOG = logging.getLogger(\"learn_langchain.api\")\n",
    "\n",
    "_LOG.info(\"python=%s\", sys.version.split()[0])\n",
    "_LOG.info(\"platform=%s\", platform.platform())\n",
    "_LOG.info(\"langchain=%s\", getattr(langchain, \"__version__\", \"unknown\"))\n",
    "_LOG.info(\"langchain_core=%s\", getattr(langchain_core, \"__version__\", \"unknown\"))\n",
    "_LOG.info(\"langgraph=%s\", getattr(langgraph, \"__version__\", \"unknown\"))\n",
    "_LOG.info(\"LLM_PROVIDER=%s\", os.getenv(\"LLM_PROVIDER\", \"(unset)\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9bd932",
   "metadata": {},
   "source": [
    "## Model (configured via `.env`)\n",
    "\n",
    "These notebooks are provider-agnostic: you pick a provider in `.env`, and the helper function builds the right chat model.\n",
    "\n",
    "What you’ll do once:\n",
    "- Copy `.env.example` → `.env`\n",
    "- Set `LLM_PROVIDER` (e.g. `openai` or `anthropic`)\n",
    "- Set the matching API key (`OPENAI_API_KEY` or `ANTHROPIC_API_KEY`)\n",
    "\n",
    "If you don’t want to spend money while reading, you can skip the LLM-invoking cells — the markdown still explains what they’re doing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3e6526",
   "metadata": {},
   "source": [
    "**This cell will:**\n",
    "- Define a small `.env`-driven factory to create the chat model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1305dbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class LlmConfig:\n",
    "    \"\"\"\n",
    "    Configuration for selecting an LLM provider + model from environment variables.\n",
    "    \"\"\"\n",
    "\n",
    "    provider: str\n",
    "    model: str\n",
    "    temperature: float\n",
    "\n",
    "\n",
    "def _require_env(var_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Return the value of `var_name` from environment variables or raise.\n",
    "    \"\"\"\n",
    "    value = os.getenv(var_name)\n",
    "    if not value:\n",
    "        raise RuntimeError(f\"Missing required environment variable `{var_name}`. See `.env.example`.\")\n",
    "    return value\n",
    "\n",
    "\n",
    "def load_llm_config() -> LlmConfig:\n",
    "    \"\"\"\n",
    "    Load `LlmConfig` from environment variables.\n",
    "    \"\"\"\n",
    "    provider = _require_env(\"LLM_PROVIDER\").lower()\n",
    "    temperature = float(os.getenv(\"LLM_TEMPERATURE\", \"0\"))\n",
    "\n",
    "    default_models = {\n",
    "        \"openai\": \"gpt-4.1-mini\",\n",
    "        \"anthropic\": \"claude-3-5-sonnet-latest\",\n",
    "    }\n",
    "    model = os.getenv(\"LLM_MODEL\", default_models.get(provider, \"\"))\n",
    "    if not model:\n",
    "        raise RuntimeError(f\"Missing `LLM_MODEL` for provider={provider!r}. See `.env.example`.\")\n",
    "\n",
    "    cfg = LlmConfig(provider=provider, model=model, temperature=temperature)\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def get_chat_model():\n",
    "    \"\"\"\n",
    "    Create a tool-calling-capable chat model using env configuration.\n",
    "    \"\"\"\n",
    "    cfg = load_llm_config()\n",
    "\n",
    "    if cfg.provider == \"openai\":\n",
    "        from langchain_openai import ChatOpenAI\n",
    "\n",
    "        _require_env(\"OPENAI_API_KEY\")\n",
    "        model = ChatOpenAI(\n",
    "            model=cfg.model,\n",
    "            temperature=cfg.temperature,\n",
    "            timeout=60,\n",
    "            max_retries=2,\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    if cfg.provider == \"anthropic\":\n",
    "        from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "        _require_env(\"ANTHROPIC_API_KEY\")\n",
    "        model = ChatAnthropic(\n",
    "            model=cfg.model,\n",
    "            temperature=cfg.temperature,\n",
    "            timeout=60,\n",
    "            max_retries=2,\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    raise ValueError(\n",
    "        f\"Unsupported `LLM_PROVIDER={cfg.provider}`. Use one of: openai, anthropic.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82117131",
   "metadata": {},
   "source": [
    "**This cell will:**\n",
    "- Instantiate the chat model from your `.env` configuration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3e55d3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(profile={'max_input_tokens': 1047576, 'max_output_tokens': 32768, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0xffff477018e0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0xffff46174b90>, root_client=<openai.OpenAI object at 0xffff44fe5d90>, root_async_client=<openai.AsyncOpenAI object at 0xffff477017c0>, model_name='gpt-4.1-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), request_timeout=60.0, stream_usage=True, max_retries=2)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = get_chat_model()\n",
    "llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac82a9ef",
   "metadata": {},
   "source": [
    "## Local dataset (`data/T1_slice.csv`)\n",
    "\n",
    "We’ll use a tiny local CSV (shipped with this tutorial) so the examples feel concrete.\n",
    "\n",
    "Two small conveniences happen in the next cell:\n",
    "- we load it into a Pandas DataFrame for prompt/context demos\n",
    "- we also copy it under `./workspace/data/` so filesystem tools can refer to it as `/workspace/data/T1_slice.csv`\n",
    "\n",
    "That “workspace” detail will matter once we get to sandboxed filesystem access.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18a5cad",
   "metadata": {},
   "source": [
    "**This cell will:**\n",
    "- Load the local dataset into a Pandas DataFrame and prepare the time column.\n",
    "- Copy the dataset under `./workspace/data/` so Deep Agents can access it via `/workspace/...`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "58128762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date/Time</th>\n",
       "      <th>LV ActivePower (kW)</th>\n",
       "      <th>Wind Speed (m/s)</th>\n",
       "      <th>Theoretical_Power_Curve (KWh)</th>\n",
       "      <th>Wind Direction (°)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>380.047791</td>\n",
       "      <td>5.311336</td>\n",
       "      <td>416.328908</td>\n",
       "      <td>259.994904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01 00:10:00</td>\n",
       "      <td>453.769196</td>\n",
       "      <td>5.672167</td>\n",
       "      <td>519.917511</td>\n",
       "      <td>268.641113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01 00:20:00</td>\n",
       "      <td>306.376587</td>\n",
       "      <td>5.216037</td>\n",
       "      <td>390.900016</td>\n",
       "      <td>272.564789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01 00:30:00</td>\n",
       "      <td>419.645905</td>\n",
       "      <td>5.659674</td>\n",
       "      <td>516.127569</td>\n",
       "      <td>271.258087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01 00:40:00</td>\n",
       "      <td>380.650696</td>\n",
       "      <td>5.577941</td>\n",
       "      <td>491.702972</td>\n",
       "      <td>265.674286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date/Time  LV ActivePower (kW)  Wind Speed (m/s)  \\\n",
       "0 2018-01-01 00:00:00           380.047791          5.311336   \n",
       "1 2018-01-01 00:10:00           453.769196          5.672167   \n",
       "2 2018-01-01 00:20:00           306.376587          5.216037   \n",
       "3 2018-01-01 00:30:00           419.645905          5.659674   \n",
       "4 2018-01-01 00:40:00           380.650696          5.577941   \n",
       "\n",
       "   Theoretical_Power_Curve (KWh)  Wind Direction (°)  \n",
       "0                     416.328908          259.994904  \n",
       "1                     519.917511          268.641113  \n",
       "2                     390.900016          272.564789  \n",
       "3                     516.127569          271.258087  \n",
       "4                     491.702972          265.674286  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "DATASET_PATH = Path(\"data/T1_slice.csv\").resolve()\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "TIME_COL = \"Date/Time\"\n",
    "if TIME_COL in df.columns:\n",
    "    df[TIME_COL] = pd.to_datetime(df[TIME_COL], format=\"%d %m %Y %H:%M\", errors=\"coerce\")\n",
    "\n",
    "# Make the dataset visible to Deep Agents filesystem tools under `/workspace/...`.\n",
    "WORKSPACE_DIR = Path(\"workspace\").resolve()\n",
    "WORKSPACE_DATA_DIR = WORKSPACE_DIR / \"data\"\n",
    "WORKSPACE_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "WORKSPACE_DATASET_PATH = WORKSPACE_DATA_DIR / \"T1_slice.csv\"\n",
    "if not WORKSPACE_DATASET_PATH.exists():\n",
    "    shutil.copyfile(str(DATASET_PATH), str(WORKSPACE_DATASET_PATH))\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004cb4ec",
   "metadata": {},
   "source": [
    "**This cell will:**\n",
    "- Build compact, JSON-serializable metadata and sample rows to pass into prompts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "55199956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': 'data/T1_slice.csv',\n",
       " 'workspace_path': 'workspace/data/T1_slice.csv',\n",
       " 'tool_path': '/workspace/data/T1_slice.csv',\n",
       " 'n_rows': 100,\n",
       " 'n_cols': 5,\n",
       " 'columns': ['Date/Time',\n",
       "  'LV ActivePower (kW)',\n",
       "  'Wind Speed (m/s)',\n",
       "  'Theoretical_Power_Curve (KWh)',\n",
       "  'Wind Direction (°)'],\n",
       " 'dtypes': {'Date/Time': 'datetime64[ns]',\n",
       "  'LV ActivePower (kW)': 'float64',\n",
       "  'Wind Speed (m/s)': 'float64',\n",
       "  'Theoretical_Power_Curve (KWh)': 'float64',\n",
       "  'Wind Direction (°)': 'float64'},\n",
       " 'sample_rows': [{'Date/Time': Timestamp('2018-01-01 00:00:00'),\n",
       "   'LV ActivePower (kW)': 380.047790527343,\n",
       "   'Wind Speed (m/s)': 5.31133604049682,\n",
       "   'Theoretical_Power_Curve (KWh)': 416.328907824861,\n",
       "   'Wind Direction (°)': 259.994903564453},\n",
       "  {'Date/Time': Timestamp('2018-01-01 00:10:00'),\n",
       "   'LV ActivePower (kW)': 453.76919555664,\n",
       "   'Wind Speed (m/s)': 5.67216682434082,\n",
       "   'Theoretical_Power_Curve (KWh)': 519.917511061494,\n",
       "   'Wind Direction (°)': 268.64111328125},\n",
       "  {'Date/Time': Timestamp('2018-01-01 00:20:00'),\n",
       "   'LV ActivePower (kW)': 306.376586914062,\n",
       "   'Wind Speed (m/s)': 5.21603679656982,\n",
       "   'Theoretical_Power_Curve (KWh)': 390.900015810951,\n",
       "   'Wind Direction (°)': 272.564788818359}],\n",
       " 'time_col': 'Date/Time',\n",
       " 'freq': '0 days 00:10:00'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_dataset_meta(df) -> dict:\n",
    "    '''\n",
    "    Build a compact JSON-serializable dataset metadata dict for demos.\n",
    "    '''\n",
    "    cols = list(df.columns)\n",
    "    dtypes = {c: str(df[c].dtype) for c in cols}\n",
    "    sample_rows = df.head(3).to_dict(orient=\"records\")\n",
    "    freq = None\n",
    "    if \"Date/Time\" in df.columns:\n",
    "        ts = df[\"Date/Time\"].dropna().sort_values()\n",
    "        if len(ts) >= 3:\n",
    "            # Estimate the most common sampling delta.\n",
    "            deltas = ts.diff().dropna()\n",
    "            freq = str(deltas.value_counts().idxmax())\n",
    "    meta = {\n",
    "        \"path\": \"data/T1_slice.csv\",\n",
    "        \"workspace_path\": \"workspace/data/T1_slice.csv\",\n",
    "        \"tool_path\": \"/workspace/data/T1_slice.csv\",\n",
    "        \"n_rows\": int(df.shape[0]),\n",
    "        \"n_cols\": int(df.shape[1]),\n",
    "        \"columns\": cols,\n",
    "        \"dtypes\": dtypes,\n",
    "        \"sample_rows\": sample_rows,\n",
    "        \"time_col\": \"Date/Time\" if \"Date/Time\" in df.columns else None,\n",
    "        \"freq\": freq,\n",
    "    }\n",
    "    return meta\n",
    "\n",
    "DATASET_META = build_dataset_meta(df)\n",
    "DATASET_META\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e77118",
   "metadata": {},
   "source": [
    "## LCEL: prompt | model | parser\n",
    "\n",
    "LCEL (LangChain Expression Language) is a **pipe** syntax for composing steps.\n",
    "If you’ve used Unix pipes (`a | b | c`), it’s the same vibe:\n",
    "\n",
    "- build a prompt\n",
    "- call a model\n",
    "- parse the result\n",
    "\n",
    "Key pieces in this notebook:\n",
    "- `ChatPromptTemplate` (how we structure instructions + user input)\n",
    "- `StrOutputParser` (turn a chat message into a plain string)\n",
    "- composition with `|` (build a reusable “chain”)\n",
    "\n",
    "As you run the next cell, focus on the *shape* of the pipeline more than the exact prompt wording.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfa3516",
   "metadata": {},
   "source": [
    "**This cell will:**\n",
    "- Demonstrate a small part of the API surface used in the examples notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "851e7be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 21:53:32,953 INFO httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LCEL (Low-Cost Embedded Linux) is a lightweight, cost-effective Linux distribution designed for embedded systems with limited resources.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a concise tutor. Answer clearly.\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "chain.invoke({\"question\": \"Explain LCEL in one sentence.\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5440be",
   "metadata": {},
   "source": [
    "## Runnables: invoke / batch / stream / RunnableParallel\n",
    "\n",
    "A “runnable” is anything you can **call**.\n",
    "LangChain standardizes that with a few common methods:\n",
    "\n",
    "- `.invoke(input)` → one input, one output\n",
    "- `.batch([inputs])` → many inputs at once (often more efficient)\n",
    "- `.stream(input)` → yield partial outputs as they arrive\n",
    "- `RunnableParallel(...)` → run independent chains side-by-side and combine the results\n",
    "\n",
    "When you’re learning, it helps to treat runnables like functions — except they can be composed and configured.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9e6630",
   "metadata": {},
   "source": [
    "**This cell will:**\n",
    "- Demonstrate a small part of the API surface used in the examples notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "39e8ad06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 21:53:34,301 INFO httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-05 21:53:35,746 INFO httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'summary': '- LangChain offers modular components designed for building applications with large language models (LLMs).  \\n- It enables easy composition and integration of various functionalities to create complex LLM-powered apps.  \\n- The framework simplifies development by providing reusable building blocks tailored for LLM workflows.',\n",
       " 'risks': 'Here are three risks or caveats to consider when using LangChain for building LLM applications:\\n\\n1. **Complexity and Learning Curve**: While LangChain offers powerful composable components, effectively integrating and orchestrating these building blocks can be complex, especially for developers new to LLMs or the framework itself. This may lead to longer development times or suboptimal implementations.\\n\\n2. **Dependency on External APIs and Models**: LangChain often relies on external language models and APIs (e.g., OpenAI, Hugging Face). This introduces risks related to API availability, rate limits, cost, and potential changes in API behavior that can affect your application’s stability and performance.\\n\\n3. **Data Privacy and Security**: When using LangChain to process sensitive or proprietary data through third-party LLM services, there are inherent privacy and security risks. Ensuring compliance with data protection regulations and safeguarding user data requires careful design and possibly additional infrastructure.'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You write crisp summaries.\"),\n",
    "    (\"human\", \"Summarize in 3 bullets:\\n\\n{text}\"),\n",
    "])\n",
    "risks_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You list caveats.\"),\n",
    "    (\"human\", \"List 3 risks/caveats:\\n\\n{text}\"),\n",
    "])\n",
    "\n",
    "summary_chain = summary_prompt | llm | StrOutputParser()\n",
    "risks_chain = risks_prompt | llm | StrOutputParser()\n",
    "\n",
    "parallel = RunnableParallel(summary=summary_chain, risks=risks_chain)\n",
    "parallel.invoke({\"text\": \"LangChain provides composable building blocks for LLM apps.\"}, config={\"max_concurrency\": 2})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80280f24",
   "metadata": {},
   "source": [
    "**This cell will:**\n",
    "- Use `ToolNode` to execute tool calls inside a graph.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "97746a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 21:53:37,079 INFO httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-05 21:53:37,251 INFO httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-05 21:53:37,570 INFO httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['In LangChain, a **tool** is a component that an agent can use to interact with external systems or perform specific actions, such as querying a database, calling an API, or running a custom function. Tools enable agents to extend their capabilities beyond just language understanding and generation.',\n",
       " 'In LangGraph, a **ToolNode** is a type of node that represents an external tool or API integrated into the graph. It allows the graph to interact with and utilize functionalities from external services or tools within the LangGraph workflow.',\n",
       " '`InjectedState` is a pattern or utility (commonly in state management libraries) that allows you to inject or provide state into a component or function from an external source. It helps in managing and sharing state across different parts of an application without tightly coupling components.\\n\\nIf you specify the context or framework (e.g., React, MobX, etc.), I can give a more precise explanation.']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = [\n",
    "    {\"question\": \"What is a tool in LangChain?\"},\n",
    "    {\"question\": \"What is ToolNode in LangGraph?\"},\n",
    "    {\"question\": \"What does InjectedState do?\"},\n",
    "]\n",
    "chain.batch(questions, return_exceptions=True, config={\"max_concurrency\": 3})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db57cecb",
   "metadata": {},
   "source": [
    "**This cell will:**\n",
    "- Demonstrate a small part of the API surface used in the examples notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a7260952",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 21:53:38,287 INFO httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'- **RunnableParallel** is a programming construct that allows multiple tasks or threads to run concurrently, improving performance by utilizing parallel execution.  \\n- It manages the coordination and synchronization of these parallel tasks to ensure correct and efficient completion.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = []\n",
    "for chunk in chain.stream({\"question\": \"Give me a 2-bullet explanation of RunnableParallel.\"}):\n",
    "    chunks.append(chunk)\n",
    "final = \"\".join(chunks)\n",
    "final[:300] + (\"...\" if len(final) > 300 else \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec595b6",
   "metadata": {},
   "source": [
    "## Tools: `@tool` + ToolNode execution\n",
    "\n",
    "A *tool* is a normal Python function with a schema.\n",
    "The LLM can “ask” to call a tool (with arguments), and your code executes it.\n",
    "\n",
    "Two ways you’ll see tools used:\n",
    "\n",
    "1) **Directly** (call the function yourself)\n",
    "2) **Inside a graph** via `ToolNode` (LangGraph executes any requested tool calls and feeds results back)\n",
    "\n",
    "If you’re new: don’t worry about the message formats yet. Focus on the story:\n",
    "\"model asks for tool\" → \"we run tool\" → \"tool returns data\" → \"model continues\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d243b46f",
   "metadata": {},
   "source": [
    "**This cell will:**\n",
    "- Build and compile a `StateGraph` (a small LangGraph workflow).\n",
    "- Use `ToolNode` to execute tool calls inside a graph.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4468a17b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AIMessage:', 'ToolMessage:2.5', 'ToolMessage:0.7071067811865488']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sqrt as _sqrt\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import AIMessage\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "def _as_floats(xs: Sequence[float]) -> list[float]:\n",
    "    \"\"\"Validate `xs` and return it as a non-empty `list[float]`.\"\"\"\n",
    "    if xs is None:\n",
    "        raise ValueError(\"xs must not be None\")\n",
    "    xs_list = [float(x) for x in xs]\n",
    "    if len(xs_list) == 0:\n",
    "        raise ValueError(\"xs must be non-empty\")\n",
    "    return xs_list\n",
    "\n",
    "@tool\n",
    "def mean(xs: Sequence[float]) -> float:\n",
    "    \"\"\"Compute the arithmetic mean of a non-empty list of numbers.\"\"\"\n",
    "    xs_list = _as_floats(xs)\n",
    "    return sum(xs_list) / len(xs_list)\n",
    "\n",
    "@tool\n",
    "def zscore(xs: Sequence[float], x: float) -> float:\n",
    "    \"\"\"Compute z = (x - mean(xs)) / std(xs).\"\"\"\n",
    "    xs_list = _as_floats(xs)\n",
    "    mu = sum(xs_list) / len(xs_list)\n",
    "    var = sum((v - mu) ** 2 for v in xs_list) / len(xs_list)\n",
    "    std = _sqrt(var)\n",
    "    if std == 0.0:\n",
    "        raise ValueError(\"std(xs) is 0; z-score undefined for constant sample\")\n",
    "    return (float(x) - mu) / std\n",
    "\n",
    "class ToolState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "tool_node = ToolNode([mean, zscore])\n",
    "\n",
    "g = StateGraph(ToolState)\n",
    "g.add_node(\"tools\", tool_node)\n",
    "g.add_edge(START, \"tools\")\n",
    "g.add_edge(\"tools\", END)\n",
    "graph = g.compile()\n",
    "\n",
    "tool_calls = [\n",
    "    {\"name\": \"mean\", \"args\": {\"xs\": [1, 2, 3, 4]}, \"id\": \"t1\", \"type\": \"tool_call\"},\n",
    "    {\"name\": \"zscore\", \"args\": {\"xs\": [9, 10, 10], \"x\": 10}, \"id\": \"t2\", \"type\": \"tool_call\"},  # error (std=0)\n",
    "]\n",
    "\n",
    "out = graph.invoke({\"messages\": [AIMessage(content=\"\", tool_calls=tool_calls)]})\n",
    "[type(m).__name__ + \":\" + (getattr(m, \"content\", \"\")[:80]) for m in out[\"messages\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737a3dea",
   "metadata": {},
   "source": [
    "## InjectedState: runtime-only args (system-owned)\n",
    "\n",
    "Sometimes a tool needs access to *system-owned* context that the model shouldn’t be allowed to spoof.\n",
    "\n",
    "`InjectedState` is the pattern for that:\n",
    "- your tool signature includes an injected parameter\n",
    "- LangGraph supplies it at runtime (not from the model’s JSON arguments)\n",
    "\n",
    "Think of it like dependency injection:\n",
    "- model controls: normal tool inputs\n",
    "- system controls: injected inputs (state, stores, call IDs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8826f94",
   "metadata": {},
   "source": [
    "**This cell will:**\n",
    "- Build and compile a `StateGraph` (a small LangGraph workflow).\n",
    "- Use `ToolNode` to execute tool calls inside a graph.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a650bb64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What columns exist and what is the sampling frequency?',\n",
       " 'n_rows': 100,\n",
       " 'n_cols': 5,\n",
       " 'columns': ['Date/Time',\n",
       "  'LV ActivePower (kW)',\n",
       "  'Wind Speed (m/s)',\n",
       "  'Theoretical_Power_Curve (KWh)',\n",
       "  'Wind Direction (°)'],\n",
       " 'dtypes': {'Date/Time': 'datetime64[ns]',\n",
       "  'LV ActivePower (kW)': 'float64',\n",
       "  'Wind Speed (m/s)': 'float64',\n",
       "  'Theoretical_Power_Curve (KWh)': 'float64',\n",
       "  'Wind Direction (°)': 'float64'},\n",
       " 'time_col': 'Date/Time',\n",
       " 'freq': '0 days 00:10:00'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from typing_extensions import Annotated as TxAnnotated\n",
    "from langgraph.prebuilt import InjectedState\n",
    "\n",
    "@tool\n",
    "def dataset_brief(\n",
    "    question: str,\n",
    "    dataset_meta: TxAnnotated[dict, InjectedState(\"dataset_meta\")],\n",
    ") -> str:\n",
    "    \"Answer a question using injected dataset metadata (InjectedState).\"\n",
    "    payload = {\n",
    "        \"question\": question,\n",
    "        \"n_rows\": dataset_meta.get(\"n_rows\"),\n",
    "        \"n_cols\": dataset_meta.get(\"n_cols\"),\n",
    "        \"columns\": dataset_meta.get(\"columns\"),\n",
    "        \"dtypes\": dataset_meta.get(\"dtypes\"),\n",
    "        \"time_col\": dataset_meta.get(\"time_col\"),\n",
    "        \"freq\": dataset_meta.get(\"freq\"),\n",
    "    }\n",
    "    return json.dumps(payload)\n",
    "\n",
    "class InjectedStateState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    dataset_meta: dict\n",
    "\n",
    "tool_node = ToolNode([dataset_brief])\n",
    "g = StateGraph(InjectedStateState)\n",
    "g.add_node(\"tools\", tool_node)\n",
    "g.add_edge(START, \"tools\")\n",
    "g.add_edge(\"tools\", END)\n",
    "graph = g.compile()\n",
    "\n",
    "state_in: InjectedStateState = {\n",
    "    \"dataset_meta\": DATASET_META,\n",
    "    \"messages\": [\n",
    "        AIMessage(\n",
    "            content=\"\",\n",
    "            tool_calls=[\n",
    "                {\"name\": \"dataset_brief\", \"args\": {\"question\": \"What columns exist and what is the sampling frequency?\"}, \"id\": \"t1\", \"type\": \"tool_call\"}\n",
    "            ],\n",
    "        )\n",
    "    ],\n",
    "}\n",
    "out = graph.invoke(state_in)\n",
    "json.loads(out[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9e2f9d",
   "metadata": {},
   "source": [
    "## InjectedStore: injected persistent store handle\n",
    "\n",
    "A store is a place to keep small bits of information across calls (like preferences, cached results, or “facts we’ve already extracted”).\n",
    "\n",
    "`InjectedStore` lets a tool receive a store handle **without** the model being able to fabricate it.\n",
    "\n",
    "In this tutorial we use `InMemoryStore` for simplicity, but the pattern generalizes to other persistence layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca26b492",
   "metadata": {},
   "source": [
    "**This cell will:**\n",
    "- Build and compile a `StateGraph` (a small LangGraph workflow).\n",
    "- Use `ToolNode` to execute tool calls inside a graph.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0daee425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('saved freq_hint=1min for user_id=u1', '1min')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing_extensions import Annotated as TxAnnotated\n",
    "from langgraph.prebuilt import InjectedStore\n",
    "from langgraph.store.base import BaseStore\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "@tool\n",
    "def save_pref(user_id: str, key: str, value: str, store: TxAnnotated[BaseStore, InjectedStore()]) -> str:\n",
    "    \"Save a user preference (key/value) into an injected store.\"\n",
    "    namespace = (\"prefs\", user_id)\n",
    "    store.put(namespace, key, {\"value\": value})\n",
    "    return f\"saved {key}={value} for user_id={user_id}\"\n",
    "\n",
    "@tool\n",
    "def load_pref(user_id: str, key: str, store: TxAnnotated[BaseStore, InjectedStore()]) -> str:\n",
    "    \"Load a user preference (key) from an injected store.\"\n",
    "    namespace = (\"prefs\", user_id)\n",
    "    item = store.get(namespace, key)\n",
    "    if not item:\n",
    "        return f\"(missing) {key}\"\n",
    "    return str(item.value.get(\"value\", f\"(missing) {key}\"))\n",
    "\n",
    "class StoreState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "store = InMemoryStore()\n",
    "tool_node = ToolNode([save_pref, load_pref])\n",
    "g = StateGraph(StoreState)\n",
    "g.add_node(\"tools\", tool_node)\n",
    "g.add_edge(START, \"tools\")\n",
    "g.add_edge(\"tools\", END)\n",
    "graph = g.compile(store=store)\n",
    "\n",
    "out1 = graph.invoke({\"messages\": [AIMessage(content=\"\", tool_calls=[{\"name\": \"save_pref\", \"args\": {\"user_id\": \"u1\", \"key\": \"freq_hint\", \"value\": \"1min\"}, \"id\": \"t1\", \"type\": \"tool_call\"}]) ]})\n",
    "out2 = graph.invoke({\"messages\": [AIMessage(content=\"\", tool_calls=[{\"name\": \"load_pref\", \"args\": {\"user_id\": \"u1\", \"key\": \"freq_hint\"}, \"id\": \"t2\", \"type\": \"tool_call\"}]) ]})\n",
    "out1[\"messages\"][-1].content, out2[\"messages\"][-1].content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec1f7ab",
   "metadata": {},
   "source": [
    "## Agent APIs used in `langchain.example.ipynb`\n",
    "\n",
    "An *agent* is a loop: the model looks at the conversation + available tools, chooses an action, and repeats until it’s done.\n",
    "\n",
    "In this tutorial we use a helper, `create_agent(...)`, to build a tool-calling agent quickly.\n",
    "Later, in the examples notebook, you’ll see the same ideas expressed as explicit LangGraph loops.\n",
    "\n",
    "If you ever feel confused, this heuristic helps:\n",
    "- **LangChain agent helpers** get you started fast.\n",
    "- **LangGraph** is what you reach for when you want full control (state, routing, memory, HITL).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0706e1f3",
   "metadata": {},
   "source": [
    "**This cell will:**\n",
    "- Create a tool-calling agent using `create_agent(...)`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "34d70fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 21:53:39,420 INFO httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-05 21:53:40,332 INFO httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('HumanMessage', 'Call utc_now and return the exact value.'),\n",
       " ('AIMessage', ''),\n",
       " ('ToolMessage', '2026-02-05T21:53:39.427501+00:00'),\n",
       " ('AIMessage', 'The current UTC time is 2026-02-05T21:53:39.427501+00:00.')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "from langchain.agents import create_agent\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def utc_now() -> str:\n",
    "    \"\"\"Return the current UTC time as an ISO string.\"\"\"\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[utc_now],\n",
    "    system_prompt=\"Use tools when a tool can answer the question more reliably than guessing.\",\n",
    ")\n",
    "out = agent.invoke({\"messages\": [HumanMessage(content=\"Call utc_now and return the exact value.\")]} )\n",
    "[(type(m).__name__, getattr(m, \"content\", \"\")[:120]) for m in out[\"messages\"]][-4:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d06ace",
   "metadata": {},
   "source": [
    "## Advanced agent tool plumbing: `AgentState`, `ToolRuntime`, `InjectedToolCallId`\n",
    "\n",
    "This section is here for when you’re ready to peek “under the hood”.\n",
    "\n",
    "The high-level story:\n",
    "- tool calls happen inside a conversation\n",
    "- each tool call has an ID\n",
    "- LangGraph/LangChain pass runtime helpers so tools can update state and emit the right `ToolMessage`\n",
    "\n",
    "If it feels advanced on a first read, that’s normal — the goal is to make the concepts *available*, not to memorize them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8217baef",
   "metadata": {},
   "source": [
    "**This cell will:**\n",
    "- Create a tool-calling agent using `create_agent(...)`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8a1ca960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 21:53:41,313 INFO httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-05 21:53:42,178 INFO httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'facts': ['tone=formal', 'n_chars=30'],\n",
       " 'last': 'The text states that LangGraph supports interrupts.'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from typing_extensions import Annotated as TxAnnotated\n",
    "\n",
    "from langchain.agents import AgentState, create_agent\n",
    "from langchain.tools import InjectedToolCallId, ToolRuntime\n",
    "from langchain.tools import tool as lc_tool\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langgraph.types import Command\n",
    "\n",
    "class CustomState(AgentState):\n",
    "    user_prefs: dict\n",
    "    facts: list[str]\n",
    "\n",
    "@lc_tool(\"extract_facts\", description=\"Extract 2 facts, store them in state, and emit a ToolMessage.\")\n",
    "def extract_facts(\n",
    "    text: str,\n",
    "    tool_call_id: TxAnnotated[str, InjectedToolCallId],\n",
    "    runtime: ToolRuntime[None, CustomState],\n",
    ") -> Command:\n",
    "    \"\"\"Extract simple facts and update the graph state via `Command(update=...)`.\"\"\"\n",
    "    tone = runtime.state.get(\"user_prefs\", {}).get(\"tone\", \"neutral\")\n",
    "    facts = [f\"tone={tone}\", f\"n_chars={len(text)}\"]\n",
    "    return Command(\n",
    "        update={\n",
    "            \"facts\": facts,\n",
    "            \"messages\": [ToolMessage(content=json.dumps({\"facts\": facts}), tool_call_id=tool_call_id)],\n",
    "        }\n",
    "    )\n",
    "\n",
    "supervisor = create_agent(\n",
    "    llm,\n",
    "    tools=[extract_facts],\n",
    "    system_prompt=\"First call extract_facts, then summarize the returned facts.\",\n",
    "    state_schema=CustomState,\n",
    ")\n",
    "\n",
    "state = supervisor.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Text: LangGraph supports interrupts.\"}], \"user_prefs\": {\"tone\": \"formal\"}, \"facts\": []}\n",
    ")\n",
    "{\"facts\": state.get(\"facts\"), \"last\": getattr(state[\"messages\"][-1], \"content\", \"\")[:160]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497b5d5d",
   "metadata": {},
   "source": [
    "## Human-in-the-loop building block: `interrupt(...)` + resume\n",
    "\n",
    "Sometimes an agent should *pause* and ask a human before doing something risky:\n",
    "- deleting a file\n",
    "- sending an email\n",
    "- running a trade\n",
    "- making an irreversible change\n",
    "\n",
    "LangGraph’s low-level building block for this is `interrupt(value)`:\n",
    "\n",
    "- The first time a node calls `interrupt(...)`, execution **stops** and the graph returns an `__interrupt__` payload.\n",
    "- To continue, you call the graph again with `Command(resume=...)`.\n",
    "- When the graph resumes, the node is **re-executed**, and `interrupt(...)` returns the human’s choice.\n",
    "\n",
    "In the next cell we create a tiny file in `tmp_runs/hitl/` and only delete it if the human approves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eceb61c",
   "metadata": {},
   "source": [
    "**This cell will:**\n",
    "- Build and compile a `StateGraph` (a small LangGraph workflow).\n",
    "- Demonstrate human-in-the-loop control using `interrupt(...)` and resume.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "751e53c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pending': {'action': 'delete_file',\n",
       "  'target_path': '/app/tmp_runs/hitl/victim.txt',\n",
       "  'message': 'Approve deletion?'},\n",
       " 'victim_exists_after': False}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import Literal, TypedDict\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from langgraph.types import Command, interrupt\n",
    "\n",
    "class HITLState(TypedDict):\n",
    "    target_path: str\n",
    "    decision: Literal[\"approve\", \"reject\", \"\"]\n",
    "\n",
    "def propose_delete(state: HITLState) -> dict:\n",
    "    \"\"\"Ask for approval to delete a file.\"\"\"\n",
    "\n",
    "    payload = {\"action\": \"delete_file\", \"target_path\": state[\"target_path\"], \"message\": \"Approve deletion?\"}\n",
    "    decision = interrupt(payload)\n",
    "    return {\"decision\": decision}\n",
    "\n",
    "def do_delete(state: HITLState) -> dict:\n",
    "    \"\"\"Delete the file if approved.\"\"\"\n",
    "\n",
    "    if state[\"decision\"] != \"approve\":\n",
    "        return {}\n",
    "    p = Path(state[\"target_path\"])\n",
    "\n",
    "    if p.exists() and p.is_file():\n",
    "        p.unlink()\n",
    "    return {}\n",
    "\n",
    "builder = StateGraph(HITLState)\n",
    "builder.add_node(\"propose\", propose_delete)\n",
    "builder.add_node(\"delete\", do_delete)\n",
    "builder.add_edge(START, \"propose\")\n",
    "builder.add_edge(\"propose\", \"delete\")\n",
    "builder.add_edge(\"delete\", END)\n",
    "graph = builder.compile(checkpointer=MemorySaver())\n",
    "\n",
    "tmp_dir = Path(\"tmp_runs/hitl\").resolve()\n",
    "tmp_dir.mkdir(parents=True, exist_ok=True)\n",
    "victim = tmp_dir / \"victim.txt\"\n",
    "victim.write_text(\"delete me\", encoding=\"utf-8\")\n",
    "\n",
    "thread_id = \"HITL_API_DEMO\"\n",
    "out1 = graph.invoke({\"target_path\": str(victim), \"decision\": \"\"}, config={\"configurable\": {\"thread_id\": thread_id}})\n",
    "pending = out1.get(\"__interrupt__\", [])[0].value if \"__interrupt__\" in out1 else None\n",
    "out2 = graph.invoke(Command(resume=\"approve\"), config={\"configurable\": {\"thread_id\": thread_id}})\n",
    "{\"pending\": pending, \"victim_exists_after\": victim.exists()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70702f1",
   "metadata": {},
   "source": [
    "## Notebook ops: nbformat + nbclient + artifacts + papermill\n",
    "\n",
    "Notebooks are just JSON documents.\n",
    "That means you can:\n",
    "- generate them (`nbformat`)\n",
    "- execute them programmatically (`nbclient`)\n",
    "- collect outputs and errors\n",
    "- parameterize runs (`papermill`)\n",
    "\n",
    "Why include this in a LangChain/LangGraph tutorial?\n",
    "Because “agents that write and run notebooks” is a surprisingly practical workflow for data work.\n",
    "We’ll keep the demos safe: everything writes under `tmp_runs/`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c67224d",
   "metadata": {},
   "source": [
    "**This cell will:**\n",
    "- Demonstrate notebook operations (write/execute/parameterize notebooks).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e5ce879b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/app/tmp_runs/smoke_out.ipynb'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import nbformat\n",
    "from nbformat import validate\n",
    "from nbclient import NotebookClient\n",
    "\n",
    "run_dir = Path(\"tmp_runs\").resolve()\n",
    "run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "nb = nbformat.v4.new_notebook()\n",
    "nb.cells = [\n",
    "    nbformat.v4.new_markdown_cell(\"# nbclient smoke test\"),\n",
    "    nbformat.v4.new_code_cell(\"x = 2 + 3\\nprint(x)\"),\n",
    "    nbformat.v4.new_code_cell(\"import math\\nprint(math.sqrt(81))\"),\n",
    "]\n",
    "validate(nb)\n",
    "\n",
    "in_path = run_dir / \"smoke_in.ipynb\"\n",
    "out_path = run_dir / \"smoke_out.ipynb\"\n",
    "nbformat.write(nb, str(in_path))\n",
    "\n",
    "nb2 = nbformat.read(str(in_path), as_version=4)\n",
    "client = NotebookClient(nb2, resources={\"metadata\": {\"path\": str(run_dir)}}, timeout=60)\n",
    "client.execute()\n",
    "nbformat.write(nb2, str(out_path))\n",
    "\n",
    "str(out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2a81ad",
   "metadata": {},
   "source": [
    "### Write a notebook via a tool (from a spec)\n",
    "\n",
    "We’ll build a tiny notebook in memory (a title + a code cell), then write it to disk.\n",
    "\n",
    "This is the first building block for “notebook automation” — generating a notebook artifact from a structured spec.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479772d1",
   "metadata": {},
   "source": [
    "**This cell will:**\n",
    "- Demonstrate notebook operations (write/execute/parameterize notebooks).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "54bc1b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/app/notebooks/demo/tool_hello.ipynb'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any, Literal\n",
    "from langchain_core.tools import tool as lc_tool\n",
    "\n",
    "WORKSPACE = Path(\"notebooks\").resolve()\n",
    "WORKSPACE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _safe_path(rel_path: str) -> Path:\n",
    "    \"\"\"Resolve `rel_path` under `WORKSPACE` and reject path traversal.\"\"\"\n",
    "    p = (WORKSPACE / rel_path).resolve()\n",
    "    if not str(p).startswith(str(WORKSPACE)):\n",
    "        raise ValueError(\"Path escapes workspace\")\n",
    "    return p\n",
    "\n",
    "@lc_tool\n",
    "def write_notebook(spec: dict[str, Any], out_rel: str) -> str:\n",
    "    \"Write a notebook from a small spec into a safe workspace path.\"\n",
    "    out_path = _safe_path(out_rel)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    nb = nbformat.v4.new_notebook()\n",
    "    cells = []\n",
    "    for c in spec.get(\"cells\", []):\n",
    "        t: Literal[\"markdown\", \"code\"] = c[\"type\"]\n",
    "        src = c.get(\"source\", \"\")\n",
    "        if t == \"markdown\":\n",
    "            cells.append(nbformat.v4.new_markdown_cell(src))\n",
    "        elif t == \"code\":\n",
    "            cells.append(nbformat.v4.new_code_cell(src))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown cell type: {t}\")\n",
    "    nb.cells = cells\n",
    "\n",
    "    validate(nb)\n",
    "    nbformat.write(nb, str(out_path))\n",
    "    return str(out_path)\n",
    "\n",
    "spec = {\"cells\": [{\"type\": \"markdown\", \"source\": \"# Tool-written notebook\"}, {\"type\": \"code\", \"source\": \"print('ok')\"}]}\n",
    "write_notebook.invoke({\"spec\": spec, \"out_rel\": \"demo/tool_hello.ipynb\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00dab2e",
   "metadata": {},
   "source": [
    "### Notebook ops as tools + secure injected workspace (ToolNode)\n",
    "\n",
    "Here we treat notebook operations as **tools** inside a LangGraph workflow.\n",
    "\n",
    "The important idea:\n",
    "- tools can be powerful (file access, execution)\n",
    "- so we often want a *controlled* workspace root\n",
    "\n",
    "You’ll see us use an injected workspace directory so the graph can safely read/write only where we intend.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f42949",
   "metadata": {},
   "source": [
    "**This cell will:**\n",
    "- Build and compile a `StateGraph` (a small LangGraph workflow).\n",
    "- Use `ToolNode` to execute tool calls inside a graph.\n",
    "- Demonstrate notebook operations (write/execute/parameterize notebooks).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "de9b1e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/app/tmp_runs/ipynb_tools_workspace/demo/in.ipynb',\n",
       " '/app/tmp_runs/ipynb_tools_workspace/demo/out.executed.ipynb',\n",
       " '[{\"path\": \"/app/tmp_runs/ipynb_tools_workspace/demo/in.ipynb\", \"size\": 458}, {\"path\": \"/app/tmp_runs/ipynb_tools_workspace/demo/out.executed.ipynb\", \"size\": 1091}]')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import base64\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing_extensions import Annotated as TxAnnotated\n",
    "\n",
    "from langgraph.prebuilt import InjectedState\n",
    "\n",
    "def _safe_injected_path(workspace_dir: str, rel_path: str) -> Path:\n",
    "    \"\"\"Resolve `rel_path` under `workspace_dir` and reject path traversal.\"\"\"\n",
    "    root = Path(workspace_dir).resolve()\n",
    "    p = (root / rel_path).resolve()\n",
    "    if not str(p).startswith(str(root)):\n",
    "        raise ValueError(\"Path escapes injected workspace\")\n",
    "    return p\n",
    "\n",
    "@lc_tool\n",
    "def nb_write(\n",
    "    spec: dict[str, Any],\n",
    "    out_rel: str,\n",
    "    workspace_dir: TxAnnotated[str, InjectedState(\"workspace_dir\")],\n",
    ") -> str:\n",
    "    \"Write a notebook under an injected workspace_dir.\"\n",
    "    out_path = _safe_injected_path(workspace_dir, out_rel)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    nb = nbformat.v4.new_notebook()\n",
    "    cells = []\n",
    "    for c in spec.get(\"cells\", []):\n",
    "        if c[\"type\"] == \"markdown\":\n",
    "            cells.append(nbformat.v4.new_markdown_cell(c.get(\"source\", \"\")))\n",
    "        elif c[\"type\"] == \"code\":\n",
    "            cells.append(nbformat.v4.new_code_cell(c.get(\"source\", \"\")))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown cell type: {c['type']}\")\n",
    "    nb.cells = cells\n",
    "    nb.metadata[\"kernelspec\"] = {\"name\": \"python3\", \"display_name\": \"Python 3\", \"language\": \"python\"}\n",
    "    validate(nb)\n",
    "    nbformat.write(nb, str(out_path))\n",
    "    return str(out_path)\n",
    "\n",
    "@lc_tool\n",
    "def nb_run(\n",
    "    in_rel: str,\n",
    "    out_rel: str,\n",
    "    timeout_s: int,\n",
    "    workspace_dir: TxAnnotated[str, InjectedState(\"workspace_dir\")],\n",
    ") -> str:\n",
    "    \"Execute a notebook with nbclient and save the executed copy under workspace_dir.\"\n",
    "    in_path = _safe_injected_path(workspace_dir, in_rel)\n",
    "    out_path = _safe_injected_path(workspace_dir, out_rel)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    nb = nbformat.read(str(in_path), as_version=4)\n",
    "    client = NotebookClient(nb, timeout=int(timeout_s), resources={\"metadata\": {\"path\": str(out_path.parent)}})\n",
    "    client.execute()\n",
    "    nbformat.write(nb, str(out_path))\n",
    "    return str(out_path)\n",
    "\n",
    "@lc_tool\n",
    "def nb_extract_errors(\n",
    "    executed_rel: str,\n",
    "    workspace_dir: TxAnnotated[str, InjectedState(\"workspace_dir\")],\n",
    ") -> str:\n",
    "    \"Extract per-cell error metadata from an executed notebook (JSON string).\"\n",
    "    p = _safe_injected_path(workspace_dir, executed_rel)\n",
    "    nb = nbformat.read(str(p), as_version=4)\n",
    "    errs = []\n",
    "    for i, cell in enumerate(nb.cells):\n",
    "        if cell.get(\"cell_type\") != \"code\":\n",
    "            continue\n",
    "        for out in cell.get(\"outputs\", []):\n",
    "            if out.get(\"output_type\") == \"error\":\n",
    "                errs.append({\"cell_index\": i, \"ename\": out.get(\"ename\"), \"evalue\": out.get(\"evalue\")})\n",
    "    return json.dumps(errs)\n",
    "\n",
    "@lc_tool\n",
    "def nb_extract_artifacts(\n",
    "    executed_rel: str,\n",
    "    artifacts_rel_dir: str,\n",
    "    workspace_dir: TxAnnotated[str, InjectedState(\"workspace_dir\")],\n",
    ") -> str:\n",
    "    \"Extract stdout + inline PNGs from an executed notebook into artifacts_rel_dir (JSON manifest).\"\n",
    "    p = _safe_injected_path(workspace_dir, executed_rel)\n",
    "    out_dir = _safe_injected_path(workspace_dir, artifacts_rel_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    nb = nbformat.read(str(p), as_version=4)\n",
    "    manifest = []\n",
    "    for i, cell in enumerate(nb.cells):\n",
    "        if cell.get(\"cell_type\") != \"code\":\n",
    "            continue\n",
    "        for j, out in enumerate(cell.get(\"outputs\", [])):\n",
    "            if out.get(\"output_type\") == \"stream\":\n",
    "                txt = out.get(\"text\", \"\")\n",
    "                fp = out_dir / f\"cell_{i}_stream_{j}.txt\"\n",
    "                fp.write_text(txt if isinstance(txt, str) else \"\".join(txt))\n",
    "                manifest.append({\"cell\": i, \"kind\": \"stream\", \"path\": str(fp)})\n",
    "            if out.get(\"output_type\") in (\"display_data\", \"execute_result\"):\n",
    "                data = out.get(\"data\", {})\n",
    "                if \"image/png\" in data:\n",
    "                    b64 = data[\"image/png\"]\n",
    "                    b = base64.b64decode(b64 if isinstance(b64, str) else \"\".join(b64))\n",
    "                    fp = out_dir / f\"cell_{i}_img_{j}.png\"\n",
    "                    fp.write_bytes(b)\n",
    "                    manifest.append({\"cell\": i, \"kind\": \"image/png\", \"path\": str(fp)})\n",
    "    (out_dir / \"manifest.json\").write_text(json.dumps(manifest, indent=2))\n",
    "    return json.dumps({\"n\": len(manifest), \"manifest\": str(out_dir / \"manifest.json\")})\n",
    "\n",
    "@lc_tool\n",
    "def nb_list_files(\n",
    "    workspace_dir: TxAnnotated[str, InjectedState(\"workspace_dir\")],\n",
    ") -> str:\n",
    "    \"List files under workspace_dir (JSON).\"\n",
    "    root = Path(workspace_dir).resolve()\n",
    "    files = []\n",
    "    for p in root.rglob(\"*\"):\n",
    "        if p.is_file():\n",
    "            files.append({\"path\": str(p), \"size\": p.stat().st_size})\n",
    "    return json.dumps(files[:200])\n",
    "\n",
    "from typing import Annotated, TypedDict\n",
    "from langchain_core.messages import AIMessage\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "class ToolGraphState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    workspace_dir: str\n",
    "\n",
    "workspace = Path(\"tmp_runs/ipynb_tools_workspace\").resolve()\n",
    "workspace.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tool_node = ToolNode([nb_write, nb_run, nb_extract_errors, nb_extract_artifacts, nb_list_files])\n",
    "g = StateGraph(ToolGraphState)\n",
    "g.add_node(\"tools\", tool_node)\n",
    "g.add_edge(START, \"tools\")\n",
    "g.add_edge(\"tools\", END)\n",
    "graph = g.compile()\n",
    "\n",
    "spec = {\n",
    "    \"cells\": [\n",
    "        {\"type\": \"markdown\", \"source\": \"# Tool-made notebook\"},\n",
    "        {\"type\": \"code\", \"source\": \"print('hello')\"},\n",
    "    ]\n",
    "}\n",
    "\n",
    "# IMPORTANT: Tool calls in a single ToolNode are not a dependency graph.\n",
    "# Execute dependent operations in separate invocations for deterministic behavior.\n",
    "\n",
    "out1 = graph.invoke(\n",
    "    {\"workspace_dir\": str(workspace), \"messages\": [AIMessage(content=\"\", tool_calls=[\n",
    "        {\"name\": \"nb_write\", \"args\": {\"spec\": spec, \"out_rel\": \"demo/in.ipynb\"}, \"id\": \"t1\", \"type\": \"tool_call\"},\n",
    "    ])]}\n",
    ")\n",
    "\n",
    "out2 = graph.invoke(\n",
    "    {\"workspace_dir\": str(workspace), \"messages\": [AIMessage(content=\"\", tool_calls=[\n",
    "        {\"name\": \"nb_run\", \"args\": {\"in_rel\": \"demo/in.ipynb\", \"out_rel\": \"demo/out.executed.ipynb\", \"timeout_s\": 60}, \"id\": \"t2\", \"type\": \"tool_call\"},\n",
    "    ])]}\n",
    ")\n",
    "\n",
    "out3 = graph.invoke(\n",
    "    {\"workspace_dir\": str(workspace), \"messages\": [AIMessage(content=\"\", tool_calls=[\n",
    "        {\"name\": \"nb_list_files\", \"args\": {}, \"id\": \"t3\", \"type\": \"tool_call\"},\n",
    "    ])]}\n",
    ")\n",
    "\n",
    "out1[\"messages\"][-1].content, out2[\"messages\"][-1].content, out3[\"messages\"][-1].content[:200]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a969b76e",
   "metadata": {},
   "source": [
    "### Execute notebooks + collect errors\n",
    "\n",
    "We’ll execute a notebook programmatically and capture:\n",
    "- stdout\n",
    "- execution errors (if any)\n",
    "\n",
    "This is a friendly way to build “run this notebook and report back” pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496df8ff",
   "metadata": {},
   "source": [
    "**This cell will:**\n",
    "- Demonstrate notebook operations (write/execute/parameterize notebooks).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f07a8da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cell_index': 2, 'ename': 'ZeroDivisionError', 'evalue': 'division by zero'}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nbclient.exceptions import CellExecutionError\n",
    "\n",
    "run_dir = Path(\"tmp_runs/execute\").resolve()\n",
    "run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Notebook that errors.\n",
    "nb_err = nbformat.v4.new_notebook()\n",
    "nb_err.cells = [\n",
    "    nbformat.v4.new_markdown_cell(\"# Intentional error\"),\n",
    "    nbformat.v4.new_code_cell(\"print('before')\"),\n",
    "    nbformat.v4.new_code_cell(\"1/0\"),\n",
    "    nbformat.v4.new_code_cell(\"print('after')\"),\n",
    "]\n",
    "validate(nb_err)\n",
    "in_path = run_dir / \"error_in.ipynb\"\n",
    "out_path = run_dir / \"error_out.executed.ipynb\"\n",
    "nbformat.write(nb_err, str(in_path))\n",
    "\n",
    "nb = nbformat.read(str(in_path), as_version=4)\n",
    "client = NotebookClient(nb, timeout=60, allow_errors=True, resources={\"metadata\": {\"path\": str(run_dir)}})\n",
    "client.execute()\n",
    "nbformat.write(nb, str(out_path))\n",
    "\n",
    "def extract_errors(nb) -> list[dict]:\n",
    "    \"\"\"Extract cell execution errors from an executed notebook.\"\"\"\n",
    "    errs = []\n",
    "    for i, cell in enumerate(nb.cells):\n",
    "        if cell.get(\"cell_type\") != \"code\":\n",
    "            continue\n",
    "        for out in cell.get(\"outputs\", []):\n",
    "            if out.get(\"output_type\") == \"error\":\n",
    "                errs.append({\"cell_index\": i, \"ename\": out.get(\"ename\"), \"evalue\": out.get(\"evalue\")})\n",
    "    return errs\n",
    "\n",
    "extract_errors(nb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7054c8",
   "metadata": {},
   "source": [
    "### Extract artifacts from executed notebooks (stdout + inline images)\n",
    "\n",
    "Executed notebooks can contain rich outputs (plots, tables, HTML).\n",
    "\n",
    "We’ll show a simple approach to pull a couple useful artifacts out of the executed notebook:\n",
    "- printed output\n",
    "- embedded images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b73c67c",
   "metadata": {},
   "source": [
    "**This cell will:**\n",
    "- Demonstrate notebook operations (write/execute/parameterize notebooks).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8083a142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'executed_nb': '/app/tmp_runs/artifacts/artifacts.executed.ipynb',\n",
       " 'n_artifacts': 3,\n",
       " 'manifest': '/app/tmp_runs/artifacts/out/manifest.json'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import base64\n",
    "import json\n",
    "\n",
    "run_dir = Path(\"tmp_runs/artifacts\").resolve()\n",
    "run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "nb = nbformat.v4.new_notebook()\n",
    "nb.cells = [\n",
    "    nbformat.v4.new_markdown_cell(\"# Artifact notebook\"),\n",
    "    nbformat.v4.new_code_cell(\"print('hello from stdout')\"),\n",
    "    nbformat.v4.new_code_cell(\n",
    "        \"import matplotlib.pyplot as plt\\n\"\n",
    "        \"plt.plot([0,1,2],[0,1,4])\\n\"\n",
    "        \"plt.title('inline')\\n\"\n",
    "        \"plt.show()\\n\"\n",
    "    ),\n",
    "]\n",
    "in_nb = run_dir / \"artifacts_in.ipynb\"\n",
    "executed_nb = run_dir / \"artifacts.executed.ipynb\"\n",
    "nbformat.write(nb, str(in_nb))\n",
    "\n",
    "nb2 = nbformat.read(str(in_nb), as_version=4)\n",
    "NotebookClient(nb2, timeout=120, resources={\"metadata\": {\"path\": str(run_dir)}}).execute()\n",
    "nbformat.write(nb2, str(executed_nb))\n",
    "\n",
    "out_dir = run_dir / \"out\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "manifest = []\n",
    "\n",
    "for i, cell in enumerate(nb2.cells):\n",
    "    if cell.get(\"cell_type\") != \"code\":\n",
    "        continue\n",
    "    for j, out in enumerate(cell.get(\"outputs\", [])):\n",
    "        if out.get(\"output_type\") == \"stream\":\n",
    "            txt = out.get(\"text\", \"\")\n",
    "            p = out_dir / f\"cell_{i}_stream_{j}.txt\"\n",
    "            p.write_text(txt if isinstance(txt, str) else \"\".join(txt))\n",
    "            manifest.append({\"cell\": i, \"kind\": \"stream\", \"path\": str(p)})\n",
    "        if out.get(\"output_type\") in (\"display_data\", \"execute_result\"):\n",
    "            data = out.get(\"data\", {})\n",
    "            if \"text/plain\" in data:\n",
    "                t = data[\"text/plain\"]\n",
    "                p = out_dir / f\"cell_{i}_text_{j}.txt\"\n",
    "                p.write_text(t if isinstance(t, str) else \"\".join(t))\n",
    "                manifest.append({\"cell\": i, \"kind\": \"text/plain\", \"path\": str(p)})\n",
    "            if \"image/png\" in data:\n",
    "                b64 = data[\"image/png\"]\n",
    "                b = base64.b64decode(b64 if isinstance(b64, str) else \"\".join(b64))\n",
    "                p = out_dir / f\"cell_{i}_img_{j}.png\"\n",
    "                p.write_bytes(b)\n",
    "                manifest.append({\"cell\": i, \"kind\": \"image/png\", \"path\": str(p)})\n",
    "\n",
    "(out_dir / \"manifest.json\").write_text(json.dumps(manifest, indent=2))\n",
    "{\"executed_nb\": str(executed_nb), \"n_artifacts\": len(manifest), \"manifest\": str(out_dir / \"manifest.json\")}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362532e3",
   "metadata": {},
   "source": [
    "### Filesystem artifacts (notebooks that write files)\n",
    "\n",
    "Sometimes notebooks produce *real files* (CSVs, images, model outputs).\n",
    "\n",
    "In the next cell we execute a notebook that writes files into a run directory, then list what it produced.\n",
    "Everything stays under `tmp_runs/`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf7da23",
   "metadata": {},
   "source": [
    "**This cell will:**\n",
    "- Demonstrate notebook operations (write/execute/parameterize notebooks).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "54b39bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot.png', 'table.csv', 'writes_files.executed.ipynb', 'writes_files.ipynb']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "run_dir = Path(\"tmp_runs/writes_files\").resolve()\n",
    "run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "nb = nbformat.v4.new_notebook()\n",
    "nb.cells = [\n",
    "    nbformat.v4.new_markdown_cell(\"# Writes files\"),\n",
    "    nbformat.v4.new_code_cell(\n",
    "        \"import csv\\n\"\n",
    "        \"import matplotlib.pyplot as plt\\n\"\n",
    "        \"\\n\"\n",
    "        \"rows = [(i, i*i) for i in range(5)]\\n\"\n",
    "        \"with open('table.csv', 'w', newline='') as f:\\n\"\n",
    "        \"    w = csv.writer(f)\\n\"\n",
    "        \"    w.writerow(['x','y'])\\n\"\n",
    "        \"    w.writerows(rows)\\n\"\n",
    "        \"\\n\"\n",
    "        \"xs = [r[0] for r in rows]\\n\"\n",
    "        \"ys = [r[1] for r in rows]\\n\"\n",
    "        \"plt.plot(xs, ys)\\n\"\n",
    "        \"plt.title('y=x^2')\\n\"\n",
    "        \"plt.savefig('plot.png', dpi=120)\\n\"\n",
    "        \"print('wrote table.csv and plot.png')\\n\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "in_nb = run_dir / \"writes_files.ipynb\"\n",
    "out_nb = run_dir / \"writes_files.executed.ipynb\"\n",
    "nbformat.write(nb, str(in_nb))\n",
    "\n",
    "nb2 = nbformat.read(str(in_nb), as_version=4)\n",
    "NotebookClient(nb2, timeout=120, resources={\"metadata\": {\"path\": str(run_dir)}}).execute()\n",
    "nbformat.write(nb2, str(out_nb))\n",
    "\n",
    "sorted([p.name for p in run_dir.iterdir() if p.is_file()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97598ef",
   "metadata": {},
   "source": [
    "### Parameterized runs (Papermill)\n",
    "\n",
    "Papermill is a simple way to run the *same* notebook with different parameters.\n",
    "\n",
    "This is useful for:\n",
    "- experiments\n",
    "- scheduled reports\n",
    "- batch runs over multiple inputs\n",
    "\n",
    "We’ll do a tiny demo so you can see the mechanics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d80b8a",
   "metadata": {},
   "source": [
    "**This cell will:**\n",
    "- Demonstrate notebook operations (write/execute/parameterize notebooks).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a0016ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 21:53:49,784 INFO papermill - Input Notebook:  /app/tmp_runs/papermill/pm_in.ipynb\n",
      "2026-02-05 21:53:49,784 INFO papermill - Output Notebook: /app/tmp_runs/papermill/pm_out.ipynb\n",
      "2026-02-05 21:53:49,785 INFO papermill - Working directory: /app/tmp_runs/papermill\n",
      "Executing:   0%|          | 0/4 [00:00<?, ?cell/s]2026-02-05 21:53:51,004 INFO papermill - Executing notebook with kernel: python3\n",
      "Executing: 100%|██████████| 4/4 [00:01<00:00,  2.92cell/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/app/tmp_runs/papermill/pm_out.ipynb'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import papermill as pm\n",
    "\n",
    "run_dir = Path(\"tmp_runs/papermill\").resolve()\n",
    "run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "nb = nbformat.v4.new_notebook()\n",
    "nb.cells = [\n",
    "    nbformat.v4.new_markdown_cell(\"# Papermill demo\"),\n",
    "    nbformat.v4.new_code_cell(\"# Parameters\\nx = 1\\ny = 2\", metadata={\"tags\": [\"parameters\"]}),\n",
    "    nbformat.v4.new_code_cell(\"print({'x': x, 'y': y, 'x_plus_y': x + y})\"),\n",
    "]\n",
    "nb.metadata[\"kernelspec\"] = {\"name\": \"python3\", \"display_name\": \"Python 3\", \"language\": \"python\"}\n",
    "\n",
    "in_nb = run_dir / \"pm_in.ipynb\"\n",
    "out_nb = run_dir / \"pm_out.ipynb\"\n",
    "nbformat.write(nb, str(in_nb))\n",
    "\n",
    "pm.execute_notebook(str(in_nb), str(out_nb), parameters={\"x\": 10, \"y\": 32}, cwd=str(run_dir), kernel_name=\"python3\")\n",
    "str(out_nb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98715036",
   "metadata": {},
   "source": [
    "## Deep Agents — API overview (used in `langchain.example.ipynb`)\n",
    "\n",
    "Deep Agents (the `deepagents` package used in this tutorial) is an optional layer that bundles a few “agent app” conveniences:\n",
    "\n",
    "- a ready-to-run agent loop (`create_deep_agent(...)`)\n",
    "- a toolbox (todos, filesystem tools, delegation to subagents)\n",
    "- pluggable **backends** (where state/files/stores live)\n",
    "- safety controls like sandboxing + HITL gates\n",
    "\n",
    "This notebook keeps Deep Agents coverage focused on the *public surface*:\n",
    "- `create_deep_agent(...)`\n",
    "- Backends: `FilesystemBackend`, `StateBackend`, `StoreBackend`, `CompositeBackend`\n",
    "- Subagents: `CompiledSubAgent`\n",
    "- HITL gates: `interrupt_on=...` and `Command(resume=...)`\n",
    "\n",
    "For the full DA1–DA8 walkthrough, see `langchain.example.ipynb`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30374b22",
   "metadata": {},
   "source": [
    "**This cell will:**\n",
    "- Run a Deep Agents demo (filesystem/todos/subagents/HITL).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "be3afa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepagents: 0.3.11\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import deepagents  # type: ignore\n",
    "    from deepagents import CompiledSubAgent, create_deep_agent  # type: ignore\n",
    "    from deepagents.backends import (  # type: ignore\n",
    "        CompositeBackend,\n",
    "        FilesystemBackend,\n",
    "        StateBackend,\n",
    "        StoreBackend,\n",
    "    )\n",
    "\n",
    "    print(\"deepagents:\", getattr(deepagents, \"__version__\", \"(unknown)\"))\n",
    "except Exception as e:  # pragma: no cover\n",
    "    raise RuntimeError(\n",
    "        \"This section requires `deepagents`.\\n\"\n",
    "        f\"Import error: {type(e).__name__}: {str(e)[:200]}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9d928e",
   "metadata": {},
   "source": [
    "This next cell shows how Deep Agents’ **virtual filesystem** works.\n",
    "\n",
    "- The agent will refer to files like `/workspace/hello.txt`.\n",
    "- Under the hood, that maps to a real folder you can see locally: `./workspace/hello.txt`.\n",
    "\n",
    "Why this matters:\n",
    "- it keeps agent file access *contained* (good for safety)\n",
    "- it makes it easy to inspect what the agent wrote\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ae371e",
   "metadata": {},
   "source": [
    "**This cell will:**\n",
    "- Run a Deep Agents demo (filesystem/todos/subagents/HITL).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "df2d8dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 21:53:52,594 INFO httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-05 21:53:53,541 INFO httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello.txt paths on disk: ['workspace/hello.txt']\n",
      "final message preview: The file /workspace/hello.txt already exists with the content: \"hello\". If you want, I can edit this file or write to a new file. What would you like to do?\n"
     ]
    }
   ],
   "source": [
    "root = Path(\".\").resolve()\n",
    "Path(\"workspace\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "backend = FilesystemBackend(root_dir=str(root), virtual_mode=True)\n",
    "agent = create_deep_agent(model=get_chat_model(), backend=backend)\n",
    "\n",
    "out = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    \"Call write_file with file_path='/workspace/hello.txt' and content='hello'. \"\n",
    "                    \"Then call read_file on '/workspace/hello.txt' and return the content.\"\n",
    "                ),\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "paths = sorted([str(p) for p in Path(\"workspace\").rglob(\"hello.txt\")])\n",
    "print(\"hello.txt paths on disk:\", paths)\n",
    "print(\"final message preview:\", getattr(out[\"messages\"][-1], \"content\", \"\")[:200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3387e10",
   "metadata": {},
   "source": [
    "Deep Agents also supports **human-in-the-loop (HITL) gating** for filesystem edits/deletes via `interrupt_on=...`.\n",
    "\n",
    "In plain English:\n",
    "- you tell the agent “you may *propose* edits, but a human must approve them”\n",
    "- when the agent tries to do the gated action, the run produces an interrupt payload\n",
    "- you resume with `Command(resume=\"approve\")` or `Command(resume=\"reject\")`\n",
    "\n",
    "The examples notebook shows a fuller interrupt + resume flow. Here we just wire up the guardrail so you can see the API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c493dc",
   "metadata": {},
   "source": [
    "**This cell will:**\n",
    "- Run a Deep Agents demo (filesystem/todos/subagents/HITL).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "647c4f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HITL gate installed for tool=edit_file (approve/reject).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "langgraph.graph.state.CompiledStateGraph"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from langchain.agents.middleware.human_in_the_loop import InterruptOnConfig\n",
    "except ModuleNotFoundError as e:  # pragma: no cover\n",
    "    raise RuntimeError(\n",
    "        \"\"\"This Deep Agents HITL demo needs the tutorial dependencies.\n",
    "\n",
    "Run it from `tutorials/LangChain_LangGraph` with `requirements.txt` installed (or via Docker).\n",
    "\"\"\"\n",
    "    ) from e\n",
    "\n",
    "root = Path(\".\").resolve()\n",
    "backend = FilesystemBackend(root_dir=str(root), virtual_mode=True)\n",
    "agent = create_deep_agent(\n",
    "    model=get_chat_model(),\n",
    "    backend=backend,\n",
    "    interrupt_on={\"edit_file\": InterruptOnConfig(allowed_decisions=[\"approve\", \"reject\"])},\n",
    ")\n",
    "\n",
    "print(\"HITL gate installed for tool=edit_file (approve/reject).\")\n",
    "type(agent)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
